/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:549: FutureWarning: The class `PretrainedBartModel` has been depreciated, please use `BartPreTrainedModel` instead.
  warnings.warn(
[nltk_data] Downloading package punkt to
[nltk_data]     /home/is/kaifan-l/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
trainable params: 184,320 || all params: 139,604,736 || trainable%: 0.13202990477343118
BartPrefixForConditionalGeneration(
  (model): PeftModelForSeq2SeqLM(
    (base_model): BartForConditionalGeneration(
      (model): BartModel(
        (shared): Embedding(50265, 768, padding_idx=1)
        (encoder): BartEncoder(
          (embed_tokens): Embedding(50265, 768, padding_idx=1)
          (embed_positions): BartLearnedPositionalEmbedding(1026, 768)
          (layers): ModuleList(
            (0-5): 6 x BartEncoderLayer(
              (self_attn): BartAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (activation_fn): GELUActivation()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
          (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (decoder): BartDecoder(
          (embed_tokens): Embedding(50265, 768, padding_idx=1)
          (embed_positions): BartLearnedPositionalEmbedding(1026, 768)
          (layers): ModuleList(
            (0-5): 6 x BartDecoderLayer(
              (self_attn): BartAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (activation_fn): GELUActivation()
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (encoder_attn): BartAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
          (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (lm_head): Linear(in_features=768, out_features=50265, bias=False)
    )
    (prompt_encoder): ModuleDict(
      (default): PrefixEncoder(
        (embedding): Embedding(20, 9216)
      )
    )
    (word_embeddings): Embedding(50265, 768, padding_idx=1)
  )
)
Start training ...
cuda
================================== epoch 0 ==================================
  0%|          | 0/36 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  3%|▎         | 1/36 [00:01<00:53,  1.53s/it]  6%|▌         | 2/36 [00:02<00:49,  1.44s/it]  8%|▊         | 3/36 [00:04<00:48,  1.46s/it] 11%|█         | 4/36 [00:06<00:50,  1.57s/it] 14%|█▍        | 5/36 [00:07<00:50,  1.63s/it] 17%|█▋        | 6/36 [00:09<00:45,  1.50s/it] 19%|█▉        | 7/36 [00:10<00:44,  1.52s/it] 22%|██▏       | 8/36 [00:12<00:43,  1.56s/it] 25%|██▌       | 9/36 [00:13<00:42,  1.57s/it] 28%|██▊       | 10/36 [00:15<00:42,  1.63s/it] 31%|███       | 11/36 [00:17<00:42,  1.69s/it] 33%|███▎      | 12/36 [00:19<00:40,  1.70s/it] 36%|███▌      | 13/36 [00:20<00:36,  1.57s/it] 39%|███▉      | 14/36 [00:21<00:33,  1.54s/it] 42%|████▏     | 15/36 [00:23<00:34,  1.63s/it] 44%|████▍     | 16/36 [00:25<00:31,  1.57s/it] 47%|████▋     | 17/36 [00:27<00:31,  1.65s/it] 50%|█████     | 18/36 [00:28<00:28,  1.58s/it] 53%|█████▎    | 19/36 [00:30<00:27,  1.64s/it] 56%|█████▌    | 20/36 [00:32<00:27,  1.71s/it] 58%|█████▊    | 21/36 [00:34<00:26,  1.75s/it] 61%|██████    | 22/36 [00:35<00:24,  1.72s/it] 64%|██████▍   | 23/36 [00:37<00:21,  1.68s/it] 67%|██████▋   | 24/36 [00:38<00:20,  1.67s/it] 69%|██████▉   | 25/36 [00:40<00:17,  1.58s/it] 72%|███████▏  | 26/36 [00:42<00:16,  1.63s/it] 75%|███████▌  | 27/36 [00:43<00:14,  1.59s/it] 78%|███████▊  | 28/36 [00:45<00:13,  1.68s/it] 81%|████████  | 29/36 [00:47<00:11,  1.67s/it] 83%|████████▎ | 30/36 [00:48<00:10,  1.71s/it] 86%|████████▌ | 31/36 [00:50<00:08,  1.73s/it] 89%|████████▉ | 32/36 [00:52<00:06,  1.68s/it] 92%|█████████▏| 33/36 [00:53<00:05,  1.71s/it] 94%|█████████▍| 34/36 [00:55<00:03,  1.70s/it] 97%|█████████▋| 35/36 [00:57<00:01,  1.71s/it]100%|██████████| 36/36 [00:58<00:00,  1.62s/it]100%|██████████| 36/36 [00:58<00:00,  1.63s/it]
GPU Memory before entering the train : 533
GPU Memory consumed at the end of the train (end-begin): 4141
GPU Peak Memory consumed during the train (max-begin): 20992
GPU Total Peak Memory consumed during the train (max): 21525
CPU Memory before entering the train : 1873
CPU Memory consumed at the end of the train (end-begin): 447
CPU Peak Memory consumed during the train (max-begin): 447
CPU Total Peak Memory consumed during the train (max): 2320
epoch=0: train_ppl=tensor(7178.3799, device='cuda:0') train_epoch_loss=tensor(8.8788, device='cuda:0')


Start evaluating ...
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:01<00:20,  1.26s/it] 12%|█▏        | 2/17 [00:02<00:21,  1.42s/it] 18%|█▊        | 3/17 [00:04<00:19,  1.39s/it] 24%|██▎       | 4/17 [00:05<00:18,  1.45s/it] 29%|██▉       | 5/17 [00:06<00:16,  1.39s/it] 35%|███▌      | 6/17 [00:08<00:14,  1.30s/it] 41%|████      | 7/17 [00:09<00:13,  1.31s/it] 47%|████▋     | 8/17 [00:10<00:12,  1.36s/it] 53%|█████▎    | 9/17 [00:12<00:11,  1.38s/it] 59%|█████▉    | 10/17 [00:13<00:10,  1.43s/it] 65%|██████▍   | 11/17 [00:15<00:08,  1.47s/it] 71%|███████   | 12/17 [00:16<00:07,  1.47s/it] 76%|███████▋  | 13/17 [00:18<00:05,  1.39s/it] 82%|████████▏ | 14/17 [00:19<00:04,  1.42s/it] 88%|████████▊ | 15/17 [00:20<00:02,  1.39s/it] 94%|█████████▍| 16/17 [00:22<00:01,  1.40s/it]100%|██████████| 17/17 [00:22<00:00,  1.17s/it]100%|██████████| 17/17 [00:22<00:00,  1.35s/it]
/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/transformers/utils/hub.py:831: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers.
  warnings.warn(
epoch=0: eval_ppl=tensor(1097.7078, device='cuda:0') eval_epoch_loss=tensor(7.0010, device='cuda:0')
Traceback (most recent call last):
  File "/home/is/kaifan-l/private_room/proj-repos/prompt-for-long-text-summarization/run.py", line 252, in <module>
    main()
  File "/home/is/kaifan-l/private_room/proj-repos/prompt-for-long-text-summarization/run.py", line 243, in main
    model.push_to_hub(
  File "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/transformers/utils/hub.py", line 869, in push_to_hub
    repo_id = self._create_repo(
              ^^^^^^^^^^^^^^^^^^
  File "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/transformers/utils/hub.py", line 708, in _create_repo
    url = create_repo(repo_id=repo_id, token=token, private=private, exist_ok=True)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/huggingface_hub/hf_api.py", line 2304, in create_repo
    headers = self._build_hf_headers(token=token, is_write_action=True)
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/huggingface_hub/hf_api.py", line 5008, in _build_hf_headers
    return build_hf_headers(
           ^^^^^^^^^^^^^^^^^
  File "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py", line 118, in _inner_fn
    return fn(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^
  File "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/huggingface_hub/utils/_headers.py", line 121, in build_hf_headers
    token_to_send = get_token_to_send(token)
                    ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/huggingface_hub/utils/_headers.py", line 153, in get_token_to_send
    raise LocalTokenNotFoundError(
huggingface_hub.utils._headers.LocalTokenNotFoundError: Token is required (`token=True`), but no token found. You need to provide a token or be logged in to Hugging Face with `huggingface-cli login` or `huggingface_hub.login`. See https://huggingface.co/settings/tokens.
