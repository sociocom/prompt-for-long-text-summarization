/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:549: FutureWarning: The class `PretrainedBartModel` has been depreciated, please use `BartPreTrainedModel` instead.
  warnings.warn(
[nltk_data] Downloading package punkt to
[nltk_data]     /home/is/kaifan-l/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
trainable params: 184,320 || all params: 139,604,736 || trainable%: 0.13202990477343118
BartPrefixForConditionalGeneration(
  (model): PeftModelForSeq2SeqLM(
    (base_model): BartForConditionalGeneration(
      (model): BartModel(
        (shared): Embedding(50265, 768, padding_idx=1)
        (encoder): BartEncoder(
          (embed_tokens): Embedding(50265, 768, padding_idx=1)
          (embed_positions): BartLearnedPositionalEmbedding(1026, 768)
          (layers): ModuleList(
            (0-5): 6 x BartEncoderLayer(
              (self_attn): BartAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (activation_fn): GELUActivation()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
          (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (decoder): BartDecoder(
          (embed_tokens): Embedding(50265, 768, padding_idx=1)
          (embed_positions): BartLearnedPositionalEmbedding(1026, 768)
          (layers): ModuleList(
            (0-5): 6 x BartDecoderLayer(
              (self_attn): BartAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (activation_fn): GELUActivation()
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (encoder_attn): BartAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
          (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (lm_head): Linear(in_features=768, out_features=50265, bias=False)
    )
    (prompt_encoder): ModuleDict(
      (default): PrefixEncoder(
        (embedding): Embedding(20, 9216)
      )
    )
    (word_embeddings): Embedding(50265, 768, padding_idx=1)
  )
)
Start training ...
cuda
================================== epoch 0 ==================================
  0%|          | 0/1795 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  0%|          | 1/1795 [00:03<1:36:18,  3.22s/it]  0%|          | 2/1795 [00:06<1:44:01,  3.48s/it]  0%|          | 3/1795 [00:10<1:42:17,  3.43s/it]  0%|          | 4/1795 [00:13<1:41:32,  3.40s/it]  0%|          | 5/1795 [00:16<1:37:14,  3.26s/it]  0%|          | 6/1795 [00:20<1:41:55,  3.42s/it]  0%|          | 7/1795 [00:24<1:44:49,  3.52s/it]  0%|          | 8/1795 [00:27<1:45:06,  3.53s/it]  1%|          | 9/1795 [00:31<1:46:41,  3.58s/it]  1%|          | 10/1795 [00:34<1:42:18,  3.44s/it]  1%|          | 11/1795 [00:37<1:37:41,  3.29s/it]  1%|          | 12/1795 [00:40<1:36:44,  3.26s/it]  1%|          | 13/1795 [00:44<1:40:44,  3.39s/it]  1%|          | 14/1795 [00:47<1:40:53,  3.40s/it]  1%|          | 15/1795 [00:50<1:37:23,  3.28s/it]  1%|          | 16/1795 [00:53<1:37:04,  3.27s/it]  1%|          | 17/1795 [00:56<1:34:14,  3.18s/it]  1%|          | 18/1795 [01:00<1:37:58,  3.31s/it]  1%|          | 19/1795 [01:03<1:36:47,  3.27s/it]  1%|          | 20/1795 [01:07<1:40:47,  3.41s/it]  1%|          | 21/1795 [01:10<1:40:25,  3.40s/it]  1%|          | 22/1795 [01:14<1:43:22,  3.50s/it]  1%|▏         | 23/1795 [01:18<1:45:19,  3.57s/it]  1%|▏         | 24/1795 [01:21<1:40:26,  3.40s/it]  1%|▏         | 25/1795 [01:24<1:38:39,  3.34s/it]  1%|▏         | 26/1795 [01:28<1:41:59,  3.46s/it]  2%|▏         | 27/1795 [01:31<1:44:04,  3.53s/it]  2%|▏         | 28/1795 [01:34<1:39:29,  3.38s/it]  2%|▏         | 29/1795 [01:38<1:40:34,  3.42s/it]  2%|▏         | 30/1795 [01:41<1:40:51,  3.43s/it]  2%|▏         | 31/1795 [01:45<1:40:56,  3.43s/it]  2%|▏         | 32/1795 [01:48<1:38:46,  3.36s/it]  2%|▏         | 33/1795 [01:52<1:39:44,  3.40s/it]  2%|▏         | 34/1795 [01:55<1:36:44,  3.30s/it]  2%|▏         | 35/1795 [01:58<1:39:01,  3.38s/it]  2%|▏         | 36/1795 [02:01<1:38:41,  3.37s/it]  2%|▏         | 37/1795 [02:05<1:40:42,  3.44s/it]  2%|▏         | 38/1795 [02:08<1:37:50,  3.34s/it]  2%|▏         | 39/1795 [02:12<1:41:15,  3.46s/it]  2%|▏         | 40/1795 [02:16<1:43:17,  3.53s/it]  2%|▏         | 41/1795 [02:19<1:45:02,  3.59s/it]  2%|▏         | 42/1795 [02:23<1:45:55,  3.63s/it]  2%|▏         | 43/1795 [02:27<1:46:47,  3.66s/it]  2%|▏         | 44/1795 [02:31<1:47:10,  3.67s/it]  3%|▎         | 45/1795 [02:34<1:46:53,  3.67s/it]  3%|▎         | 46/1795 [02:38<1:47:18,  3.68s/it]  3%|▎         | 47/1795 [02:41<1:43:03,  3.54s/it]  3%|▎         | 48/1795 [02:45<1:43:54,  3.57s/it]  3%|▎         | 49/1795 [02:48<1:39:16,  3.41s/it]  3%|▎         | 50/1795 [02:52<1:42:02,  3.51s/it]  3%|▎         | 51/1795 [02:55<1:38:54,  3.40s/it]  3%|▎         | 52/1795 [02:58<1:38:07,  3.38s/it]  3%|▎         | 53/1795 [03:01<1:38:49,  3.40s/it]  3%|▎         | 54/1795 [03:05<1:37:14,  3.35s/it]  3%|▎         | 55/1795 [03:08<1:35:55,  3.31s/it]  3%|▎         | 56/1795 [03:12<1:39:38,  3.44s/it]  3%|▎         | 57/1795 [03:15<1:37:23,  3.36s/it]  3%|▎         | 58/1795 [03:18<1:35:11,  3.29s/it]  3%|▎         | 59/1795 [03:21<1:33:47,  3.24s/it]  3%|▎         | 60/1795 [03:25<1:37:56,  3.39s/it]  3%|▎         | 61/1795 [03:28<1:39:38,  3.45s/it]  3%|▎         | 62/1795 [03:32<1:37:59,  3.39s/it]  4%|▎         | 63/1795 [03:35<1:41:20,  3.51s/it]  4%|▎         | 64/1795 [03:39<1:42:34,  3.56s/it]  4%|▎         | 65/1795 [03:43<1:45:18,  3.65s/it]