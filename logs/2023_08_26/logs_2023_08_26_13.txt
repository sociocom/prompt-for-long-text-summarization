/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:549: FutureWarning: The class `PretrainedBartModel` has been depreciated, please use `BartPreTrainedModel` instead.
  warnings.warn(
[nltk_data] Downloading package punkt to
[nltk_data]     /home/is/kaifan-l/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
trainable params: 184,320 || all params: 139,604,736 || trainable%: 0.13202990477343118
BartPrefixForConditionalGeneration(
  (model): PeftModelForSeq2SeqLM(
    (base_model): BartForConditionalGeneration(
      (model): BartModel(
        (shared): Embedding(50265, 768, padding_idx=1)
        (encoder): BartEncoder(
          (embed_tokens): Embedding(50265, 768, padding_idx=1)
          (embed_positions): BartLearnedPositionalEmbedding(1026, 768)
          (layers): ModuleList(
            (0-5): 6 x BartEncoderLayer(
              (self_attn): BartAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (activation_fn): GELUActivation()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
          (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (decoder): BartDecoder(
          (embed_tokens): Embedding(50265, 768, padding_idx=1)
          (embed_positions): BartLearnedPositionalEmbedding(1026, 768)
          (layers): ModuleList(
            (0-5): 6 x BartDecoderLayer(
              (self_attn): BartAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (activation_fn): GELUActivation()
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (encoder_attn): BartAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
          (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (lm_head): Linear(in_features=768, out_features=50265, bias=False)
    )
    (prompt_encoder): ModuleDict(
      (default): PrefixEncoder(
        (embedding): Embedding(20, 9216)
      )
    )
    (word_embeddings): Embedding(50265, 768, padding_idx=1)
  )
)
Start training ...
cuda
================================== epoch 0 ==================================
  0%|          | 0/36 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  3%|▎         | 1/36 [00:01<00:54,  1.54s/it]  6%|▌         | 2/36 [00:02<00:48,  1.44s/it]  8%|▊         | 3/36 [00:04<00:48,  1.46s/it] 11%|█         | 4/36 [00:06<00:50,  1.58s/it] 14%|█▍        | 5/36 [00:07<00:50,  1.63s/it] 17%|█▋        | 6/36 [00:09<00:45,  1.50s/it] 19%|█▉        | 7/36 [00:10<00:44,  1.52s/it] 22%|██▏       | 8/36 [00:12<00:43,  1.57s/it] 25%|██▌       | 9/36 [00:13<00:42,  1.57s/it] 28%|██▊       | 10/36 [00:15<00:42,  1.64s/it] 31%|███       | 11/36 [00:17<00:42,  1.70s/it] 33%|███▎      | 12/36 [00:19<00:40,  1.70s/it] 36%|███▌      | 13/36 [00:20<00:36,  1.58s/it] 39%|███▉      | 14/36 [00:22<00:34,  1.55s/it] 42%|████▏     | 15/36 [00:23<00:34,  1.64s/it] 44%|████▍     | 16/36 [00:25<00:31,  1.57s/it] 47%|████▋     | 17/36 [00:27<00:31,  1.66s/it] 50%|█████     | 18/36 [00:28<00:28,  1.60s/it] 53%|█████▎    | 19/36 [00:30<00:28,  1.66s/it] 56%|█████▌    | 20/36 [00:32<00:27,  1.72s/it] 58%|█████▊    | 21/36 [00:34<00:26,  1.77s/it] 61%|██████    | 22/36 [00:35<00:24,  1.72s/it] 64%|██████▍   | 23/36 [00:37<00:21,  1.68s/it] 67%|██████▋   | 24/36 [00:39<00:20,  1.69s/it] 69%|██████▉   | 25/36 [00:40<00:17,  1.59s/it] 72%|███████▏  | 26/36 [00:42<00:16,  1.64s/it] 75%|███████▌  | 27/36 [00:43<00:14,  1.60s/it] 78%|███████▊  | 28/36 [00:45<00:13,  1.68s/it] 81%|████████  | 29/36 [00:47<00:11,  1.67s/it] 83%|████████▎ | 30/36 [00:49<00:10,  1.71s/it] 86%|████████▌ | 31/36 [00:50<00:08,  1.75s/it] 89%|████████▉ | 32/36 [00:52<00:06,  1.69s/it] 92%|█████████▏| 33/36 [00:54<00:05,  1.72s/it] 94%|█████████▍| 34/36 [00:55<00:03,  1.71s/it] 97%|█████████▋| 35/36 [00:57<00:01,  1.72s/it]100%|██████████| 36/36 [00:59<00:00,  1.63s/it]100%|██████████| 36/36 [00:59<00:00,  1.64s/it]
GPU Memory before entering the train : 533
GPU Memory consumed at the end of the train (end-begin): 4141
GPU Peak Memory consumed during the train (max-begin): 20992
GPU Total Peak Memory consumed during the train (max): 21525
CPU Memory before entering the train : 1884
CPU Memory consumed at the end of the train (end-begin): 440
CPU Peak Memory consumed during the train (max-begin): 440
CPU Total Peak Memory consumed during the train (max): 2324
epoch=0: train_ppl=tensor(7178.3799, device='cuda:0') train_epoch_loss=tensor(8.8788, device='cuda:0')


Start evaluating ...
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:01<00:20,  1.25s/it] 12%|█▏        | 2/17 [00:02<00:21,  1.41s/it] 18%|█▊        | 3/17 [00:04<00:19,  1.39s/it] 24%|██▎       | 4/17 [00:05<00:18,  1.45s/it] 29%|██▉       | 5/17 [00:06<00:16,  1.39s/it] 35%|███▌      | 6/17 [00:08<00:14,  1.30s/it] 41%|████      | 7/17 [00:09<00:13,  1.31s/it] 47%|████▋     | 8/17 [00:10<00:12,  1.37s/it] 53%|█████▎    | 9/17 [00:12<00:11,  1.39s/it] 59%|█████▉    | 10/17 [00:13<00:10,  1.44s/it] 65%|██████▍   | 11/17 [00:15<00:08,  1.47s/it] 71%|███████   | 12/17 [00:16<00:07,  1.47s/it] 76%|███████▋  | 13/17 [00:18<00:05,  1.39s/it] 82%|████████▏ | 14/17 [00:19<00:04,  1.42s/it] 88%|████████▊ | 15/17 [00:20<00:02,  1.39s/it] 94%|█████████▍| 16/17 [00:22<00:01,  1.40s/it]100%|██████████| 17/17 [00:23<00:00,  1.17s/it]100%|██████████| 17/17 [00:23<00:00,  1.35s/it]
epoch=0: eval_ppl=tensor(1097.7078, device='cuda:0') eval_epoch_loss=tensor(7.0010, device='cuda:0')
  0%|          | 0/15 [00:00<?, ?it/s]  0%|          | 0/15 [00:21<?, ?it/s]
labels:  torch.Size([8, 256]) <class 'torch.Tensor'>
decoded_model_outputs:   in the years for the first time in the history of the first in the first years of the second to the first to the second in the second year of the Second year in the last one, the first two years in the third in the United states of the last decade in the U.S. and the first one in the twentieth century, and it was the first. The first one. In the last two years, the second, and the second. The second is in the fourth in the fifth in the sixth in the seventh in the ninth in the eighth in the tenth in the next generation to the fifth year for the second time. The third year for a third and the fourth year, the fifth and the fifth to the sixth to the seventh to the fourth to the eighth to the last in the Fifth in the US for the United to the United States.In the second generation in the 1960s in the 1970s.The second generation of the United in the Second in the 1980s. The fourth generation of a generation.The fourth generation in America in the 1990s in America. The fifth generation in New York for the fourth generation generation in a generation in Washington for the fifth generation generation generation for the sixth generation in this generation and the United for
generated_tokens: [tensor([[    2,     0,   243,  ...,   194,    11,     2],
        [    2,     0,   894,  ...,   382,    13,     2],
        [    2,     0,    23,  ...,     8,   188,     2],
        ...,
        [    2,     0, 13400,  ...,     5,   987,     2],
        [    2,     0, 24338,  ...,     1,     1,     1],
        [    2,     0,  4688,  ...,    11,  3345,     2]], device='cuda:0'), tensor([[  2,   0, 100,  ...,  13,  10,   2],
        [  2,   0, 128,  ...,   1,   1,   1],
        [  2,   0, 894,  ...,   1,   1,   1],
        ...,
        [  2,   0,  17,  ...,  93,  17,   2],
        [  2,   0,  44,  ...,   1,   1,   1],
        [  2,   0,  16,  ...,   1,   1,   1]], device='cuda:0'), tensor([[   2,    0,   11,  ...,  315,   13,    2],
        [   2,    0,   17,  ...,   93,   17,    2],
        [   2,    0,   17,  ...,   93,   17,    2],
        ...,
        [   2,    0,   17,  ...,   93,   17,    2],
        [   2,    0,  243,  ...,    1,    1,    1],
        [   2,    0,  521,  ..., 5703,  111,    2]], device='cuda:0')]
Traceback (most recent call last):
  File "/home/is/kaifan-l/private_room/proj-repos/prompt-for-long-text-summarization/run.py", line 252, in <module>
    main()
  File "/home/is/kaifan-l/private_room/proj-repos/prompt-for-long-text-summarization/run.py", line 230, in main
    summarization_metric.calculate_metrics(
  File "/home/is/kaifan-l/private_room/proj-repos/prompt-for-long-text-summarization/utils/evaluate_utils.py", line 57, in calculate_metrics
    generated_tokens = torch.tensor(generated_tokens)
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
TypeError: only integer tensors of a single element can be converted to an index
