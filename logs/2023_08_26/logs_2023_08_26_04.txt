/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:549: FutureWarning: The class `PretrainedBartModel` has been depreciated, please use `BartPreTrainedModel` instead.
  warnings.warn(
[nltk_data] Downloading package punkt to
[nltk_data]     /home/is/kaifan-l/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
trainable params: 184,320 || all params: 139,604,736 || trainable%: 0.13202990477343118
BartPrefixForConditionalGeneration(
  (model): PeftModelForSeq2SeqLM(
    (base_model): BartForConditionalGeneration(
      (model): BartModel(
        (shared): Embedding(50265, 768, padding_idx=1)
        (encoder): BartEncoder(
          (embed_tokens): Embedding(50265, 768, padding_idx=1)
          (embed_positions): BartLearnedPositionalEmbedding(1026, 768)
          (layers): ModuleList(
            (0-5): 6 x BartEncoderLayer(
              (self_attn): BartAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (activation_fn): GELUActivation()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
          (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (decoder): BartDecoder(
          (embed_tokens): Embedding(50265, 768, padding_idx=1)
          (embed_positions): BartLearnedPositionalEmbedding(1026, 768)
          (layers): ModuleList(
            (0-5): 6 x BartDecoderLayer(
              (self_attn): BartAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (activation_fn): GELUActivation()
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (encoder_attn): BartAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
          (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (lm_head): Linear(in_features=768, out_features=50265, bias=False)
    )
    (prompt_encoder): ModuleDict(
      (default): PrefixEncoder(
        (embedding): Embedding(20, 9216)
      )
    )
    (word_embeddings): Embedding(50265, 768, padding_idx=1)
  )
)
Start training ...
cuda
================================== epoch 0 ==================================
  0%|          | 0/36 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  3%|▎         | 1/36 [00:01<00:54,  1.55s/it]  6%|▌         | 2/36 [00:02<00:49,  1.44s/it]  8%|▊         | 3/36 [00:04<00:48,  1.47s/it] 11%|█         | 4/36 [00:06<00:50,  1.58s/it] 14%|█▍        | 5/36 [00:07<00:50,  1.64s/it] 17%|█▋        | 6/36 [00:09<00:45,  1.51s/it] 19%|█▉        | 7/36 [00:10<00:44,  1.54s/it] 22%|██▏       | 8/36 [00:12<00:44,  1.58s/it] 25%|██▌       | 9/36 [00:14<00:42,  1.58s/it] 28%|██▊       | 10/36 [00:15<00:42,  1.63s/it] 31%|███       | 11/36 [00:17<00:42,  1.70s/it] 33%|███▎      | 12/36 [00:19<00:40,  1.70s/it] 36%|███▌      | 13/36 [00:20<00:36,  1.58s/it] 39%|███▉      | 14/36 [00:22<00:34,  1.55s/it] 42%|████▏     | 15/36 [00:23<00:34,  1.65s/it] 44%|████▍     | 16/36 [00:25<00:31,  1.58s/it] 47%|████▋     | 17/36 [00:27<00:31,  1.66s/it] 50%|█████     | 18/36 [00:28<00:28,  1.59s/it] 53%|█████▎    | 19/36 [00:30<00:28,  1.66s/it] 56%|█████▌    | 20/36 [00:32<00:27,  1.72s/it] 58%|█████▊    | 21/36 [00:34<00:26,  1.77s/it] 61%|██████    | 22/36 [00:35<00:24,  1.72s/it] 64%|██████▍   | 23/36 [00:37<00:21,  1.68s/it] 67%|██████▋   | 24/36 [00:39<00:20,  1.69s/it] 69%|██████▉   | 25/36 [00:40<00:17,  1.59s/it] 72%|███████▏  | 26/36 [00:42<00:16,  1.64s/it] 75%|███████▌  | 27/36 [00:43<00:14,  1.60s/it] 78%|███████▊  | 28/36 [00:45<00:13,  1.68s/it] 81%|████████  | 29/36 [00:47<00:11,  1.67s/it] 83%|████████▎ | 30/36 [00:49<00:10,  1.71s/it] 86%|████████▌ | 31/36 [00:50<00:08,  1.75s/it] 89%|████████▉ | 32/36 [00:52<00:06,  1.69s/it] 92%|█████████▏| 33/36 [00:54<00:05,  1.73s/it] 94%|█████████▍| 34/36 [00:56<00:03,  1.72s/it] 97%|█████████▋| 35/36 [00:57<00:01,  1.73s/it]100%|██████████| 36/36 [00:59<00:00,  1.64s/it]100%|██████████| 36/36 [00:59<00:00,  1.64s/it]
GPU Memory before entering the train : 533
GPU Memory consumed at the end of the train (end-begin): 4141
GPU Peak Memory consumed during the train (max-begin): 20992
GPU Total Peak Memory consumed during the train (max): 21525
CPU Memory before entering the train : 1884
CPU Memory consumed at the end of the train (end-begin): 436
CPU Peak Memory consumed during the train (max-begin): 435
CPU Total Peak Memory consumed during the train (max): 2319
epoch=0: train_ppl=tensor(7178.3799, device='cuda:0') train_epoch_loss=tensor(8.8788, device='cuda:0')


Start evaluating ...
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:01<00:20,  1.27s/it] 12%|█▏        | 2/17 [00:02<00:21,  1.43s/it] 18%|█▊        | 3/17 [00:04<00:19,  1.40s/it] 24%|██▎       | 4/17 [00:05<00:18,  1.46s/it] 29%|██▉       | 5/17 [00:07<00:16,  1.40s/it] 35%|███▌      | 6/17 [00:08<00:14,  1.31s/it] 41%|████      | 7/17 [00:09<00:13,  1.31s/it] 47%|████▋     | 8/17 [00:10<00:12,  1.37s/it] 53%|█████▎    | 9/17 [00:12<00:11,  1.39s/it] 59%|█████▉    | 10/17 [00:13<00:10,  1.44s/it] 65%|██████▍   | 11/17 [00:15<00:08,  1.48s/it] 71%|███████   | 12/17 [00:16<00:07,  1.47s/it] 76%|███████▋  | 13/17 [00:18<00:05,  1.40s/it] 82%|████████▏ | 14/17 [00:19<00:04,  1.42s/it] 88%|████████▊ | 15/17 [00:21<00:02,  1.39s/it] 94%|█████████▍| 16/17 [00:22<00:01,  1.41s/it]100%|██████████| 17/17 [00:23<00:00,  1.18s/it]100%|██████████| 17/17 [00:23<00:00,  1.36s/it]
epoch=0: eval_ppl=tensor(1097.7078, device='cuda:0') eval_epoch_loss=tensor(7.0010, device='cuda:0')
Downloading builder script:   0%|          | 0.00/8.15k [00:00<?, ?B/s]Downloading builder script: 100%|██████████| 8.15k/8.15k [00:00<00:00, 18.5MB/s]
Traceback (most recent call last):
  File "/home/is/kaifan-l/private_room/proj-repos/prompt-for-long-text-summarization/run.py", line 251, in <module>
    if __name__ == "__main__":
        ^^^^^^
  File "/home/is/kaifan-l/private_room/proj-repos/prompt-for-long-text-summarization/run.py", line 229, in main
    summarization_metric = evaluate_utils.SummarizationMetric()
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/is/kaifan-l/private_room/proj-repos/prompt-for-long-text-summarization/utils/evaluate_utils.py", line 16, in __init__
    self.bleu_metrics = evaluate.load("sacrebleu")
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/evaluate/loading.py", line 731, in load
    evaluation_module = evaluation_module_factory(
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/evaluate/loading.py", line 680, in evaluation_module_factory
    raise e1 from None
  File "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/evaluate/loading.py", line 639, in evaluation_module_factory
    ).get_module()
      ^^^^^^^^^^^^
  File "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/evaluate/loading.py", line 489, in get_module
    local_imports = _download_additional_modules(
                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/evaluate/loading.py", line 265, in _download_additional_modules
    raise ImportError(
ImportError: To be able to use evaluate-metric/sacrebleu, you need to install the following dependencies['sacrebleu'] using 'pip install sacrebleu' for instance'
