/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:549: FutureWarning: The class `PretrainedBartModel` has been depreciated, please use `BartPreTrainedModel` instead.
  warnings.warn(
[nltk_data] Downloading package punkt to
[nltk_data]     /home/is/kaifan-l/nltk_data...
[nltk_data]   Package punkt is already up-to-date!
/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/torch/cuda/memory.py:303: FutureWarning: torch.cuda.reset_max_memory_allocated now calls torch.cuda.reset_peak_memory_stats, which resets /all/ peak memory stats.
  warnings.warn(
trainable params: 184,320 || all params: 139,604,736 || trainable%: 0.13202990477343118
BartPrefixForConditionalGeneration(
  (model): PeftModelForSeq2SeqLM(
    (base_model): BartForConditionalGeneration(
      (model): BartModel(
        (shared): Embedding(50265, 768, padding_idx=1)
        (encoder): BartEncoder(
          (embed_tokens): Embedding(50265, 768, padding_idx=1)
          (embed_positions): BartLearnedPositionalEmbedding(1026, 768)
          (layers): ModuleList(
            (0-5): 6 x BartEncoderLayer(
              (self_attn): BartAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (activation_fn): GELUActivation()
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
          (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
        (decoder): BartDecoder(
          (embed_tokens): Embedding(50265, 768, padding_idx=1)
          (embed_positions): BartLearnedPositionalEmbedding(1026, 768)
          (layers): ModuleList(
            (0-5): 6 x BartDecoderLayer(
              (self_attn): BartAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (activation_fn): GELUActivation()
              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (encoder_attn): BartAttention(
                (k_proj): Linear(in_features=768, out_features=768, bias=True)
                (v_proj): Linear(in_features=768, out_features=768, bias=True)
                (q_proj): Linear(in_features=768, out_features=768, bias=True)
                (out_proj): Linear(in_features=768, out_features=768, bias=True)
              )
              (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
              (fc1): Linear(in_features=768, out_features=3072, bias=True)
              (fc2): Linear(in_features=3072, out_features=768, bias=True)
              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
            )
          )
          (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
        )
      )
      (lm_head): Linear(in_features=768, out_features=50265, bias=False)
    )
    (prompt_encoder): ModuleDict(
      (default): PrefixEncoder(
        (embedding): Embedding(20, 9216)
      )
    )
    (word_embeddings): Embedding(50265, 768, padding_idx=1)
  )
)
Start training ...
cuda
================================== epoch 0 ==================================
  0%|          | 0/36 [00:00<?, ?it/s]You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.
  3%|▎         | 1/36 [00:01<00:54,  1.56s/it]  6%|▌         | 2/36 [00:02<00:48,  1.44s/it]  8%|▊         | 3/36 [00:04<00:48,  1.46s/it] 11%|█         | 4/36 [00:06<00:50,  1.58s/it] 14%|█▍        | 5/36 [00:07<00:50,  1.63s/it] 17%|█▋        | 6/36 [00:09<00:45,  1.51s/it] 19%|█▉        | 7/36 [00:10<00:44,  1.54s/it] 22%|██▏       | 8/36 [00:12<00:44,  1.58s/it] 25%|██▌       | 9/36 [00:13<00:42,  1.56s/it] 28%|██▊       | 10/36 [00:15<00:42,  1.63s/it] 31%|███       | 11/36 [00:17<00:42,  1.70s/it] 33%|███▎      | 12/36 [00:19<00:40,  1.69s/it] 36%|███▌      | 13/36 [00:20<00:36,  1.57s/it] 39%|███▉      | 14/36 [00:22<00:34,  1.55s/it] 42%|████▏     | 15/36 [00:23<00:34,  1.64s/it] 44%|████▍     | 16/36 [00:25<00:31,  1.58s/it] 47%|████▋     | 17/36 [00:27<00:31,  1.67s/it] 50%|█████     | 18/36 [00:28<00:28,  1.60s/it] 53%|█████▎    | 19/36 [00:30<00:28,  1.66s/it] 56%|█████▌    | 20/36 [00:32<00:27,  1.73s/it] 58%|█████▊    | 21/36 [00:34<00:26,  1.77s/it] 61%|██████    | 22/36 [00:35<00:24,  1.72s/it] 64%|██████▍   | 23/36 [00:37<00:21,  1.68s/it] 67%|██████▋   | 24/36 [00:39<00:20,  1.68s/it] 69%|██████▉   | 25/36 [00:40<00:17,  1.58s/it] 72%|███████▏  | 26/36 [00:42<00:16,  1.64s/it] 75%|███████▌  | 27/36 [00:43<00:14,  1.60s/it] 78%|███████▊  | 28/36 [00:45<00:13,  1.68s/it] 81%|████████  | 29/36 [00:47<00:11,  1.66s/it] 83%|████████▎ | 30/36 [00:49<00:10,  1.71s/it] 86%|████████▌ | 31/36 [00:50<00:08,  1.75s/it] 89%|████████▉ | 32/36 [00:52<00:06,  1.69s/it] 92%|█████████▏| 33/36 [00:54<00:05,  1.72s/it] 94%|█████████▍| 34/36 [00:55<00:03,  1.71s/it] 97%|█████████▋| 35/36 [00:57<00:01,  1.74s/it]100%|██████████| 36/36 [00:59<00:00,  1.64s/it]100%|██████████| 36/36 [00:59<00:00,  1.64s/it]
GPU Memory before entering the train : 533
GPU Memory consumed at the end of the train (end-begin): 4141
GPU Peak Memory consumed during the train (max-begin): 20992
GPU Total Peak Memory consumed during the train (max): 21525
CPU Memory before entering the train : 1886
CPU Memory consumed at the end of the train (end-begin): 431
CPU Peak Memory consumed during the train (max-begin): 431
CPU Total Peak Memory consumed during the train (max): 2317
epoch=0: train_ppl=tensor(7178.3799, device='cuda:0') train_epoch_loss=tensor(8.8788, device='cuda:0')


Start evaluating ...
  0%|          | 0/17 [00:00<?, ?it/s]  6%|▌         | 1/17 [00:01<00:20,  1.25s/it] 12%|█▏        | 2/17 [00:02<00:21,  1.42s/it] 18%|█▊        | 3/17 [00:04<00:19,  1.39s/it] 24%|██▎       | 4/17 [00:05<00:18,  1.44s/it] 29%|██▉       | 5/17 [00:06<00:16,  1.39s/it] 35%|███▌      | 6/17 [00:08<00:14,  1.30s/it] 41%|████      | 7/17 [00:09<00:13,  1.30s/it] 47%|████▋     | 8/17 [00:10<00:12,  1.37s/it] 53%|█████▎    | 9/17 [00:12<00:11,  1.39s/it] 59%|█████▉    | 10/17 [00:13<00:10,  1.43s/it] 65%|██████▍   | 11/17 [00:15<00:08,  1.46s/it] 71%|███████   | 12/17 [00:16<00:07,  1.47s/it] 76%|███████▋  | 13/17 [00:18<00:05,  1.39s/it] 82%|████████▏ | 14/17 [00:19<00:04,  1.42s/it] 88%|████████▊ | 15/17 [00:20<00:02,  1.39s/it] 94%|█████████▍| 16/17 [00:22<00:01,  1.40s/it]100%|██████████| 17/17 [00:22<00:00,  1.17s/it]100%|██████████| 17/17 [00:22<00:00,  1.35s/it]
epoch=0: eval_ppl=tensor(1097.7078, device='cuda:0') eval_epoch_loss=tensor(7.0010, device='cuda:0')
  0%|          | 0/15 [00:00<?, ?it/s]  0%|          | 0/15 [00:34<?, ?it/s]
model_outputs:   in the years for the first time in the history of the first in the first years of the second to the first to the second in the second year of the Second year in the last one, the first two years in the third in the United states of the last decade in the U.S. in the 1970s, and it was the first one. In the second decade of the twentieth in the twentieth century, it was both the first and the second. It was the second time. The second year for the second, in the fourth in the fifth in the sixth in the seventh in the eighth in the ninth in the tenth in the next generation for the fifth year for a second-quarter to the fourth and a fifth in its first-quarter for the fourth-quarter in the Fifth in the US for the United States.In the first generation in the Second in the 1960s in the 1990s.The second generation of the United to the United from the United in the 1980s to the third generation from the second generation to the fifth generation, to the sixth generation to a generation.The fourth generation generation generation, a generation, and a generation generation - a generation - and a young generation in America for the last generation generation and a child in the
Traceback (most recent call last):
  File "/home/is/kaifan-l/private_room/proj-repos/prompt-for-long-text-summarization/run.py", line 252, in <module>
    main()
  File "/home/is/kaifan-l/private_room/proj-repos/prompt-for-long-text-summarization/run.py", line 230, in main
    summarization_metric.calculate_metrics(
  File "/home/is/kaifan-l/private_room/proj-repos/prompt-for-long-text-summarization/utils/evaluate_utils.py", line 54, in calculate_metrics
    generated_tokens = accelerator.pad_across_processes(
                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/accelerate/accelerator.py", line 2176, in pad_across_processes
    return pad_across_processes(tensor, dim=dim, pad_index=pad_index, pad_first=pad_first)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/accelerate/utils/operations.py", line 345, in wrapper
    return function(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/accelerate/utils/operations.py", line 544, in pad_across_processes
    return recursively_apply(
           ^^^^^^^^^^^^^^^^^^
  File "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/accelerate/utils/operations.py", line 130, in recursively_apply
    raise TypeError(
TypeError: Unsupported types (<class 'NoneType'>) passed to `_pad_across_processes`. Only nested list/tuple/dicts of objects that are valid for `is_torch_tensor` should be passed.
