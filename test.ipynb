{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ['WANDB_DIR'] = os.getcwd() + '/wandb/'\n",
    "os.environ['WANDB_CACHE_DIR'] = os.getcwd() + '/wandb/.cache/'\n",
    "os.environ['WANDB_CONFIG_DIR'] = os.getcwd() + '/wandb/.config/'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 初始化wandb\n",
    "wandb.init(\n",
    "    entity='kaifan-li',\n",
    "    project=\"my_project\"\n",
    ")\n",
    "\n",
    "# 构建模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i in range(100):\n",
    "        inputs = torch.randn(32, 10)  # 随机生成输入数据\n",
    "        labels = torch.randn(32, 1)   # 随机生成标签\n",
    "        \n",
    "        # 正向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # 记录训练过程和指标\n",
    "    avg_loss = running_loss / 100\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": avg_loss})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = \"Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(long_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/is/kaifan-l/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024])\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "\n",
    "import numpy as np\n",
    "import psutil\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "\n",
    "from peft import PrefixTuningConfig, TaskType, get_peft_model\n",
    "\n",
    "accelerator = Accelerator() # device_placement=\"cuda:0\"\n",
    "model_name_or_path = \"facebook/bart-base\"\n",
    "dataset_name = \"cnn_dailymail\"\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    num_virtual_tokens=20,\n",
    ")\n",
    "text_column = 'article'\n",
    "label_column = 'highlights'\n",
    "lr = 3e-3\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "do_test = True\n",
    "set_seed(seed)\n",
    "cnn_dataset = load_dataset(dataset_name, \"3.0.0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "target_max_length = 128\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[text_column]\n",
    "    targets = examples[label_column]\n",
    "    model_inputs = tokenizer(inputs, truncation=True) # 这里暂时不padding\n",
    "    targets = tokenizer(\n",
    "        targets,\n",
    "        max_length=target_max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    targets = targets['input_ids']\n",
    "    targets[targets == tokenizer.pad_token_id] = -100\n",
    "    model_inputs['labels'] = targets\n",
    "    \n",
    "    return model_inputs\n",
    "with accelerator.main_process_first():\n",
    "    cnn_dataset = cnn_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        remove_columns=cnn_dataset[\"train\"].column_names,\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "accelerator.wait_for_everyone()\n",
    "train_size = int(len(cnn_dataset[\"train\"]) * 0.001)\n",
    "eval_size = int(len(cnn_dataset[\"validation\"]) * 0.001)\n",
    "test_size = int(len(cnn_dataset[\"test\"]) * 0.001)\n",
    "\n",
    "# 从打乱后的数据集中随机抽取指定数量的数据\n",
    "train_dataset = cnn_dataset[\"train\"].shuffle(seed=42).select(range(train_size))\n",
    "eval_dataset = cnn_dataset[\"validation\"].shuffle(seed=42).select(range(eval_size))\n",
    "test_dataset = cnn_dataset[\"test\"].shuffle(seed=42).select(range(test_size))\n",
    "def collate_fn(examples):\n",
    "    return tokenizer.pad(examples, padding='longest', return_tensors='pt')\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True, # 将数据加载到固定的内存中，可以加速数据加载\n",
    ")\n",
    "\n",
    "for num, batch in enumerate(train_dataloader):\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import logging\n",
    "import copy\n",
    "import math\n",
    "from typing import List, Optional, Tuple, Union, Iterable\n",
    "\n",
    "from transformers import (\n",
    "    BartPretrainedModel,\n",
    "    BartForConditionalGeneration, \n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "from transformers import BartConfig, T5Config\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "from model.prefix_encoder import PrefixEncoder\n",
    "from peft import PrefixTuningConfig, TaskType, get_peft_model\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "# from model.summarization import BartPrefixForConditionalGeneration\n",
    "checkpoint = 'facebook/bart-base'\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    num_virtual_tokens=20,\n",
    ")\n",
    "model = BartPrefixForConditionalGeneration(    \n",
    "    checkpoint=checkpoint,\n",
    "    config=custom_config,\n",
    "    peft_config=peft_config\n",
    ")\n",
    "model.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, batch in enumerate(train_dataloader):\n",
    "    input_ids = batch[\"input_ids\"].to(model.device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "    labels = batch[\"labels\"].to(model.device)\n",
    "    \n",
    "    print(\"input_ids.shape:\", input_ids.shape)\n",
    "    print(\"attention_mask.shape:\", attention_mask.shape)\n",
    "    print(\"labels.shape:\", labels.shape)\n",
    "\n",
    "    out = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels,\n",
    "    )\n",
    "    print(out.keys())\n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        # attention_mask=attention_mask,\n",
    "        min_length=0,\n",
    "        max_length=142,\n",
    "        num_beams=4\n",
    "    )\n",
    "    # print(\"out:\", tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_tokens = torch.arange(20).long()\n",
    "prefix_tokens\n",
    "prefix_tokens = prefix_tokens.unsqueeze(0)\n",
    "prefix_tokens\n",
    "prefix_tokens = prefix_tokens.expand(32, -1)\n",
    "prefix_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
    "segment_size = 20\n",
    "def get_full_padding_segment():\n",
    "    padding_segment = [tokenizer.pad_token_id for _ in range(segment_size)]\n",
    "    return padding_segment\n",
    "\n",
    "test = get_full_padding_segment()\n",
    "test\n",
    "[test]\n",
    "[[12312414]] + [test] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceil = math.ceil(1024/3)\n",
    "ceil, 1024//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import PromptBartConfig\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModel\n",
    "from model import BartPrefixForConditionalGeneration\n",
    "from peft import PrefixTuningConfig\n",
    "\n",
    "bart_config = AutoConfig.from_pretrained('facebook/bart-base')\n",
    "custom_config = PromptBartConfig(**bart_config.to_dict())\n",
    "peft_config = PrefixTuningConfig.from_pretrained(\"kaifanli/prefix_rmt_bart_cnn-dm\")\n",
    "model = BartPrefixForConditionalGeneration(\n",
    "    checkpoint='facebook/bart-base',\n",
    "    config=custom_config,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_config = AutoConfig.from_pretrained('facebook/bart-base')\n",
    "custom_config = PromptBartConfig(**bart_config.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "bart_config = AutoConfig.from_pretrained('facebook/bart-base')\n",
    "custom_config = PromptBartConfig(**bart_config.to_dict())\n",
    "custom_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_model = BartPrefixForConditionalGeneration(custom_config, tokenizer_path='facebook/bart-base')\n",
    "pretrained_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "bart_model.model.load_state_dict(pretrained_model.state_dict())\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    num_virtual_tokens=20,\n",
    ")\n",
    "bart_model.model = get_peft_model(bart_model.model, peft_config)\n",
    "bart_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_model.model.save_pretrained('./test_peft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = BartPrefixForConditionalGeneration(custom_config, tokenizer_path='facebook/bart-base')\n",
    "pretrained_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "test_model.model.load_state_dict(pretrained_model.state_dict())\n",
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "test_model.model = PeftModel.from_pretrained(test_model.model, './test_peft')\n",
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_seg = torch.tensor([1 for _ in range(5)])\n",
    "non_empyty_mask = torch.tensor([True, False, True, True, True])\n",
    "padding_seg[non_empyty_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs = []\n",
    "app = torch.tensor([1,2,3])\n",
    "model_outputs.append(app)\n",
    "model_outputs[0] = torch.tensor([1,2,3])\n",
    "model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs = [1]\n",
    "not len(model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\ntest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "generated_tokens = torch.tensor([[1,2,3], [4,5,6], [7,8,9]])\n",
    "target_max_length = 5\n",
    "generated_tokens = torch.stack([F.pad(t, (0, target_max_length - t.size(0)), value=1) for t in generated_tokens])\n",
    "generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "# Encode your input text into input_ids\n",
    "input_text = \"Because the purpose of prompt construction is to find a method that allows an LM to effectively perform a task, rather than being for human consumption, it is not necessary to limit the prompt to human-interpretable natural language. Because of this, there are also methods that examine continuous prompts (a.k.a. soft prompts) that perform prompting directly in the embedding space of the model. Specifically, continuous prompts remove two constraints: (1) relax the constraint that the embeddings of template words be the embeddings of natural language (e.g., English) words. (2) Remove the restriction that the template is parameterized by the pre-trained LM’s parameters. Instead, templates have their own parameters that can be tuned based on training data from the downstream task. We highlight several representative methods below\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "print(input_ids)\n",
    "# Generate output\n",
    "out = bart_model.generate(input_ids, max_length=40)\n",
    "print(out)\n",
    "# Decode the generated output\n",
    "decoded_output = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartRMTForConditionalGeneration(\n",
       "  (model): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): Embedding(50265, 768, padding_idx=1)\n",
       "      (encoder): BartEncoder(\n",
       "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import BartRMTForConditionalGeneration\n",
    "from config import PromptBartConfig\n",
    "\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "from transformers import AutoConfig\n",
    "bart_config = AutoConfig.from_pretrained('facebook/bart-base')\n",
    "custom_config = PromptBartConfig(**bart_config.to_dict())\n",
    "\n",
    "test_model = BartRMTForConditionalGeneration(custom_config, tokenizer_name_or_path='facebook/bart-base')\n",
    "test_model.model.load_state_dict(bart_model.state_dict())\n",
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=768, out_features=768, bias=True)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model.model.model.encoder.layers[0].self_attn.k_proj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.1010e-03, -5.4346e-02,  3.3621e-03,  2.0014e-02, -5.4740e-03,\n",
       "        -2.2844e-02,  8.1586e-03,  1.0935e-02, -8.9161e-03,  8.4671e-03,\n",
       "         1.2857e-02, -1.1055e-03, -1.7958e-02, -3.0604e-02,  1.3681e-02,\n",
       "        -7.4673e-03,  1.4274e-03,  3.6727e-03,  3.1749e-02, -1.0873e-03,\n",
       "         1.3540e-02,  8.3294e-03, -8.8273e-03,  6.1580e-03,  1.0899e-02,\n",
       "        -2.4739e-03, -1.4442e-02,  4.5778e-03,  1.7585e-03, -2.3869e-03,\n",
       "        -2.3175e-02,  1.7471e-02,  9.2127e-05, -7.8743e-03,  1.9892e-02,\n",
       "         3.3545e-03, -2.5371e-02,  2.1744e-02, -4.6021e-03, -8.5722e-03,\n",
       "        -2.3547e-02,  1.4463e-02,  4.6816e-03, -1.5214e-02,  5.4567e-03,\n",
       "         1.1418e-02, -8.2172e-03,  3.1732e-03, -7.8813e-03, -8.3404e-03,\n",
       "         2.2891e-02, -9.9614e-03,  6.8114e-04,  2.0809e-02,  3.7695e-02,\n",
       "         2.1046e-02,  2.0017e-02, -1.2675e-02,  2.2624e-04, -1.3586e-02,\n",
       "         2.3958e-02,  3.3551e-02,  3.0247e-02,  1.4323e-02, -9.8782e-03,\n",
       "         6.8953e-03, -1.9203e-02,  1.9234e-02, -9.4810e-03, -1.9539e-02,\n",
       "        -1.4641e-02,  1.3573e-02,  3.3305e-03,  2.1150e-02,  4.9548e-04,\n",
       "         2.5713e-03,  4.9624e-02, -2.0774e-02,  2.4452e-02,  1.4955e-03,\n",
       "         3.9376e-02, -5.0843e-03,  2.8069e-02, -2.4724e-02, -2.0300e-03,\n",
       "         1.1132e-02,  2.9539e-03, -3.4122e-02,  3.4985e-02, -7.5394e-03,\n",
       "         1.9173e-02,  1.4404e-02,  3.3125e-03, -6.6224e-03,  1.1337e-02,\n",
       "         1.8355e-02,  5.1295e-03, -1.9857e-02, -5.1633e-03, -1.3831e-02,\n",
       "         7.8444e-03,  5.6568e-02,  1.3555e-02,  5.4727e-03, -1.4080e-03,\n",
       "        -1.7866e-02, -1.3838e-03, -1.5070e-02, -3.9744e-03, -1.0218e-02,\n",
       "         1.5736e-02, -4.5873e-03,  1.2054e-02,  2.0516e-02, -4.2480e-02,\n",
       "        -2.2683e-02, -3.2084e-02,  4.7252e-03, -2.4144e-03,  3.3921e-03,\n",
       "        -4.9446e-03, -1.0460e-02, -1.2082e-02,  2.0245e-02,  7.4597e-03,\n",
       "         6.9809e-04,  2.5283e-02,  3.7411e-02, -1.8960e-02,  3.6405e-02,\n",
       "        -2.1613e-02,  3.6723e-02,  3.0307e-02,  9.2545e-03, -1.9718e-02,\n",
       "        -5.5440e-03, -2.1132e-02, -3.1777e-03, -2.1431e-02,  6.9234e-03,\n",
       "        -4.5922e-03,  1.7676e-02,  4.5864e-03,  5.1277e-03,  2.3427e-02,\n",
       "        -1.9391e-02,  1.4087e-02,  1.4011e-03, -2.6158e-02, -2.2604e-02,\n",
       "        -3.0659e-03, -4.5126e-04, -1.8614e-02,  1.2093e-02, -2.4288e-02,\n",
       "         1.9467e-02,  2.5440e-02, -6.8373e-03,  4.8278e-03, -3.4477e-03,\n",
       "         5.8447e-03,  1.4352e-02,  2.6854e-02, -1.0864e-03, -5.1172e-03,\n",
       "         1.6535e-02,  1.3286e-02, -2.0407e-02, -1.9421e-02, -2.0593e-02,\n",
       "         1.6971e-02, -3.8544e-02, -1.2788e-02,  3.6844e-02, -1.6840e-02,\n",
       "        -1.7732e-02, -5.3317e-04, -3.2404e-02,  8.4356e-03, -4.4733e-03,\n",
       "        -3.2720e-02, -7.0890e-03, -5.7640e-03, -2.0644e-02,  5.5747e-02,\n",
       "         2.0419e-02,  3.3977e-02, -2.2671e-02, -5.7668e-03,  9.6351e-03,\n",
       "        -4.9674e-03,  1.9052e-02,  2.8485e-02, -7.2040e-03, -2.5785e-02,\n",
       "         1.2895e-02,  2.0971e-02,  1.9168e-02, -1.7481e-02, -4.6445e-03,\n",
       "         1.9813e-02, -3.2884e-02, -1.4886e-02, -6.8975e-03, -1.4235e-02,\n",
       "         4.3689e-02,  1.2943e-02, -1.8008e-02,  1.5409e-03, -3.4990e-02,\n",
       "        -3.2566e-02,  1.4899e-02,  3.1538e-02, -5.3536e-03, -3.3042e-03,\n",
       "        -1.4255e-02, -1.4655e-02, -9.5598e-03, -2.2773e-02,  1.4230e-02,\n",
       "         1.7774e-02, -2.3387e-02,  1.1222e-02, -2.0001e-03, -1.1009e-02,\n",
       "        -3.4804e-04,  1.2414e-02, -3.2271e-02, -3.4811e-02, -3.8940e-02,\n",
       "         5.7492e-03,  1.5333e-03, -1.3320e-02,  2.6008e-02, -6.0631e-03,\n",
       "        -9.6708e-03, -1.6025e-02, -3.6102e-02, -8.0330e-04, -2.6772e-02,\n",
       "        -2.7980e-02,  1.2309e-02, -1.5311e-03, -3.4171e-02, -3.8342e-02,\n",
       "        -1.8900e-02, -4.3873e-03, -2.6297e-02, -3.1267e-02,  1.1813e-04,\n",
       "        -6.0445e-03,  2.5799e-02, -1.0741e-02, -1.9848e-02, -1.0437e-02,\n",
       "         3.3435e-02,  3.2764e-02,  4.6417e-02,  2.9486e-03, -1.2108e-02,\n",
       "        -2.7739e-02,  6.6059e-03,  2.3286e-02,  6.1905e-02, -4.7037e-03,\n",
       "         3.5335e-03,  1.1479e-02, -2.7852e-02, -3.7367e-02, -7.7530e-03,\n",
       "         1.6490e-02, -2.0537e-02, -1.0399e-02, -1.8117e-02, -3.3055e-02,\n",
       "         1.3599e-02,  1.4349e-02, -3.2713e-03,  2.1623e-02, -2.7083e-02,\n",
       "         2.9424e-04, -2.3716e-02, -2.0769e-02,  3.0288e-02,  1.1074e-02,\n",
       "         2.6261e-02, -9.2326e-03,  4.7270e-02,  5.3718e-03, -1.1152e-02,\n",
       "        -3.3149e-02,  3.4270e-02, -5.5443e-03,  8.1444e-03,  1.2541e-02,\n",
       "         3.4397e-02,  7.2973e-03, -9.7830e-03,  1.9894e-02,  8.9791e-03,\n",
       "        -2.5690e-02, -8.8697e-03,  4.9182e-03, -3.1011e-03,  2.0487e-02,\n",
       "         1.0605e-02, -2.9298e-02, -2.9068e-02,  5.3399e-03,  2.2609e-02,\n",
       "        -7.4837e-04, -9.2199e-03, -7.9927e-03, -1.0603e-02,  7.3136e-03,\n",
       "         1.7751e-02, -1.9004e-03, -9.8671e-03, -1.8412e-02,  3.4492e-03,\n",
       "        -3.1096e-03,  2.9274e-02, -9.0068e-03, -5.9346e-03,  1.2709e-02,\n",
       "        -4.0902e-02,  6.3964e-03, -1.4958e-02,  3.3533e-02, -3.2431e-02,\n",
       "        -4.5200e-02, -2.4824e-02,  8.8715e-03,  7.5653e-04,  2.0024e-02,\n",
       "        -6.4139e-03, -5.1563e-03,  1.0662e-03, -7.1428e-04,  1.4971e-02,\n",
       "         1.9721e-02,  2.1441e-02,  1.4946e-02, -1.9191e-03,  7.1162e-03,\n",
       "        -3.9283e-03,  4.5818e-02,  3.0582e-02, -3.6063e-02,  8.1777e-03,\n",
       "         8.3104e-03, -1.0389e-03, -8.2002e-04, -1.9539e-02, -1.3936e-02,\n",
       "        -1.1295e-02,  9.9579e-03, -1.4326e-02, -2.4452e-03,  4.7079e-02,\n",
       "         4.9879e-02,  4.1174e-03, -6.8819e-03,  5.6902e-03, -2.9869e-04,\n",
       "        -1.0711e-02, -1.0388e-02,  8.4616e-03, -1.3451e-03,  3.7639e-02,\n",
       "        -1.0389e-02, -7.8714e-03,  2.4094e-05, -5.7460e-05, -2.2744e-02,\n",
       "         1.3011e-02, -2.5212e-02, -8.1460e-03, -1.8263e-03,  2.9605e-02,\n",
       "         2.0990e-03, -4.8201e-02,  2.0092e-02, -4.3214e-02,  3.3527e-03,\n",
       "         4.3269e-02,  2.2604e-02,  1.4628e-03, -3.1572e-02, -2.1128e-02,\n",
       "         1.4477e-02, -4.8634e-03,  4.2598e-02,  2.7969e-03,  7.0727e-03,\n",
       "        -7.2971e-04,  2.6588e-02,  8.7303e-03, -1.0412e-02,  1.6589e-02,\n",
       "         3.6183e-02,  1.5299e-02, -2.1251e-02, -1.2364e-02,  2.0326e-02,\n",
       "        -9.4278e-03, -1.3805e-02, -1.4249e-02, -3.8285e-02,  3.5628e-02,\n",
       "        -3.8860e-03, -4.1517e-02, -4.6495e-03, -1.5034e-03, -2.5816e-02,\n",
       "        -1.4758e-02, -2.0487e-02,  8.1740e-03,  5.0654e-03, -3.4853e-04,\n",
       "         2.0066e-03,  2.6574e-02,  1.4955e-02,  1.0135e-02,  5.8793e-03,\n",
       "         7.4320e-03,  2.8283e-02, -4.4336e-03, -1.3204e-02,  3.1230e-02,\n",
       "        -3.8325e-02, -3.6824e-02,  1.2167e-02, -3.6153e-02, -6.4243e-03,\n",
       "        -9.0984e-04, -2.7457e-02,  4.0394e-02, -9.5404e-03,  4.9521e-03,\n",
       "         4.2780e-02,  2.3063e-02,  4.3367e-03,  4.3394e-02, -4.3507e-02,\n",
       "         6.1703e-03,  1.4936e-02,  2.7080e-02,  1.3967e-02, -1.4611e-02,\n",
       "        -1.9940e-03,  1.7020e-02,  3.0999e-03, -1.2865e-03,  5.6459e-02,\n",
       "         1.8266e-02,  7.4334e-03,  9.8998e-04,  2.1773e-02, -5.7682e-03,\n",
       "        -3.7797e-02,  4.7609e-03, -1.0700e-03, -1.7711e-02,  2.3988e-03,\n",
       "         2.6180e-02,  1.1976e-02, -6.6105e-04, -2.3392e-03, -1.4867e-02,\n",
       "         1.3906e-02, -2.1566e-02, -1.1913e-02, -1.7054e-03,  1.4952e-02,\n",
       "        -1.0542e-02, -1.3938e-02,  1.7500e-02, -1.9637e-02, -8.8173e-03,\n",
       "         2.2494e-02,  2.2109e-02, -1.9617e-02, -5.9156e-03, -1.2608e-02,\n",
       "        -2.0149e-02, -5.0291e-02,  3.5336e-02, -6.2966e-03, -6.6021e-03,\n",
       "         5.7907e-03,  2.3739e-02,  2.7720e-02, -2.0470e-02, -1.6255e-02,\n",
       "         1.9650e-02, -1.9226e-02,  3.3661e-03,  1.6981e-02, -3.2638e-04,\n",
       "         2.1044e-02, -6.0678e-04, -2.8585e-02, -1.3126e-02, -1.6484e-02,\n",
       "        -3.1538e-02, -9.8693e-03, -2.7497e-02,  1.5178e-02, -2.0368e-02,\n",
       "        -4.8925e-02, -1.4435e-02,  2.7920e-02, -2.6256e-02,  3.0817e-03,\n",
       "        -4.3106e-03, -1.0768e-02, -7.4122e-03,  2.4877e-02, -2.5184e-02,\n",
       "        -1.9396e-03, -1.8663e-02,  1.4377e-02, -1.1063e-02, -3.4346e-02,\n",
       "         1.6218e-02,  2.1623e-02, -1.5229e-02, -5.8382e-02, -1.7449e-02,\n",
       "        -5.8650e-02, -3.9513e-02, -1.0542e-02,  9.5486e-03, -1.9539e-02,\n",
       "         4.2165e-02, -4.0826e-03,  1.2027e-02,  1.8179e-02,  3.4367e-03,\n",
       "         4.0310e-03,  3.2367e-03, -6.1698e-03, -1.9327e-03,  9.6323e-03,\n",
       "         1.5989e-02, -6.8415e-03, -5.6105e-02, -1.4487e-02,  1.5531e-02,\n",
       "        -7.1005e-03,  1.1736e-02, -1.1687e-02,  1.9415e-02, -1.8540e-02,\n",
       "         2.6486e-02,  7.7899e-04,  5.4941e-03, -4.3876e-02,  5.0862e-03,\n",
       "         1.9139e-02,  2.7737e-02,  1.3398e-03,  1.2730e-02, -1.4826e-02,\n",
       "         1.6329e-02, -3.4430e-03, -2.1701e-02,  1.9319e-03,  2.2819e-02,\n",
       "         1.4116e-02, -2.4999e-02, -3.9421e-03,  2.6979e-03,  3.1388e-02,\n",
       "        -7.4657e-03,  3.0980e-03,  5.2954e-03,  6.6523e-03, -1.7819e-02,\n",
       "        -1.6500e-03, -1.5836e-02, -1.9084e-02,  1.7754e-02,  4.2244e-03,\n",
       "        -5.0166e-03,  2.4163e-02,  3.8719e-02, -1.6979e-02, -2.3420e-02,\n",
       "         3.6102e-03, -8.8145e-03,  4.7700e-03, -2.8193e-02, -6.8863e-03,\n",
       "         5.2710e-02, -1.0598e-03,  5.1600e-03, -1.9598e-02, -6.1383e-04,\n",
       "         1.9108e-02, -6.3368e-03, -4.1925e-03, -3.8430e-02, -1.1915e-02,\n",
       "         7.3050e-03,  5.9251e-03, -2.2314e-02, -1.6802e-02, -1.4416e-02,\n",
       "        -1.8599e-02,  2.1548e-02, -3.2089e-02,  1.2801e-02,  3.4508e-03,\n",
       "         9.1345e-03,  3.6210e-02, -1.2429e-02,  1.2660e-03, -1.8376e-02,\n",
       "        -4.4037e-03,  1.4973e-02,  1.1458e-02,  4.5519e-04,  9.9172e-03,\n",
       "         2.3999e-02,  1.9342e-02,  8.1675e-03, -6.9261e-03,  1.0607e-02,\n",
       "         1.0034e-03, -9.4109e-03,  1.4982e-02,  5.0637e-02, -7.0057e-03,\n",
       "        -4.3698e-02, -3.1152e-03,  1.1337e-02, -1.9235e-02,  2.3043e-02,\n",
       "         2.4225e-02,  4.6567e-03, -2.5284e-02, -1.0789e-02,  7.5889e-03,\n",
       "         8.0026e-04,  2.5767e-03,  8.6299e-04,  2.7949e-02,  8.9887e-03,\n",
       "        -1.3177e-02,  3.0807e-02, -6.4777e-03,  2.0716e-02,  7.7882e-03,\n",
       "         1.5200e-03,  1.6457e-02,  1.1741e-02, -7.1915e-03,  8.5615e-03,\n",
       "         1.4513e-02,  2.7663e-03, -9.0910e-03, -2.1523e-02,  3.5135e-02,\n",
       "         1.7219e-02, -7.2633e-03,  2.4986e-02, -2.8099e-02, -2.5532e-02,\n",
       "        -3.6468e-03, -8.9572e-03, -8.2584e-03, -4.6749e-03, -3.4254e-03,\n",
       "         6.7149e-03, -2.2769e-02,  1.0617e-03, -3.8464e-02, -1.5232e-02,\n",
       "        -1.0198e-02,  5.8299e-03, -3.6675e-02, -3.3153e-02,  2.4826e-02,\n",
       "         9.8881e-03,  2.0216e-02, -7.0450e-03,  2.5327e-02, -7.8844e-03,\n",
       "        -9.2137e-03, -1.7762e-02,  1.6949e-02, -4.6961e-03,  4.1605e-02,\n",
       "        -2.3122e-02, -6.9998e-03, -4.4271e-02,  1.8462e-02,  4.4732e-02,\n",
       "         1.4102e-02,  5.3635e-04, -1.9915e-02, -2.3893e-02, -5.4736e-03,\n",
       "        -1.3068e-02,  5.8864e-03, -9.9589e-03, -7.3972e-03, -3.5314e-03,\n",
       "        -2.4431e-03,  1.2980e-02,  3.9295e-02,  2.4217e-04,  1.8837e-02,\n",
       "         1.6041e-02,  4.9186e-02, -2.3436e-02, -1.9162e-02,  1.9130e-02,\n",
       "         6.4228e-02, -5.6544e-03, -1.0271e-02, -2.6065e-02, -5.4189e-03,\n",
       "        -9.5207e-03, -3.1378e-03, -6.1415e-03, -2.1335e-02, -1.5797e-02,\n",
       "         3.0103e-04, -1.2288e-02,  1.2473e-02, -5.7723e-03,  6.7440e-03,\n",
       "        -1.2787e-02, -1.0068e-02, -1.5972e-02, -1.2132e-02,  6.6959e-03,\n",
       "         3.3799e-02,  8.9317e-03,  1.0754e-02, -5.3893e-03,  1.7598e-02,\n",
       "        -8.8251e-03, -2.3544e-02, -6.9096e-03, -1.4069e-03, -2.2897e-02,\n",
       "         2.3139e-02, -2.1221e-02, -2.3776e-02, -2.2615e-02,  1.2394e-02,\n",
       "        -8.2121e-03,  1.4558e-02, -1.3262e-02, -3.1992e-02,  1.3840e-02,\n",
       "        -5.0948e-03, -2.2917e-03, -2.0737e-04,  1.3542e-02, -3.5919e-03,\n",
       "        -1.4090e-02,  1.1762e-02,  2.8051e-02], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test_model.model.get_input_embeddings()\n",
    "test.weight[-1]\n",
    "# test(torch.tensor([[1,2,3], [4,5,6], [7,8,9]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embeding dimension will be 50297. This might induce some performance reduction as *Tensor Cores* will not be available. For more details  about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([-1.0562e-02, -6.8915e-03,  2.2760e-02, -4.8992e-03, -2.2368e-03,\n",
       "          4.4224e-03, -2.3508e-02, -2.6659e-02, -2.2960e-02,  2.9333e-02,\n",
       "          3.1265e-02, -3.2977e-02, -1.1224e-02, -8.8240e-03, -2.1939e-02,\n",
       "          4.6327e-02, -6.8187e-03,  4.7823e-03, -1.3393e-02, -7.7753e-03,\n",
       "          8.5366e-03,  9.2487e-03,  8.1838e-04,  1.0599e-02, -1.2390e-03,\n",
       "         -9.9141e-03, -1.6118e-02, -1.8759e-02, -5.5478e-03,  1.6991e-02,\n",
       "         -2.3425e-02, -8.6783e-03, -4.8355e-03,  2.2593e-03, -1.3003e-02,\n",
       "          1.5787e-02,  1.2046e-02, -2.4905e-03,  1.7844e-02,  1.2127e-02,\n",
       "          2.6637e-03, -4.1223e-03,  1.0468e-02,  2.9245e-02,  1.4475e-02,\n",
       "         -9.4979e-03,  2.6671e-03,  2.7053e-02, -9.4503e-03, -3.3185e-02,\n",
       "          1.5298e-02,  4.1891e-03, -1.6320e-02, -6.0812e-04, -5.6122e-03,\n",
       "          2.7983e-02,  3.3466e-02, -1.4077e-02, -6.0464e-03,  6.7403e-03,\n",
       "         -9.8866e-03,  2.0478e-02,  1.5812e-02, -7.5136e-03, -1.8610e-02,\n",
       "          1.6465e-02,  6.7966e-04, -2.0589e-02, -9.1076e-03,  6.8979e-03,\n",
       "          1.3976e-02,  2.5263e-02,  3.0608e-03,  1.2604e-02,  6.2199e-03,\n",
       "          5.8039e-03, -3.7183e-02, -8.4196e-03,  1.1855e-02,  2.5647e-02,\n",
       "         -3.1825e-02,  1.5856e-02,  4.2701e-02,  1.3226e-02, -1.0072e-02,\n",
       "          2.0053e-02,  2.0435e-02,  2.5323e-02, -4.8536e-03,  2.9677e-02,\n",
       "         -6.9108e-03, -7.3610e-03, -6.2631e-03, -2.6588e-02, -5.7064e-02,\n",
       "         -2.0896e-02, -2.3537e-02, -2.8535e-02,  3.6260e-03, -2.1285e-03,\n",
       "         -7.1851e-03,  2.5355e-02, -2.0659e-02, -5.5114e-04, -6.9888e-04,\n",
       "          1.8941e-02,  1.9119e-02,  1.6730e-02,  4.3477e-03, -1.4860e-02,\n",
       "          2.3992e-02, -1.3930e-02,  1.4209e-02, -3.6937e-02,  1.6529e-03,\n",
       "          1.5710e-02, -1.6761e-02, -1.0637e-02, -3.0593e-02,  5.1775e-03,\n",
       "         -1.9114e-02,  4.2047e-02, -7.5752e-04, -2.2593e-02, -3.1593e-04,\n",
       "         -2.8201e-02, -5.3577e-03,  2.4617e-03,  6.8621e-03, -2.7188e-02,\n",
       "          3.0626e-02,  2.3798e-02,  2.8575e-02,  8.7948e-03, -1.3589e-02,\n",
       "          9.3312e-03,  6.9892e-03,  1.1614e-02, -2.3955e-02,  1.6554e-02,\n",
       "         -8.9477e-03, -2.4729e-02,  1.2670e-03,  2.6792e-03, -1.4522e-02,\n",
       "          1.2110e-02,  1.1753e-02, -8.5915e-03, -7.3339e-03, -1.6340e-02,\n",
       "          1.7692e-02, -5.9223e-03, -5.0135e-02,  6.3012e-03,  1.3753e-02,\n",
       "          8.5412e-03,  8.6025e-03,  2.1969e-02,  2.8295e-02,  2.7356e-02,\n",
       "         -1.2878e-04,  9.4325e-03,  1.7968e-02, -1.8845e-02, -3.3507e-02,\n",
       "         -1.8268e-03, -9.9624e-03, -2.8637e-02, -2.3120e-03,  2.5599e-04,\n",
       "          5.3450e-03,  1.6316e-02,  8.7243e-03,  1.6499e-02,  1.2438e-02,\n",
       "          2.7361e-03, -2.0039e-02,  3.8409e-03, -1.0305e-03, -4.9480e-03,\n",
       "         -1.9236e-03,  3.4632e-02, -1.3325e-02, -7.6676e-03, -8.0195e-03,\n",
       "          2.5223e-03,  1.0969e-02,  1.0672e-02,  1.6867e-02, -3.2839e-03,\n",
       "         -1.3315e-02, -1.4539e-02, -3.0611e-03, -1.6055e-02,  3.0667e-02,\n",
       "          6.7286e-03, -1.3837e-02,  7.3375e-03, -3.5051e-03, -4.6399e-03,\n",
       "         -4.5667e-02, -2.6575e-02, -3.1327e-02,  1.3574e-03, -3.0422e-02,\n",
       "         -4.0031e-03, -4.8849e-02, -2.7864e-02,  5.1988e-03, -2.4432e-02,\n",
       "         -5.4450e-03, -1.2450e-02, -4.0730e-03, -5.0042e-03,  1.2024e-02,\n",
       "         -1.4938e-02, -1.0469e-02,  1.0991e-02, -1.9131e-02,  9.7296e-03,\n",
       "         -2.4074e-02,  2.5874e-02,  2.9465e-02, -4.8200e-03, -1.1046e-02,\n",
       "          8.4892e-03,  5.2563e-03,  3.4919e-03, -2.0583e-02, -1.8868e-02,\n",
       "          1.6309e-02, -1.6617e-02, -5.5302e-02,  2.7242e-02, -2.1138e-03,\n",
       "         -7.5847e-03, -3.1660e-03,  4.0410e-03, -4.8766e-03, -2.7948e-02,\n",
       "          1.0816e-02, -2.2400e-02, -1.3333e-02, -3.2520e-03, -1.6066e-02,\n",
       "          1.8191e-02,  3.1435e-02,  2.2645e-02, -1.5301e-02,  1.4490e-02,\n",
       "          3.8314e-02,  4.2475e-03,  4.0352e-02, -1.5937e-03,  3.2263e-02,\n",
       "         -1.1255e-02,  5.3007e-03, -4.9947e-03, -1.9266e-02,  4.8258e-02,\n",
       "          5.1486e-02, -1.4338e-02, -3.4740e-02, -7.1862e-03,  6.9362e-04,\n",
       "         -1.3396e-02,  1.5809e-02, -3.3115e-02,  5.3615e-03,  3.9750e-02,\n",
       "          1.2291e-02,  8.7890e-03,  3.1338e-03, -1.6697e-02, -4.9213e-03,\n",
       "         -1.2863e-02,  3.8701e-02,  8.7948e-03,  1.1751e-03, -5.4121e-03,\n",
       "         -1.9466e-02,  2.2241e-02, -5.3323e-03,  2.8412e-02, -4.4363e-04,\n",
       "         -2.6001e-02,  1.1819e-03,  2.9063e-02,  2.2365e-03, -3.0106e-02,\n",
       "          4.1567e-02,  5.8372e-03, -1.4546e-02,  3.8359e-02, -3.7410e-02,\n",
       "         -8.6762e-03, -3.3517e-02, -6.5029e-03, -4.5638e-02,  4.5938e-02,\n",
       "          2.4538e-02, -2.9246e-03,  1.0670e-02, -1.5455e-02, -3.1493e-02,\n",
       "         -2.6197e-02, -1.2862e-03, -3.1736e-03,  1.4552e-02,  1.1016e-02,\n",
       "         -8.0368e-03, -1.2497e-02,  1.8009e-02, -8.4459e-03, -2.5526e-03,\n",
       "         -4.6661e-02, -1.3614e-02, -6.4903e-03,  2.0277e-02,  1.2130e-02,\n",
       "          8.4469e-03,  1.4774e-02, -2.6660e-02, -3.8676e-02, -2.0288e-02,\n",
       "          2.2605e-02, -1.8140e-02,  5.6999e-02, -1.2794e-02,  1.9280e-02,\n",
       "         -7.1852e-04, -3.2222e-03, -8.2112e-04,  2.1047e-02,  4.3749e-03,\n",
       "         -3.5842e-02, -1.2157e-03, -9.3366e-03, -2.0236e-02, -2.0995e-02,\n",
       "         -3.0129e-02,  2.2532e-02, -1.8828e-02, -2.0453e-02,  4.4191e-03,\n",
       "         -2.9448e-02, -6.1735e-03,  2.8669e-02, -1.2011e-02,  2.4950e-02,\n",
       "          7.5218e-04,  1.2531e-02, -2.0435e-02,  6.6540e-03,  2.5362e-03,\n",
       "          5.4483e-03, -4.0869e-02,  2.1489e-02,  2.9605e-02, -1.4665e-02,\n",
       "         -2.8597e-02, -1.3625e-03, -3.4903e-02, -5.7325e-03, -6.1269e-03,\n",
       "          1.4012e-02, -5.2503e-03, -8.9190e-03,  1.4742e-02,  3.5549e-02,\n",
       "         -1.1377e-02,  2.3215e-02,  9.3570e-04,  2.9958e-02, -1.9815e-02,\n",
       "          1.1045e-02, -1.4084e-03,  1.2089e-02,  2.0504e-02,  2.7300e-02,\n",
       "          2.7394e-03,  1.0182e-02, -2.9904e-02,  8.2172e-03,  9.2075e-04,\n",
       "          2.5589e-02, -1.0202e-02,  2.1685e-02, -1.7107e-02,  6.8077e-03,\n",
       "         -1.5446e-02,  1.5333e-02,  1.5966e-02, -1.2769e-02, -4.5059e-04,\n",
       "          7.2639e-03, -2.3865e-02, -1.1838e-02,  1.9863e-02, -7.1778e-03,\n",
       "         -4.9009e-02,  3.1258e-02, -4.3150e-03,  1.3566e-03,  4.0072e-03,\n",
       "          3.4985e-02,  1.7296e-02, -6.7764e-03, -1.7754e-02, -2.7149e-02,\n",
       "         -1.5907e-02,  8.3022e-03, -2.9098e-02, -9.4167e-04,  3.3448e-03,\n",
       "         -1.8293e-02, -1.4897e-02, -6.5456e-03, -4.9461e-03,  5.1853e-03,\n",
       "         -4.0254e-02,  1.5685e-02, -3.8270e-02, -1.1391e-02,  2.3701e-02,\n",
       "          2.8381e-02,  1.2334e-02, -4.5084e-02,  1.3684e-02, -4.4201e-03,\n",
       "         -2.5398e-03, -1.4389e-02, -7.9026e-03, -4.3154e-02, -2.1813e-02,\n",
       "          2.1540e-02, -1.7469e-02,  1.6331e-02, -9.6300e-03, -2.0677e-02,\n",
       "          4.6386e-02,  5.2201e-03,  2.2503e-04, -4.1848e-03, -1.4158e-02,\n",
       "          2.5154e-02, -4.5682e-02, -1.9159e-03,  1.2950e-04,  5.4400e-03,\n",
       "         -2.7889e-02,  1.8082e-02,  9.9920e-03, -1.8464e-02,  1.3591e-02,\n",
       "          2.2004e-02, -1.8809e-03,  4.4390e-02, -1.8175e-02,  2.5986e-02,\n",
       "         -1.0620e-02, -2.2079e-03, -2.3864e-02,  2.0904e-03,  1.8435e-02,\n",
       "         -1.2158e-02, -8.1127e-03,  9.1722e-03, -3.2478e-02, -1.2952e-02,\n",
       "          1.7860e-02,  8.3207e-04, -1.2964e-02,  2.0265e-02, -2.6056e-03,\n",
       "          1.7949e-02, -1.1338e-03, -2.9303e-02,  1.9277e-02, -1.5763e-02,\n",
       "          2.6326e-02, -1.9148e-02,  1.6597e-02,  1.4935e-02,  1.8452e-02,\n",
       "          1.2515e-02,  1.7099e-03,  2.9506e-02,  8.6155e-03, -1.5077e-02,\n",
       "         -1.1872e-02,  8.7057e-03, -1.6768e-02,  1.0805e-02, -8.2731e-03,\n",
       "          3.8104e-02,  1.2050e-02,  1.7102e-02, -3.5465e-02, -8.8846e-04,\n",
       "         -2.3977e-02, -1.4999e-02,  2.2513e-02,  3.0941e-02,  2.9651e-02,\n",
       "          1.6233e-02,  1.3388e-03, -1.1177e-02, -1.0195e-02,  9.8479e-03,\n",
       "          1.1985e-02, -3.7234e-04, -6.1173e-03,  1.7047e-02, -1.0771e-02,\n",
       "         -1.6610e-03,  3.4706e-02,  1.9027e-02,  6.5808e-03,  3.0975e-03,\n",
       "         -1.2943e-03,  2.6351e-03,  3.4354e-03, -2.2809e-02,  1.3490e-02,\n",
       "         -7.4325e-04, -1.7072e-02, -1.6292e-02, -3.4964e-02,  9.4741e-03,\n",
       "          3.7716e-05,  7.8602e-03, -1.7344e-02,  3.0087e-02, -2.3807e-02,\n",
       "          1.1331e-03, -1.7148e-02,  5.3475e-03, -3.2752e-02,  6.3366e-03,\n",
       "          1.1112e-02,  2.3450e-02, -3.0107e-03, -1.5147e-02, -4.2946e-03,\n",
       "         -1.2073e-02, -2.2982e-02, -1.3175e-02,  2.3355e-02,  3.0421e-02,\n",
       "          3.2575e-02, -2.3919e-03,  1.7568e-02, -1.9201e-02, -1.6519e-02,\n",
       "          6.5316e-03, -6.6147e-03, -1.5189e-03,  6.4331e-03, -1.9213e-02,\n",
       "          4.0485e-02, -4.4304e-03, -1.5900e-02,  2.2829e-02, -1.8087e-02,\n",
       "          4.3716e-03, -4.8648e-03,  4.4592e-03,  1.4370e-03, -1.4885e-02,\n",
       "         -9.3943e-03,  7.5893e-03,  1.4450e-02,  2.0672e-02, -7.0749e-03,\n",
       "         -1.0794e-02, -1.3502e-02,  1.8751e-02,  2.9280e-02,  2.1321e-02,\n",
       "         -4.2515e-02,  7.9661e-03, -6.5047e-03,  2.8773e-02,  6.9138e-03,\n",
       "          3.4855e-02,  5.7526e-05,  2.5193e-02, -2.7768e-02,  1.1133e-02,\n",
       "         -1.0647e-02, -2.5965e-02, -1.8423e-02, -1.1143e-03, -2.5935e-02,\n",
       "         -6.8495e-03, -3.5629e-02, -8.7080e-03,  2.5823e-02,  4.1077e-03,\n",
       "         -2.2314e-02, -2.3593e-03, -7.9961e-04,  1.6178e-02, -3.6658e-02,\n",
       "          3.5606e-03, -1.3140e-02, -3.1747e-02,  1.8767e-02, -1.3536e-02,\n",
       "         -1.5283e-02, -4.0273e-03,  1.9093e-02, -4.4818e-03,  1.4983e-02,\n",
       "         -1.2980e-03, -4.6800e-04,  9.2554e-03, -1.5672e-02, -1.6190e-02,\n",
       "         -3.3259e-02,  3.3642e-03,  8.5263e-03, -2.2077e-02, -2.3243e-02,\n",
       "          4.6622e-02,  3.3521e-02, -1.3790e-02,  1.4125e-02,  3.4600e-02,\n",
       "          2.5376e-02, -5.6924e-03,  1.5472e-02,  1.1933e-02,  9.7701e-03,\n",
       "          2.7542e-02,  2.6531e-02,  1.9188e-02,  1.1241e-03,  1.4466e-02,\n",
       "          2.0140e-02, -6.1512e-03,  1.5341e-02,  2.7121e-02,  1.0066e-02,\n",
       "         -3.1604e-02, -1.7679e-03, -1.9942e-02, -1.3481e-02,  2.8871e-02,\n",
       "         -7.9480e-03, -1.1633e-02,  2.9079e-02, -2.6060e-02,  3.4800e-03,\n",
       "         -1.2241e-02, -4.9414e-03, -1.2638e-02, -1.2713e-02,  4.0490e-03,\n",
       "          1.6809e-02, -1.3122e-02, -2.0436e-02, -2.3438e-02,  5.7724e-03,\n",
       "         -1.3133e-02, -2.6821e-02,  3.1797e-03,  3.5392e-02,  1.3739e-02,\n",
       "          3.6833e-03,  1.6831e-02, -1.0504e-02, -7.7389e-03, -4.0486e-02,\n",
       "         -2.3371e-03, -1.4531e-02,  6.1603e-02,  2.5996e-02, -2.4022e-02,\n",
       "          1.5362e-02, -2.8249e-02, -2.7765e-02, -1.1750e-04, -6.9475e-03,\n",
       "          1.7379e-02,  1.5804e-02,  3.6557e-03,  6.4108e-03, -9.5362e-03,\n",
       "          1.7565e-02,  4.3390e-03, -1.4810e-02, -4.4451e-02, -9.2682e-05,\n",
       "         -3.3306e-02,  8.1840e-03,  2.8728e-03, -1.6228e-02, -8.1281e-03,\n",
       "         -5.8421e-03, -3.2729e-02, -1.7835e-02,  1.8650e-02,  4.4291e-02,\n",
       "         -9.0613e-03,  4.5865e-02, -1.7649e-02,  6.5305e-03,  1.8670e-02,\n",
       "          2.8040e-02, -4.3607e-02,  5.0448e-03, -1.3521e-02, -4.2855e-02,\n",
       "         -1.2246e-02, -2.4670e-02,  1.4080e-02,  1.1388e-02, -3.2527e-02,\n",
       "         -6.5782e-03, -2.7939e-03, -8.9316e-03, -1.4558e-03, -1.2587e-02,\n",
       "         -3.8217e-02,  3.1986e-02,  1.7457e-03,  7.2649e-03,  2.9843e-04,\n",
       "         -3.9305e-03, -2.1820e-02,  3.3527e-02, -5.2265e-03,  1.6740e-02,\n",
       "         -3.8497e-03, -2.5510e-02, -2.2205e-02,  6.7366e-03, -1.6710e-02,\n",
       "          2.7112e-02, -5.0855e-03, -2.0074e-02,  2.9400e-02, -1.1400e-02,\n",
       "          1.2574e-02, -1.7149e-02, -1.8368e-02, -5.0316e-04,  3.8388e-02,\n",
       "          8.2464e-03, -2.8299e-02,  3.4496e-02,  1.3956e-02,  1.9015e-02,\n",
       "         -2.9490e-02,  2.6627e-02,  2.0822e-02, -1.3936e-02, -1.6836e-02,\n",
       "          1.5042e-02,  1.9963e-02,  1.4607e-02, -2.8356e-02, -9.4422e-03,\n",
       "          1.2833e-02,  4.2643e-03, -2.7534e-02], grad_fn=<SelectBackward0>),\n",
       " Embedding(50297, 768))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 50265\n",
    "extended_vocab_size = vocab_size + 32\n",
    "# test.resize_token_embeddings(extended_vocab_size)\n",
    "test_model.model.resize_token_embeddings(extended_vocab_size)\n",
    "new_embeds = test_model.model.get_input_embeddings()\n",
    "new_embeds.weight[-1], new_embeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seg_kwargs['inputs_embeds'].shape=torch.Size([3, 512, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2SeqLMOutput(loss=tensor(6.0389, grad_fn=<MeanBackward1>), logits=tensor([[[ 0.0298,  0.0000,  5.0382,  ..., -0.7277,  0.3345, -0.7886],\n",
       "         [ 0.6401,  0.0000,  4.9469,  ..., -0.7017,  0.1738, -0.9237],\n",
       "         [ 0.2603,  0.0000,  4.6532,  ..., -0.4851, -0.0927, -0.4176],\n",
       "         ...,\n",
       "         [ 0.1754,  0.0000,  1.0521,  ..., -0.3606,  0.1263, -0.0388],\n",
       "         [-0.4033,  0.0000,  0.6403,  ..., -0.2754,  0.0245, -0.9709],\n",
       "         [-0.1433,  0.0000,  0.1041,  ...,  0.0193,  0.2408, -0.8375]],\n",
       "\n",
       "        [[ 0.0608,  0.0000,  4.7712,  ..., -0.5538,  0.6644, -0.6049],\n",
       "         [-0.0098,  0.0000,  4.5302,  ..., -0.7639,  0.5371,  0.2808],\n",
       "         [ 0.5453,  0.0000,  4.9672,  ..., -0.4192,  0.6529,  0.0123],\n",
       "         ...,\n",
       "         [-0.3294,  0.0000,  0.7642,  ..., -0.2994, -0.4102,  0.3852],\n",
       "         [-0.3356,  0.0000,  0.8574,  ...,  0.1801,  0.0284, -0.8613],\n",
       "         [ 0.3301,  0.0000,  0.1639,  ..., -0.1535,  0.3822, -0.3297]],\n",
       "\n",
       "        [[-0.1292,  0.0000,  5.3026,  ..., -0.2340,  0.5904, -0.6807],\n",
       "         [ 0.0619,  0.0000,  5.0649,  ..., -0.7914,  0.9552,  0.1394],\n",
       "         [ 0.1168,  0.0000,  4.6918,  ..., -0.3291,  0.6297, -0.5222],\n",
       "         ...,\n",
       "         [-0.2806,  0.0000,  0.5039,  ..., -0.8861,  0.0356,  0.8144],\n",
       "         [-0.3498,  0.0000,  0.6852,  ...,  0.0623,  0.1950, -0.4856],\n",
       "         [-0.1079,  0.0000, -0.3971,  ...,  0.1615,  0.6664, -0.5097]]],\n",
       "       grad_fn=<AddBackward0>), past_key_values=None, decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=None, encoder_hidden_states=None, encoder_attentions=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor([[1,2,3], [4,5,6], [7,8,9]])\n",
    "attention_mask = torch.tensor([[1,1,1], [1,1,1], [1,1,1]])\n",
    "labels = torch.tensor([[1,2,3], [4,5,6], [7,8,9]])\n",
    "\n",
    "test_model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    labels=labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids.shape: torch.Size([32, 1024])\n",
      "attention_mask.shape: torch.Size([32, 1024])\n",
      "labels.shape: torch.Size([32, 128])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "input_ids.shape: torch.Size([32, 1024])\n",
      "attention_mask.shape: torch.Size([32, 1024])\n",
      "labels.shape: torch.Size([32, 128])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "input_ids.shape: torch.Size([32, 1024])\n",
      "attention_mask.shape: torch.Size([32, 1024])\n",
      "labels.shape: torch.Size([32, 128])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "input_ids.shape: torch.Size([32, 1024])\n",
      "attention_mask.shape: torch.Size([32, 1024])\n",
      "labels.shape: torch.Size([32, 128])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "input_ids.shape: torch.Size([32, 1024])\n",
      "attention_mask.shape: torch.Size([32, 1024])\n",
      "labels.shape: torch.Size([32, 128])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "input_ids.shape: torch.Size([32, 1024])\n",
      "attention_mask.shape: torch.Size([32, 1024])\n",
      "labels.shape: torch.Size([32, 128])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "input_ids.shape: torch.Size([32, 1024])\n",
      "attention_mask.shape: torch.Size([32, 1024])\n",
      "labels.shape: torch.Size([32, 128])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "input_ids.shape: torch.Size([32, 1024])\n",
      "attention_mask.shape: torch.Size([32, 1024])\n",
      "labels.shape: torch.Size([32, 128])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "input_ids.shape: torch.Size([31, 1024])\n",
      "attention_mask.shape: torch.Size([31, 1024])\n",
      "labels.shape: torch.Size([31, 128])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([31, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([31, 512, 768])\n",
      "memory.shape=torch.Size([31, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([31, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([31, 512, 768])\n",
      "memory.shape=torch.Size([31, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([31, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([31, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([31, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([31, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "for _, batch in enumerate(train_dataloader):\n",
    "    input_ids = batch[\"input_ids\"].to(test_model.device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(test_model.device)\n",
    "    labels = batch[\"labels\"].to(test_model.device)\n",
    "    \n",
    "    print(\"input_ids.shape:\", input_ids.shape)\n",
    "    print(\"attention_mask.shape:\", attention_mask.shape)\n",
    "    print(\"labels.shape:\", labels.shape)\n",
    "\n",
    "    out = test_model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels,\n",
    "    )\n",
    "    print(out.keys())\n",
    "    out = test_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        # attention_mask=attention_mask,\n",
    "        min_length=0,\n",
    "        max_length=142,\n",
    "        num_beams=4\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.activations import ACT2FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.activations import ACT2FN\n",
    "from transformers.modeling_outputs import (\n",
    "    BaseModelOutput,\n",
    "    BaseModelOutputWithPastAndCrossAttentions,\n",
    "    CausalLMOutputWithCrossAttentions,\n",
    "    Seq2SeqLMOutput,\n",
    "    Seq2SeqModelOutput,\n",
    "    Seq2SeqQuestionAnsweringModelOutput,\n",
    "    Seq2SeqSequenceClassifierOutput,\n",
    ")\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from transformers.utils import (\n",
    "    add_code_sample_docstrings,\n",
    "    add_end_docstrings,\n",
    "    add_start_docstrings,\n",
    "    add_start_docstrings_to_model_forward,\n",
    "    logging,\n",
    "    replace_return_docstrings,\n",
    ")\n",
    "from transformers import BartConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 15, 31, 47, 63, 79, 95, 111, 127]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_inds = (list(range(127, 0, -16)) + [0])[::-1]\n",
    "split_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 14, 30, 46, 62, 78, 94, 110, 126]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_inds = (list(range(126, 0, -16)) + [0])[::-1]\n",
    "split_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.arange(1,1)\n",
    "len(a) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import SummarizationMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_metric = SummarizationMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "from .trace_malloc import * \n",
    "\n",
    "\n",
    "rouge = evaluate.load('rouge')\n",
    "predictions = [\"Photographer Jeffrey Milstein has shot a fascinating project documenting black boxes, shedding light on what happens to the devices after tragic crashes.\\nDespite the name, the boxes are usually red to help rescuers spot them among the wreckage of crashed planes and helicopters.\\nMr Milstein's work saw him gain access to air safety storage facilities where data recorders end up following an accident.\", \"Cruise ship passengers on today relived their two day ordeal at sea after being trapped on Carnival Spirit as Sydney was battered by the storm of the century.\\nHuge waves as high as 40 feet smashed glass panels, shattered plates and cups – and left many on-board violently seasick as the boat pitched from side to side.\\nA Sydney harbour pilot finally boarded this morning and the ship, carrying 2,500 holidaymakers and 1,500 crew, docked at the Overseas Passenger Terminal at around 10am.\\n'Inside everyone was sick, it was really quite rocky,' one passenger said.\", 'Fred Hatch, 76, was battered to death with a claw hammer by his neighbour.\\nHis wife Enid, 70, ran out to help what she thought was an injured man.\\nShe discovered blood splattered walls where her husband had been beaten.\\nThe man was in fact Alan Rogers, 73, who had smashed the skull.\\nRogers was sentenced to a hospital order under the Mental Health Act.\\nHe may never be released and was suffering from paranoid schizophrenia.', 'Floyd Mayweather v Manny Pacquiao will be the biggest fight of all time financially and the most significant this century.\\nThe perceived wisdom was that Ali-Frazier was coming five years too late for two great boxers in their 30s.\\nLet us hope they are so spectacularly wrong again.\\nIn this, the fifth in a series of 12 fights that shaped boxing history, I look back on one of the greatest sporting events.', \"TripAdvisor names Providenciales as the world's top island.\\nAmbergris Caye had held the top ranking for two years in a row.\\nKo Tao, Thailand, gained five spots this year, moving up from No.\\n10 last year.\\nHere are the top 10 U.S. islands:\", 'Michael Scott Shemansky is on the run after authorities named him as a suspect in the murder of his mother.\\nThe body was discovered at the West Colonial Drive home when an out-of-state family member called in a well-being check.\\nHe is already on parole for battery on law enforcement, has a history of bipolar mental illness, and was involuntarily committed to a psychiatric ward for an attempted suicide.\\nHis wife filed a domestic violence injunction against him two years ago, according to her attorney.', 'Dame Sally Davies is said to be concerned at number of children suffering from the condition, which is caused by a deficiency in vitamin D. The disease, a scourge of Victorian Britain, was virtually eradicated after the Second World War.\\nBut it is returning as more and more youngsters are used to staying indoors playing video games than going outside.\\nNow, it has been reported that Professor Davies has ordered the National Institute for Health and Clinical Excellence to review the cost of providing vitamin supplements.', 'John Doyle, 79, was helping two 12-year-old girls cross the road to school.\\nGenti Rustemi, 45, drove over the crossing point without slowing down.\\nMr Doyle tapped his lollipop stick on the roof of the BMW as it hurtled past.\\nHe then told the driver to be more careful and told him to open his eyes.\\nBut after Rustemi had dropped off his daughter at a different school, he returned to the crossing minutes later in a fit of temper and attacked Mr Doyle.']\n",
    "\n",
    "references = ['Fascinating photographs show black boxes salvaged from the wreckage of plane and helicopter crashes.\\nJeffrey Milstein took the pictures to shed light on what happens to flight recorders after tragic accidents.\\nMany of them are dented, mangled and even damaged by fire but they could still be used by crash investigators.', '4,000 on board the Carnival Spirit have been stranded outside Sydney Harbour.\\nPassengers were to disembark the ship on Tuesday but the wild storm prevented it from entering the harbour.\\nThe vessel has suffered damage with smashed glass panels and a door ripped open by the crashing waves.\\nCarnival Spirit was returning from a 12-night cruise to New Caledonia, Vanuatu and Fiji.', \"Enid Hatch found blood splattered walls after going to look for her husband.\\nHe had been beaten with a claw hammer by a crazed pensioner neighbour.\\nAlan Rogers sentenced under Mental Health Act and may never be released.\\nMrs Hatch's sister Betty was murdered in 1971 by mental health absconder.\", \"Floyd Mayweather v Manny Pacquiao is now just eight days away.\\nSportsmail's Jeff Powell has been counting down the greatest fights.\\nIn the fifth in a series of 12 fights that shaped boxing history, we have Muhammad Ali v Joe Frazier, the Thrilla in Manila.\\nIt was said to have come\\xa0five years late, much like Mayweather v Pacquiao.\\nYet those doubting the two greats in their 30s were spectacularly wrong.\", \"Providenciales in the Turks and Caicos is the top TripAdvisor Travelers' Choice award winner for islands.\\nMaui ranks first on the top 10 islands list for the United States.\", 'Police say Michael Scott Shemansky came to their attention after he failed to appear for a supervised visit with his son Saturday.\\nThat same day mother Sandra Shemansky, 57, was found dead at the home they shared in Winter Garden, Florida.\\nMichael Shemansky was going through a difficult divorce and neighbors believe the stress may have caused him to snap.', 'Dame Sally Davies has ordered review into cost of giving out free vitamins.\\nComes after a rise in the number of cases of rickets in children in the UK.\\nIncrease is being put down to fact children spend less time outside playing.\\nRickets can cause bone deformities such as bowed legs and a curved spine.', \"Parent Genti Rustemi 45, dropping off daughter and refused to slow down.\\nHe took child to school, then returned and punched lollipop man John Doyle to the floor in'repugnant' attack in front of schoolchildren.\\nChip shop bosses admitted attack and given suspended sentence.\\nGrandfather says Rustemi's failure to stop was 'like going through a red light', and both he and the children\\xa0'could have been run over'\"]\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [label.strip() for label in labels]\n",
    "\n",
    "    # ROUGE expects a newline after each sentence\n",
    "    preds = [\"\\n\".join(nltk.sent_tokenize(pred)) for pred in preds]\n",
    "    labels = [\"\\n\".join(nltk.sent_tokenize(label)) for label in labels]\n",
    "\n",
    "    return preds, labels\n",
    "predictions, references=postprocess_text(predictions, references)\n",
    "print(predictions[0])\n",
    "results = rouge.compute(predictions=predictions,\n",
    "                        references=references,\n",
    "                        rouge_types=['rouge1', 'rouge2', 'rougeL', 'rougeLsum'],\n",
    "                        use_aggregator=True)\n",
    "print(list(results.keys()))\n",
    "# ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
    "print(results)\n",
    "# [0.5, 0.0]\n",
    "# use_aggregator = False时，输出每个句子的评分\n",
    "\n",
    "# use_aggregator = True时，得分求平均，输出为一个值\n",
    "print(results[\"rouge1\"])\n",
    "0.25\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123\n",
      "123\n"
     ]
    }
   ],
   "source": [
    "print('123\\n123')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartRMTForConditionalGeneration were not initialized from the model checkpoint at kaifanli/RMTbart-base and are newly initialized: ['model.model.encoder.layers.0.self_attn.k_proj.weight', 'model.model.decoder.layers.2.self_attn.q_proj.weight', 'model.model.encoder.layers.4.self_attn.v_proj.weight', 'model.model.encoder.layers.5.self_attn.v_proj.weight', 'model.model.encoder.layers.1.self_attn.q_proj.weight', 'model.model.encoder.layernorm_embedding.bias', 'model.model.decoder.layers.0.encoder_attn.k_proj.weight', 'model.model.encoder.layers.2.self_attn.out_proj.weight', 'model.model.decoder.layers.5.fc2.bias', 'model.model.encoder.layers.5.self_attn.k_proj.weight', 'model.model.decoder.layers.2.self_attn.out_proj.bias', 'model.model.decoder.layers.3.self_attn.k_proj.bias', 'model.model.decoder.layers.2.final_layer_norm.bias', 'model.model.encoder.layers.1.self_attn.v_proj.bias', 'model.model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.model.encoder.layers.1.final_layer_norm.bias', 'model.model.decoder.layers.4.fc1.weight', 'model.model.decoder.layers.0.final_layer_norm.weight', 'model.model.encoder.layers.0.self_attn_layer_norm.bias', 'model.model.encoder.layers.1.self_attn.out_proj.weight', 'model.model.encoder.layers.5.final_layer_norm.weight', 'model.model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.model.decoder.layers.4.self_attn.v_proj.weight', 'model.model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.model.decoder.layers.3.fc1.weight', 'model.model.encoder.layers.0.final_layer_norm.weight', 'model.model.encoder.layers.5.fc2.bias', 'model.model.decoder.layers.4.self_attn.q_proj.weight', 'model.model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.model.decoder.layers.2.fc2.weight', 'model.model.decoder.layers.4.self_attn.q_proj.bias', 'model.model.encoder.layers.3.self_attn.v_proj.weight', 'model.model.encoder.layers.0.self_attn.k_proj.bias', 'model.model.encoder.layers.4.fc1.weight', 'model.model.decoder.layers.0.self_attn.v_proj.bias', 'model.model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.model.encoder.layers.0.final_layer_norm.bias', 'model.model.decoder.layers.5.fc1.bias', 'model.model.encoder.layers.3.fc2.bias', 'model.model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.model.decoder.layers.1.self_attn.k_proj.weight', 'model.model.encoder.layers.1.fc2.bias', 'model.model.encoder.layers.3.self_attn.k_proj.bias', 'model.model.decoder.layers.1.self_attn_layer_norm.weight', 'model.model.decoder.layers.0.self_attn.out_proj.weight', 'model.model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.model.decoder.layers.1.self_attn.v_proj.bias', 'model.model.decoder.layers.0.fc2.weight', 'model.model.decoder.layers.3.fc1.bias', 'model.model.encoder.embed_positions.weight', 'model.model.encoder.layers.2.self_attn_layer_norm.weight', 'model.model.decoder.layers.1.fc1.bias', 'model.model.decoder.layers.5.self_attn.out_proj.bias', 'model.model.decoder.layers.5.self_attn.k_proj.bias', 'model.model.encoder.layers.0.self_attn.out_proj.weight', 'model.model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.model.decoder.layers.0.self_attn.q_proj.bias', 'model.model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.model.encoder.layers.1.fc1.bias', 'model.model.decoder.layers.3.self_attn.v_proj.bias', 'model.model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.model.decoder.layers.2.fc2.bias', 'model.model.encoder.layers.4.final_layer_norm.weight', 'cls_token', 'model.model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.model.encoder.layers.3.self_attn.q_proj.bias', 'model.model.decoder.embed_tokens.weight', 'model.model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.model.decoder.layers.1.self_attn.out_proj.weight', 'model.model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.model.decoder.layers.5.final_layer_norm.bias', 'model.model.decoder.layers.4.self_attn.k_proj.weight', 'model.model.encoder.layers.2.self_attn.k_proj.weight', 'model.model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.model.encoder.layers.2.final_layer_norm.weight', 'model.model.decoder.layers.3.self_attn.q_proj.weight', 'model.model.shared.weight', 'model.model.encoder.layers.2.fc1.weight', 'model.model.decoder.layers.0.self_attn_layer_norm.weight', 'model.model.encoder.layers.2.final_layer_norm.bias', 'model.model.decoder.layers.3.fc2.weight', 'model.model.decoder.layernorm_embedding.bias', 'model.model.encoder.layers.4.fc1.bias', 'model.model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.model.encoder.layers.0.self_attn.v_proj.weight', 'model.model.encoder.layers.5.self_attn.q_proj.bias', 'model.model.encoder.layers.3.self_attn.v_proj.bias', 'model.model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.model.decoder.layers.3.fc2.bias', 'model.model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.model.encoder.layers.3.fc1.bias', 'model.model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.model.encoder.layers.1.final_layer_norm.weight', 'model.model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.model.encoder.layers.0.self_attn.q_proj.bias', 'model.model.encoder.layers.0.fc2.bias', 'model.model.decoder.layers.2.self_attn.v_proj.weight', 'model.model.encoder.layers.2.self_attn.v_proj.weight', 'model.model.decoder.layers.0.self_attn.q_proj.weight', 'model.model.decoder.layers.1.fc2.bias', 'model.model.decoder.layers.5.self_attn.v_proj.weight', 'model.final_logits_bias', 'model.model.decoder.layers.2.self_attn.q_proj.bias', 'model.model.encoder.layers.4.self_attn.k_proj.bias', 'model.model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.model.decoder.layers.1.self_attn.k_proj.bias', 'model.model.decoder.layers.2.final_layer_norm.weight', 'model.model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.model.decoder.layers.1.final_layer_norm.bias', 'model.model.decoder.layers.4.encoder_attn.q_proj.weight', 'sep_token', 'model.model.encoder.layers.1.self_attn.v_proj.weight', 'model.model.encoder.layers.4.final_layer_norm.bias', 'model.model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.model.decoder.layers.2.self_attn_layer_norm.weight', 'model.model.decoder.layers.4.self_attn.v_proj.bias', 'model.model.encoder.layers.0.fc2.weight', 'model.model.decoder.layers.5.self_attn.q_proj.weight', 'model.model.decoder.layers.2.fc1.weight', 'model.model.decoder.layers.2.self_attn.out_proj.weight', 'model.model.encoder.layers.5.fc1.weight', 'model.model.decoder.layers.0.self_attn.out_proj.bias', 'model.model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.model.decoder.layers.5.self_attn.out_proj.weight', 'model.model.encoder.layers.1.self_attn_layer_norm.bias', 'model.model.encoder.layers.3.self_attn.q_proj.weight', 'model.model.decoder.layers.5.self_attn.v_proj.bias', 'model.model.encoder.layers.4.fc2.bias', 'model.model.decoder.layers.0.fc1.bias', 'model.model.decoder.layers.3.self_attn_layer_norm.bias', 'model.model.decoder.layers.4.self_attn_layer_norm.weight', 'model.model.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.model.decoder.layers.3.final_layer_norm.weight', 'model.model.decoder.layers.1.self_attn.v_proj.weight', 'model.model.decoder.layers.3.self_attn_layer_norm.weight', 'model.model.decoder.layers.4.fc2.weight', 'model.model.encoder.layers.3.final_layer_norm.weight', 'model.model.encoder.layers.4.self_attn_layer_norm.bias', 'model.model.decoder.layers.4.self_attn.out_proj.bias', 'model.model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.model.decoder.layers.3.final_layer_norm.bias', 'model.model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.model.decoder.layers.1.fc2.weight', 'model.model.encoder.layers.0.fc1.weight', 'model.model.decoder.layers.2.self_attn.k_proj.weight', 'model.model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.model.encoder.layers.4.fc2.weight', 'model.model.encoder.layers.4.self_attn.q_proj.weight', 'model.model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.model.encoder.layers.5.self_attn_layer_norm.bias', 'model.model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.model.decoder.layers.2.self_attn.v_proj.bias', 'model.model.decoder.layers.0.fc2.bias', 'model.model.decoder.layers.3.self_attn.v_proj.weight', 'model.model.encoder.layers.5.self_attn.v_proj.bias', 'model.model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.model.decoder.layers.2.self_attn_layer_norm.bias', 'model.model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.model.encoder.layers.5.self_attn.q_proj.weight', 'model.model.encoder.layers.0.self_attn.v_proj.bias', 'model.model.encoder.layers.1.fc1.weight', 'model.model.encoder.layers.1.self_attn.k_proj.weight', 'model.model.encoder.layers.0.self_attn.q_proj.weight', 'model.model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.model.decoder.layers.0.fc1.weight', 'model.model.decoder.layers.3.self_attn.q_proj.bias', 'model.model.decoder.layers.3.self_attn.out_proj.bias', 'model.model.decoder.layers.2.self_attn.k_proj.bias', 'model.model.decoder.layers.4.self_attn.out_proj.weight', 'model.model.decoder.layers.3.self_attn.k_proj.weight', 'model.model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.model.decoder.layers.0.self_attn_layer_norm.bias', 'model.model.decoder.layers.5.encoder_attn.q_proj.weight', 'model.model.encoder.layers.0.fc1.bias', 'model.model.decoder.layernorm_embedding.weight', 'model.model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.model.encoder.layers.2.self_attn.k_proj.bias', 'model.model.encoder.layers.3.self_attn.out_proj.weight', 'model.model.encoder.layers.2.self_attn_layer_norm.bias', 'model.model.decoder.layers.4.self_attn_layer_norm.bias', 'model.model.encoder.layers.0.self_attn.out_proj.bias', 'model.model.encoder.layers.3.self_attn.out_proj.bias', 'model.model.encoder.layers.1.self_attn.k_proj.bias', 'model.model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.model.decoder.layers.4.self_attn.k_proj.bias', 'model.model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.model.decoder.layers.0.self_attn.v_proj.weight', 'model.model.decoder.layers.5.final_layer_norm.weight', 'model.model.encoder.layers.1.self_attn.out_proj.bias', 'model.model.encoder.layers.2.fc2.bias', 'model.model.decoder.layers.0.final_layer_norm.bias', 'model.model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.model.decoder.layers.5.fc2.weight', 'model.model.decoder.layers.0.self_attn.k_proj.weight', 'model.model.decoder.layers.1.self_attn.q_proj.bias', 'model.model.decoder.layers.1.self_attn_layer_norm.bias', 'model.model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.model.decoder.layers.2.fc1.bias', 'model.model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.model.decoder.layers.5.self_attn.q_proj.bias', 'model.model.decoder.layers.5.fc1.weight', 'model.model.encoder.layers.5.fc1.bias', 'model.lm_head.weight', 'model.model.encoder.layers.1.self_attn_layer_norm.weight', 'model.model.encoder.layers.2.self_attn.q_proj.bias', 'model.model.encoder.layers.5.self_attn.k_proj.bias', 'model.model.decoder.layers.5.self_attn_layer_norm.bias', 'eos_token', 'model.model.decoder.layers.1.self_attn.out_proj.bias', 'model.model.encoder.layers.3.fc2.weight', 'model.model.encoder.layers.4.self_attn.v_proj.bias', 'model.model.encoder.layernorm_embedding.weight', 'model.model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.model.encoder.layers.5.self_attn.out_proj.weight', 'model.model.decoder.layers.4.fc1.bias', 'model.model.encoder.layers.5.self_attn.out_proj.bias', 'model.model.decoder.layers.4.final_layer_norm.weight', 'model.model.encoder.layers.3.final_layer_norm.bias', 'model.model.encoder.layers.2.fc1.bias', 'model.model.encoder.layers.3.self_attn_layer_norm.weight', 'model.model.decoder.layers.1.self_attn.q_proj.weight', 'model.model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.model.decoder.layers.4.fc2.bias', 'model.model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.model.encoder.layers.1.self_attn.q_proj.bias', 'model.model.encoder.layers.2.self_attn.v_proj.bias', 'model.model.decoder.layers.5.self_attn.k_proj.weight', 'model.model.decoder.layers.1.fc1.weight', 'model.model.encoder.layers.4.self_attn.out_proj.weight', 'model.model.decoder.layers.0.self_attn.k_proj.bias', 'bos_token', 'model.model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.model.encoder.layers.2.self_attn.q_proj.weight', 'model.model.encoder.layers.4.self_attn_layer_norm.weight', 'model.model.encoder.layers.3.self_attn.k_proj.weight', 'model.model.encoder.layers.2.self_attn.out_proj.bias', 'model.model.encoder.layers.4.self_attn.q_proj.bias', 'model.model.encoder.layers.5.final_layer_norm.bias', 'model.model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.model.encoder.layers.4.self_attn.out_proj.bias', 'model.model.encoder.layers.5.self_attn_layer_norm.weight', 'model.model.encoder.layers.0.self_attn_layer_norm.weight', 'model.model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.model.decoder.layers.5.self_attn_layer_norm.weight', 'model.model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.model.decoder.layers.4.final_layer_norm.bias', 'model.model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.model.decoder.layers.3.self_attn.out_proj.weight', 'model.model.decoder.embed_positions.weight', 'model.model.encoder.layers.3.fc1.weight', 'model.model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.model.encoder.layers.4.self_attn.k_proj.weight', 'model.model.encoder.layers.1.fc2.weight', 'model.model.encoder.layers.3.self_attn_layer_norm.bias', 'model.model.encoder.layers.5.fc2.weight', 'model.model.encoder.layers.2.fc2.weight', 'model.model.encoder.embed_tokens.weight', 'model.model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.model.decoder.layers.1.final_layer_norm.weight', 'model.model.decoder.layers.0.encoder_attn_layer_norm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = BartRMTForConditionalGeneration.from_pretrained('kaifanli/RMTbart-base', tokenizer_name_or_path='facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartRMTForConditionalGeneration(\n",
       "  (model): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): Embedding(50281, 768, padding_idx=1)\n",
       "      (encoder): BartEncoder(\n",
       "        (embed_tokens): Embedding(50281, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): Embedding(50281, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50281, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at kaifanli/RMTbart-base were not used when initializing BartForConditionalGeneration: ['embeddings.weight']\n",
      "- This IS expected if you are initializing BartForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BartForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Downloading (…)neration_config.json: 100%|██████████| 292/292 [00:00<00:00, 1.53MB/s]\n"
     ]
    }
   ],
   "source": [
    "test_model = BartForConditionalGeneration.from_pretrained('kaifanli/RMTbart-base')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50281, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50281, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50281, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50281, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "face = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.configuration_utils import "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
