{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ['WANDB_DIR'] = os.getcwd() + '/wandb/'\n",
    "os.environ['WANDB_CACHE_DIR'] = os.getcwd() + '/wandb/.cache/'\n",
    "os.environ['WANDB_CONFIG_DIR'] = os.getcwd() + '/wandb/.config/'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 初始化wandb\n",
    "wandb.init(\n",
    "    entity='kaifan-li',\n",
    "    project=\"my_project\"\n",
    ")\n",
    "\n",
    "# 构建模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i in range(100):\n",
    "        inputs = torch.randn(32, 10)  # 随机生成输入数据\n",
    "        labels = torch.randn(32, 1)   # 随机生成标签\n",
    "        \n",
    "        # 正向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # 记录训练过程和指标\n",
    "    avg_loss = running_loss / 100\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": avg_loss})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = \"Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(long_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/is/kaifan-l/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024])\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "\n",
    "import numpy as np\n",
    "import psutil\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "\n",
    "from peft import PrefixTuningConfig, TaskType, get_peft_model\n",
    "\n",
    "accelerator = Accelerator() # device_placement=\"cuda:0\"\n",
    "model_name_or_path = \"facebook/bart-base\"\n",
    "dataset_name = \"cnn_dailymail\"\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    num_virtual_tokens=20,\n",
    ")\n",
    "text_column = 'article'\n",
    "label_column = 'highlights'\n",
    "lr = 3e-3\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "do_test = True\n",
    "set_seed(seed)\n",
    "cnn_dataset = load_dataset(dataset_name, \"3.0.0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "target_max_length = 128\n",
    "def preprocess_function(examples):\n",
    "    inputs = examples[text_column]\n",
    "    targets = examples[label_column]\n",
    "    model_inputs = tokenizer(inputs, truncation=True) # 这里暂时不padding\n",
    "    targets = tokenizer(\n",
    "        targets,\n",
    "        max_length=target_max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    targets = targets['input_ids']\n",
    "    targets[targets == tokenizer.pad_token_id] = -100\n",
    "    model_inputs['labels'] = targets\n",
    "    \n",
    "    return model_inputs\n",
    "with accelerator.main_process_first():\n",
    "    cnn_dataset = cnn_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        remove_columns=cnn_dataset[\"train\"].column_names,\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "accelerator.wait_for_everyone()\n",
    "train_size = int(len(cnn_dataset[\"train\"]) * 0.001)\n",
    "eval_size = int(len(cnn_dataset[\"validation\"]) * 0.001)\n",
    "test_size = int(len(cnn_dataset[\"test\"]) * 0.001)\n",
    "\n",
    "# 从打乱后的数据集中随机抽取指定数量的数据\n",
    "train_dataset = cnn_dataset[\"train\"].shuffle(seed=42).select(range(train_size))\n",
    "eval_dataset = cnn_dataset[\"validation\"].shuffle(seed=42).select(range(eval_size))\n",
    "test_dataset = cnn_dataset[\"test\"].shuffle(seed=42).select(range(test_size))\n",
    "def collate_fn(examples):\n",
    "    return tokenizer.pad(examples, padding='longest', return_tensors='pt')\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True, # 将数据加载到固定的内存中，可以加速数据加载\n",
    ")\n",
    "\n",
    "for num, batch in enumerate(train_dataloader):\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import logging\n",
    "import copy\n",
    "import math\n",
    "from typing import List, Optional, Tuple, Union, Iterable\n",
    "\n",
    "from transformers import (\n",
    "    BartPretrainedModel,\n",
    "    BartForConditionalGeneration, \n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "from transformers import BartConfig, T5Config\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "from model.prefix_encoder import PrefixEncoder\n",
    "from peft import PrefixTuningConfig, TaskType, get_peft_model\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "# from model.summarization import BartPrefixForConditionalGeneration\n",
    "checkpoint = 'facebook/bart-base'\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    num_virtual_tokens=20,\n",
    ")\n",
    "model = BartPrefixForConditionalGeneration(    \n",
    "    checkpoint=checkpoint,\n",
    "    config=custom_config,\n",
    "    peft_config=peft_config\n",
    ")\n",
    "model.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, batch in enumerate(train_dataloader):\n",
    "    input_ids = batch[\"input_ids\"].to(model.device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "    labels = batch[\"labels\"].to(model.device)\n",
    "    \n",
    "    print(\"input_ids.shape:\", input_ids.shape)\n",
    "    print(\"attention_mask.shape:\", attention_mask.shape)\n",
    "    print(\"labels.shape:\", labels.shape)\n",
    "\n",
    "    out = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels,\n",
    "    )\n",
    "    print(out.keys())\n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        # attention_mask=attention_mask,\n",
    "        min_length=0,\n",
    "        max_length=142,\n",
    "        num_beams=4\n",
    "    )\n",
    "    # print(\"out:\", tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_tokens = torch.arange(20).long()\n",
    "prefix_tokens\n",
    "prefix_tokens = prefix_tokens.unsqueeze(0)\n",
    "prefix_tokens\n",
    "prefix_tokens = prefix_tokens.expand(32, -1)\n",
    "prefix_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
    "segment_size = 20\n",
    "def get_full_padding_segment():\n",
    "    padding_segment = [tokenizer.pad_token_id for _ in range(segment_size)]\n",
    "    return padding_segment\n",
    "\n",
    "test = get_full_padding_segment()\n",
    "test\n",
    "[test]\n",
    "[[12312414]] + [test] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceil = math.ceil(1024/3)\n",
    "ceil, 1024//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import PromptBartConfig\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModel\n",
    "from model import BartPrefixForConditionalGeneration\n",
    "from peft import PrefixTuningConfig\n",
    "\n",
    "bart_config = AutoConfig.from_pretrained('facebook/bart-base')\n",
    "custom_config = PromptBartConfig(**bart_config.to_dict())\n",
    "peft_config = PrefixTuningConfig.from_pretrained(\"kaifanli/prefix_rmt_bart_cnn-dm\")\n",
    "model = BartPrefixForConditionalGeneration(\n",
    "    checkpoint='facebook/bart-base',\n",
    "    config=custom_config,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_config = AutoConfig.from_pretrained('facebook/bart-base')\n",
    "custom_config = PromptBartConfig(**bart_config.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "bart_config = AutoConfig.from_pretrained('facebook/bart-base')\n",
    "custom_config = PromptBartConfig(**bart_config.to_dict())\n",
    "custom_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_model = BartPrefixForConditionalGeneration(custom_config, tokenizer_path='facebook/bart-base')\n",
    "pretrained_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "bart_model.model.load_state_dict(pretrained_model.state_dict())\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    num_virtual_tokens=20,\n",
    ")\n",
    "bart_model.model = get_peft_model(bart_model.model, peft_config)\n",
    "bart_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_model.model.save_pretrained('./test_peft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = BartPrefixForConditionalGeneration(custom_config, tokenizer_path='facebook/bart-base')\n",
    "pretrained_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "test_model.model.load_state_dict(pretrained_model.state_dict())\n",
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "test_model.model = PeftModel.from_pretrained(test_model.model, './test_peft')\n",
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_seg = torch.tensor([1 for _ in range(5)])\n",
    "non_empyty_mask = torch.tensor([True, False, True, True, True])\n",
    "padding_seg[non_empyty_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs = []\n",
    "app = torch.tensor([1,2,3])\n",
    "model_outputs.append(app)\n",
    "model_outputs[0] = torch.tensor([1,2,3])\n",
    "model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_outputs = [1]\n",
    "not len(model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\ntest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "generated_tokens = torch.tensor([[1,2,3], [4,5,6], [7,8,9]])\n",
    "target_max_length = 5\n",
    "generated_tokens = torch.stack([F.pad(t, (0, target_max_length - t.size(0)), value=1) for t in generated_tokens])\n",
    "generated_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "# Encode your input text into input_ids\n",
    "input_text = \"Because the purpose of prompt construction is to find a method that allows an LM to effectively perform a task, rather than being for human consumption, it is not necessary to limit the prompt to human-interpretable natural language. Because of this, there are also methods that examine continuous prompts (a.k.a. soft prompts) that perform prompting directly in the embedding space of the model. Specifically, continuous prompts remove two constraints: (1) relax the constraint that the embeddings of template words be the embeddings of natural language (e.g., English) words. (2) Remove the restriction that the template is parameterized by the pre-trained LM’s parameters. Instead, templates have their own parameters that can be tuned based on training data from the downstream task. We highlight several representative methods below\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "print(input_ids)\n",
    "# Generate output\n",
    "out = bart_model.generate(input_ids, max_length=40)\n",
    "print(out)\n",
    "# Decode the generated output\n",
    "decoded_output = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartRMTForConditionalGeneration(\n",
       "  (model): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): Embedding(50265, 768, padding_idx=1)\n",
       "      (encoder): BartEncoder(\n",
       "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import BartRMTForConditionalGeneration\n",
    "from config import PromptBartConfig\n",
    "\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "import torch\n",
    "\n",
    "# Load the model and tokenizer\n",
    "bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n",
    "\n",
    "from transformers import AutoConfig\n",
    "bart_config = AutoConfig.from_pretrained('facebook/bart-base')\n",
    "custom_config = PromptBartConfig(**bart_config.to_dict())\n",
    "\n",
    "test_model = BartRMTForConditionalGeneration(custom_config, tokenizer_name_or_path='facebook/bart-base')\n",
    "test_model.model.load_state_dict(bart_model.state_dict())\n",
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000e+00,  0.0000e+00,  0.0000e+00,  ...,  0.0000e+00,\n",
       "           0.0000e+00,  0.0000e+00],\n",
       "         [ 2.4270e-02, -1.1350e-03,  8.4792e-04,  ..., -1.9378e-02,\n",
       "          -2.5391e-02,  6.4892e-03],\n",
       "         [-2.6794e-02, -2.0669e-04,  1.8810e-02,  ..., -2.2899e-02,\n",
       "          -9.0091e-03,  1.5300e-02]],\n",
       "\n",
       "        [[-2.3746e-02,  1.3553e-02, -3.2782e-02,  ...,  4.8456e-03,\n",
       "          -3.7510e-02, -7.4061e-03],\n",
       "         [-4.6803e-02,  5.4993e-03,  2.5370e-02,  ..., -2.5416e-02,\n",
       "           1.0124e-02, -6.2738e-04],\n",
       "         [ 7.5001e-03,  1.0069e-02,  1.3370e-03,  ..., -2.5324e-02,\n",
       "           1.0454e-03, -8.1416e-03]],\n",
       "\n",
       "        [[ 6.8983e-04, -1.8809e-02, -2.9562e-02,  ..., -1.4763e-02,\n",
       "          -6.9431e-03,  4.0965e-03],\n",
       "         [ 2.5917e-02, -1.6582e-03, -2.0851e-02,  ...,  1.6838e-03,\n",
       "           2.0986e-02, -1.1919e-02],\n",
       "         [-5.0537e-02, -2.3622e-02,  4.1862e-04,  ...,  2.3304e-02,\n",
       "          -2.1748e-02, -1.5817e-06]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = test_model.model.get_input_embeddings()\n",
    "test(torch.tensor([[1,2,3], [4,5,6], [7,8,9]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seg_kwargs['inputs_embeds'].shape=torch.Size([3, 512, 768])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2SeqLMOutput(loss=tensor(6.0389, grad_fn=<MeanBackward1>), logits=tensor([[[ 0.0298,  0.0000,  5.0382,  ..., -0.7277,  0.3345, -0.7886],\n",
       "         [ 0.6401,  0.0000,  4.9469,  ..., -0.7017,  0.1738, -0.9237],\n",
       "         [ 0.2603,  0.0000,  4.6532,  ..., -0.4851, -0.0927, -0.4176],\n",
       "         ...,\n",
       "         [ 0.1754,  0.0000,  1.0521,  ..., -0.3606,  0.1263, -0.0388],\n",
       "         [-0.4033,  0.0000,  0.6403,  ..., -0.2754,  0.0245, -0.9709],\n",
       "         [-0.1433,  0.0000,  0.1041,  ...,  0.0193,  0.2408, -0.8375]],\n",
       "\n",
       "        [[ 0.0608,  0.0000,  4.7712,  ..., -0.5538,  0.6644, -0.6049],\n",
       "         [-0.0098,  0.0000,  4.5302,  ..., -0.7639,  0.5371,  0.2808],\n",
       "         [ 0.5453,  0.0000,  4.9672,  ..., -0.4192,  0.6529,  0.0123],\n",
       "         ...,\n",
       "         [-0.3294,  0.0000,  0.7642,  ..., -0.2994, -0.4102,  0.3852],\n",
       "         [-0.3356,  0.0000,  0.8574,  ...,  0.1801,  0.0284, -0.8613],\n",
       "         [ 0.3301,  0.0000,  0.1639,  ..., -0.1535,  0.3822, -0.3297]],\n",
       "\n",
       "        [[-0.1292,  0.0000,  5.3026,  ..., -0.2340,  0.5904, -0.6807],\n",
       "         [ 0.0619,  0.0000,  5.0649,  ..., -0.7914,  0.9552,  0.1394],\n",
       "         [ 0.1168,  0.0000,  4.6918,  ..., -0.3291,  0.6297, -0.5222],\n",
       "         ...,\n",
       "         [-0.2806,  0.0000,  0.5039,  ..., -0.8861,  0.0356,  0.8144],\n",
       "         [-0.3498,  0.0000,  0.6852,  ...,  0.0623,  0.1950, -0.4856],\n",
       "         [-0.1079,  0.0000, -0.3971,  ...,  0.1615,  0.6664, -0.5097]]],\n",
       "       grad_fn=<AddBackward0>), past_key_values=None, decoder_hidden_states=None, decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=None, encoder_hidden_states=None, encoder_attentions=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids = torch.tensor([[1,2,3], [4,5,6], [7,8,9]])\n",
    "attention_mask = torch.tensor([[1,1,1], [1,1,1], [1,1,1]])\n",
    "labels = torch.tensor([[1,2,3], [4,5,6], [7,8,9]])\n",
    "\n",
    "test_model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    labels=labels,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids.shape: torch.Size([32, 1024])\n",
      "attention_mask.shape: torch.Size([32, 1024])\n",
      "labels.shape: torch.Size([32, 128])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "input_ids.shape: torch.Size([32, 1024])\n",
      "attention_mask.shape: torch.Size([32, 1024])\n",
      "labels.shape: torch.Size([32, 128])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "input_ids.shape: torch.Size([32, 1024])\n",
      "attention_mask.shape: torch.Size([32, 1024])\n",
      "labels.shape: torch.Size([32, 128])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "input_ids.shape: torch.Size([32, 1024])\n",
      "attention_mask.shape: torch.Size([32, 1024])\n",
      "labels.shape: torch.Size([32, 128])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "input_ids.shape: torch.Size([32, 1024])\n",
      "attention_mask.shape: torch.Size([32, 1024])\n",
      "labels.shape: torch.Size([32, 128])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "input_ids.shape: torch.Size([32, 1024])\n",
      "attention_mask.shape: torch.Size([32, 1024])\n",
      "labels.shape: torch.Size([32, 128])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "input_ids.shape: torch.Size([32, 1024])\n",
      "attention_mask.shape: torch.Size([32, 1024])\n",
      "labels.shape: torch.Size([32, 128])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "input_ids.shape: torch.Size([32, 1024])\n",
      "attention_mask.shape: torch.Size([32, 1024])\n",
      "labels.shape: torch.Size([32, 128])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "memory.shape=torch.Size([32, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([32, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([32, 512, 768])\n",
      "input_ids.shape: torch.Size([31, 1024])\n",
      "attention_mask.shape: torch.Size([31, 1024])\n",
      "labels.shape: torch.Size([31, 128])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([31, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([31, 512, 768])\n",
      "memory.shape=torch.Size([31, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([31, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([31, 512, 768])\n",
      "memory.shape=torch.Size([31, 20, 768])\n",
      "seg_kwargs['inputs_embeds'][:, self.pre_seq_len, :].shape=torch.Size([31, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([31, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([31, 512, 768])\n",
      "seg_kwargs['inputs_embeds'].shape=torch.Size([31, 512, 768])\n"
     ]
    }
   ],
   "source": [
    "for _, batch in enumerate(train_dataloader):\n",
    "    input_ids = batch[\"input_ids\"].to(test_model.device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(test_model.device)\n",
    "    labels = batch[\"labels\"].to(test_model.device)\n",
    "    \n",
    "    print(\"input_ids.shape:\", input_ids.shape)\n",
    "    print(\"attention_mask.shape:\", attention_mask.shape)\n",
    "    print(\"labels.shape:\", labels.shape)\n",
    "\n",
    "    out = test_model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels,\n",
    "    )\n",
    "    print(out.keys())\n",
    "    out = test_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        # attention_mask=attention_mask,\n",
    "        min_length=0,\n",
    "        max_length=142,\n",
    "        num_beams=4\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
