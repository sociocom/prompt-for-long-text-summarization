{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "from datasets import load_dataset, load_metric\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "!wandb login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vw970or3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e259c5475ee34e1fa7a9bdecc7283256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.013 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.227164…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▄▅▁▂▅▄▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.99713</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rare-wind-1</strong> at: <a href='https://wandb.ai/kaifan-li/my_project/runs/vw970or3' target=\"_blank\">https://wandb.ai/kaifan-li/my_project/runs/vw970or3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230804_225122-vw970or3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vw970or3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07bc6da2c3d4e70a35c6eab7030ed93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668189813693366, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/is/kaifan-l/private_room/proj-repos/prompt-for-long-text-summarization/wandb/run-20230804_225935-7shln82z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaifan-li/my_project/runs/7shln82z' target=\"_blank\">swept-dawn-2</a></strong> to <a href='https://wandb.ai/kaifan-li/my_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaifan-li/my_project' target=\"_blank\">https://wandb.ai/kaifan-li/my_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaifan-li/my_project/runs/7shln82z' target=\"_blank\">https://wandb.ai/kaifan-li/my_project/runs/7shln82z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ['WANDB_DIR'] = os.getcwd() + '/wandb/'\n",
    "os.environ['WANDB_CACHE_DIR'] = os.getcwd() + '/wandb/.cache/'\n",
    "os.environ['WANDB_CONFIG_DIR'] = os.getcwd() + '/wandb/.config/'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 初始化wandb\n",
    "wandb.init(\n",
    "    entity='kaifan-li',\n",
    "    project=\"my_project\"\n",
    ")\n",
    "\n",
    "# 构建模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i in range(100):\n",
    "        inputs = torch.randn(32, 10)  # 随机生成输入数据\n",
    "        labels = torch.randn(32, 1)   # 随机生成标签\n",
    "        \n",
    "        # 正向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # 记录训练过程和指标\n",
    "    avg_loss = running_loss / 100\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": avg_loss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BartModel, BartConfig, BartForConditionalGeneration\n",
    "from model.prefix_encoder import PrefixEncoder\n",
    "class BartPrefixForConditionalGeneration(BartForConditionalGeneration):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # self.model = BartModel(config)\n",
    "        # self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
    "        # self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n",
    "        \n",
    "        # MODIFIED\n",
    "        # Start\n",
    "        self.config = config\n",
    "        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # End\n",
    "        \n",
    "        # https://github.com/huggingface/transformers/issues/4701\n",
    "        # if we use BartPrefixForConditionalGeneration.from_pretrained() to load the model, \n",
    "        # it will not overwrite the pretrained weights of the model\n",
    "        # Initialize weights and apply final processing\n",
    "        # self.post_init()\n",
    "        \n",
    "        # MODIFIED\n",
    "        # Start\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.pre_seq_len = config.pre_seq_len\n",
    "        self.n_layer = config.num_hidden_layers\n",
    "        self.n_head = config.num_attention_heads\n",
    "        self.n_embd = config.hidden_size // config.num_attention_heads\n",
    "        \n",
    "        self.prefix_tokens = torch.arange(self.pre_seq_len).long()\n",
    "        self.prefix_encoder = PrefixEncoder(config)\n",
    "        \n",
    "        bart_param = 0\n",
    "        all_param = 0\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            bart_param += param.numel() # numel() returns the total number of elements in the input tensor\n",
    "        \n",
    "        for name, param in self.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            \n",
    "        trainable_param = all_param - bart_param\n",
    "        \n",
    "        print(\"Total parameters: {:,}\".format(all_param))\n",
    "        print(\"Trainable parameters: {:,} {:,%}\".format((trainable_param), trainable_param/all_param))\n",
    "        # End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 406,781,952\n",
      "Trainable parameters: 491,520 0.120831%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartPrefixForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50264, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       "  (prefix_encoder): PrefixEncoder(\n",
       "    (embedding): Embedding(20, 24576)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = BartConfig.from_pretrained('facebook/bart-large-cnn')\n",
    "config.pre_seq_len=20\n",
    "config.prefix_projection=False\n",
    "bart = BartPrefixForConditionalGeneration(config)\n",
    "bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartPrefixForConditionalGeneration 模型成功加载了预训练的权重！\n"
     ]
    }
   ],
   "source": [
    "# 检查模型的参数是否正确加载\n",
    "state_dict = bart.state_dict()\n",
    "if any(key.startswith('model.') for key in state_dict.keys()):\n",
    "    print(\"BartPrefixForConditionalGeneration 模型成功加载了预训练的权重！\")\n",
    "else:\n",
    "    print(\"BartPrefixForConditionalGeneration 模型没有成功加载预训练的权重，请检查模型定义。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BartConfig.from_pretrained('facebook/bart-large-cnn')\n",
    "config.pre_seq_len=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [1,2,3,4,5]\n",
    "test = test[-1:]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flag\n",
      "this is 0 epoch\n",
      "x is tensor([[-0.2290,  1.2786,  0.3341,  0.0985, -0.0992, -0.7334, -0.8649, -0.4964,\n",
      "         -0.4566,  1.5048],\n",
      "        [-0.3796, -0.4937, -0.8009, -0.2741,  0.1658, -0.0121,  0.4416, -0.2015,\n",
      "         -0.0962,  0.0998],\n",
      "        [ 0.3215, -0.4579, -1.1388, -0.0204,  0.2511, -0.0035,  0.6961,  0.9450,\n",
      "          0.1879, -0.3639],\n",
      "        [ 0.0036, -0.5386, -1.3375, -0.5841,  0.0916, -1.3121, -0.6515, -0.2979,\n",
      "          1.0804, -0.0919],\n",
      "        [-0.2241,  0.4265,  0.0054, -0.0826, -0.5758, -0.6925,  0.0994, -0.6229,\n",
      "         -0.8576,  0.3012],\n",
      "        [ 0.2235,  1.4409,  0.8928,  0.7223, -0.6116, -1.6856, -0.0596, -0.7095,\n",
      "         -1.2028,  0.3562],\n",
      "        [-0.0972,  0.3060,  0.6573,  0.3085, -0.3260, -0.3235,  0.5225, -1.1482,\n",
      "         -1.2006,  0.5086],\n",
      "        [-0.6056, -1.3465, -1.2345, -0.8795,  0.2474,  0.7569, -0.2256, -0.3199,\n",
      "          1.2152,  0.7675],\n",
      "        [-0.3077, -0.2454, -0.5742,  0.2105, -0.5111,  0.5800, -0.4270, -0.8334,\n",
      "          0.1129,  1.0073],\n",
      "        [-0.2244, -0.7581, -0.1726, -0.9786, -0.1148,  0.3047, -0.1370, -1.2486,\n",
      "         -0.0082,  1.2642],\n",
      "        [ 0.7363, -1.0554, -1.2198,  0.1757,  0.6501,  0.1162,  0.4916,  0.8010,\n",
      "          0.5142, -1.2458],\n",
      "        [ 1.0706, -0.3344, -0.2811,  0.7619, -0.2338, -0.6389,  0.8668,  0.1817,\n",
      "         -0.1951, -0.7718],\n",
      "        [ 0.1537,  0.1790, -0.4062,  0.0823,  0.2425,  0.9443, -0.2859,  0.6139,\n",
      "         -0.0501,  0.9342],\n",
      "        [ 0.4157, -0.0185, -0.1596,  0.5004,  1.3144, -1.0198,  0.6735,  0.8632,\n",
      "          0.6933, -0.1800],\n",
      "        [ 0.0273, -1.5596, -0.8260, -0.6566,  0.2447, -0.1912,  0.6333, -0.5165,\n",
      "          0.7057, -0.2742],\n",
      "        [-0.8096,  0.0310, -1.1589,  0.8551, -0.2126,  0.7085, -0.1717, -0.6420,\n",
      "          0.0095,  1.4974],\n",
      "        [ 0.4610, -0.2436, -0.2599,  0.9264,  0.3943, -0.1581,  0.8231,  0.2787,\n",
      "          0.1979, -0.0723],\n",
      "        [ 0.1962, -1.0721, -0.9161, -0.4365, -0.2829, -0.2950,  0.3255, -0.6865,\n",
      "          0.4168,  0.2679],\n",
      "        [-0.3913,  0.3830,  0.2195,  0.3993,  0.5711, -0.2029, -0.2336, -0.0846,\n",
      "          0.0195,  0.1945],\n",
      "        [ 0.2759, -0.5956, -0.4890,  0.3098, -0.2145,  0.5754,  0.8844,  0.1313,\n",
      "          0.1245,  0.6919]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "this is 1 epoch\n",
      "x is tensor([[-0.2290,  1.2786,  0.3341,  0.0985, -0.0992, -0.7334, -0.8649, -0.4964,\n",
      "         -0.4566,  1.5048],\n",
      "        [-0.3796, -0.4937, -0.8009, -0.2741,  0.1658, -0.0121,  0.4416, -0.2015,\n",
      "         -0.0962,  0.0998],\n",
      "        [ 0.3215, -0.4579, -1.1388, -0.0204,  0.2511, -0.0035,  0.6961,  0.9450,\n",
      "          0.1879, -0.3639],\n",
      "        [ 0.0036, -0.5386, -1.3375, -0.5841,  0.0916, -1.3121, -0.6515, -0.2979,\n",
      "          1.0804, -0.0919],\n",
      "        [-0.2241,  0.4265,  0.0054, -0.0826, -0.5758, -0.6925,  0.0994, -0.6229,\n",
      "         -0.8576,  0.3012],\n",
      "        [ 0.2235,  1.4409,  0.8928,  0.7223, -0.6116, -1.6856, -0.0596, -0.7095,\n",
      "         -1.2028,  0.3562],\n",
      "        [-0.0972,  0.3060,  0.6573,  0.3085, -0.3260, -0.3235,  0.5225, -1.1482,\n",
      "         -1.2006,  0.5086],\n",
      "        [-0.6056, -1.3465, -1.2345, -0.8795,  0.2474,  0.7569, -0.2256, -0.3199,\n",
      "          1.2152,  0.7675],\n",
      "        [-0.3077, -0.2454, -0.5742,  0.2105, -0.5111,  0.5800, -0.4270, -0.8334,\n",
      "          0.1129,  1.0073],\n",
      "        [-0.2244, -0.7581, -0.1726, -0.9786, -0.1148,  0.3047, -0.1370, -1.2486,\n",
      "         -0.0082,  1.2642],\n",
      "        [ 0.7363, -1.0554, -1.2198,  0.1757,  0.6501,  0.1162,  0.4916,  0.8010,\n",
      "          0.5142, -1.2458],\n",
      "        [ 1.0706, -0.3344, -0.2811,  0.7619, -0.2338, -0.6389,  0.8668,  0.1817,\n",
      "         -0.1951, -0.7718],\n",
      "        [ 0.1537,  0.1790, -0.4062,  0.0823,  0.2425,  0.9443, -0.2859,  0.6139,\n",
      "         -0.0501,  0.9342],\n",
      "        [ 0.4157, -0.0185, -0.1596,  0.5004,  1.3144, -1.0198,  0.6735,  0.8632,\n",
      "          0.6933, -0.1800],\n",
      "        [ 0.0273, -1.5596, -0.8260, -0.6566,  0.2447, -0.1912,  0.6333, -0.5165,\n",
      "          0.7057, -0.2742],\n",
      "        [-0.8096,  0.0310, -1.1589,  0.8551, -0.2126,  0.7085, -0.1717, -0.6420,\n",
      "          0.0095,  1.4974],\n",
      "        [ 0.4610, -0.2436, -0.2599,  0.9264,  0.3943, -0.1581,  0.8231,  0.2787,\n",
      "          0.1979, -0.0723],\n",
      "        [ 0.1962, -1.0721, -0.9161, -0.4365, -0.2829, -0.2950,  0.3255, -0.6865,\n",
      "          0.4168,  0.2679],\n",
      "        [-0.3913,  0.3830,  0.2195,  0.3993,  0.5711, -0.2029, -0.2336, -0.0846,\n",
      "          0.0195,  0.1945],\n",
      "        [ 0.2759, -0.5956, -0.4890,  0.3098, -0.2145,  0.5754,  0.8844,  0.1313,\n",
      "          0.1245,  0.6919]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "this is 2 epoch\n",
      "x is tensor([[-0.2290,  1.2786,  0.3341,  0.0985, -0.0992, -0.7334, -0.8649, -0.4964,\n",
      "         -0.4566,  1.5048],\n",
      "        [-0.3796, -0.4937, -0.8009, -0.2741,  0.1658, -0.0121,  0.4416, -0.2015,\n",
      "         -0.0962,  0.0998],\n",
      "        [ 0.3215, -0.4579, -1.1388, -0.0204,  0.2511, -0.0035,  0.6961,  0.9450,\n",
      "          0.1879, -0.3639],\n",
      "        [ 0.0036, -0.5386, -1.3375, -0.5841,  0.0916, -1.3121, -0.6515, -0.2979,\n",
      "          1.0804, -0.0919],\n",
      "        [-0.2241,  0.4265,  0.0054, -0.0826, -0.5758, -0.6925,  0.0994, -0.6229,\n",
      "         -0.8576,  0.3012],\n",
      "        [ 0.2235,  1.4409,  0.8928,  0.7223, -0.6116, -1.6856, -0.0596, -0.7095,\n",
      "         -1.2028,  0.3562],\n",
      "        [-0.0972,  0.3060,  0.6573,  0.3085, -0.3260, -0.3235,  0.5225, -1.1482,\n",
      "         -1.2006,  0.5086],\n",
      "        [-0.6056, -1.3465, -1.2345, -0.8795,  0.2474,  0.7569, -0.2256, -0.3199,\n",
      "          1.2152,  0.7675],\n",
      "        [-0.3077, -0.2454, -0.5742,  0.2105, -0.5111,  0.5800, -0.4270, -0.8334,\n",
      "          0.1129,  1.0073],\n",
      "        [-0.2244, -0.7581, -0.1726, -0.9786, -0.1148,  0.3047, -0.1370, -1.2486,\n",
      "         -0.0082,  1.2642],\n",
      "        [ 0.7363, -1.0554, -1.2198,  0.1757,  0.6501,  0.1162,  0.4916,  0.8010,\n",
      "          0.5142, -1.2458],\n",
      "        [ 1.0706, -0.3344, -0.2811,  0.7619, -0.2338, -0.6389,  0.8668,  0.1817,\n",
      "         -0.1951, -0.7718],\n",
      "        [ 0.1537,  0.1790, -0.4062,  0.0823,  0.2425,  0.9443, -0.2859,  0.6139,\n",
      "         -0.0501,  0.9342],\n",
      "        [ 0.4157, -0.0185, -0.1596,  0.5004,  1.3144, -1.0198,  0.6735,  0.8632,\n",
      "          0.6933, -0.1800],\n",
      "        [ 0.0273, -1.5596, -0.8260, -0.6566,  0.2447, -0.1912,  0.6333, -0.5165,\n",
      "          0.7057, -0.2742],\n",
      "        [-0.8096,  0.0310, -1.1589,  0.8551, -0.2126,  0.7085, -0.1717, -0.6420,\n",
      "          0.0095,  1.4974],\n",
      "        [ 0.4610, -0.2436, -0.2599,  0.9264,  0.3943, -0.1581,  0.8231,  0.2787,\n",
      "          0.1979, -0.0723],\n",
      "        [ 0.1962, -1.0721, -0.9161, -0.4365, -0.2829, -0.2950,  0.3255, -0.6865,\n",
      "          0.4168,  0.2679],\n",
      "        [-0.3913,  0.3830,  0.2195,  0.3993,  0.5711, -0.2029, -0.2336, -0.0846,\n",
      "          0.0195,  0.1945],\n",
      "        [ 0.2759, -0.5956, -0.4890,  0.3098, -0.2145,  0.5754,  0.8844,  0.1313,\n",
      "          0.1245,  0.6919]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "this is 3 epoch\n",
      "x is tensor([[-0.2290,  1.2786,  0.3341,  0.0985, -0.0992, -0.7334, -0.8649, -0.4964,\n",
      "         -0.4566,  1.5048],\n",
      "        [-0.3796, -0.4937, -0.8009, -0.2741,  0.1658, -0.0121,  0.4416, -0.2015,\n",
      "         -0.0962,  0.0998],\n",
      "        [ 0.3215, -0.4579, -1.1388, -0.0204,  0.2511, -0.0035,  0.6961,  0.9450,\n",
      "          0.1879, -0.3639],\n",
      "        [ 0.0036, -0.5386, -1.3375, -0.5841,  0.0916, -1.3121, -0.6515, -0.2979,\n",
      "          1.0804, -0.0919],\n",
      "        [-0.2241,  0.4265,  0.0054, -0.0826, -0.5758, -0.6925,  0.0994, -0.6229,\n",
      "         -0.8576,  0.3012],\n",
      "        [ 0.2235,  1.4409,  0.8928,  0.7223, -0.6116, -1.6856, -0.0596, -0.7095,\n",
      "         -1.2028,  0.3562],\n",
      "        [-0.0972,  0.3060,  0.6573,  0.3085, -0.3260, -0.3235,  0.5225, -1.1482,\n",
      "         -1.2006,  0.5086],\n",
      "        [-0.6056, -1.3465, -1.2345, -0.8795,  0.2474,  0.7569, -0.2256, -0.3199,\n",
      "          1.2152,  0.7675],\n",
      "        [-0.3077, -0.2454, -0.5742,  0.2105, -0.5111,  0.5800, -0.4270, -0.8334,\n",
      "          0.1129,  1.0073],\n",
      "        [-0.2244, -0.7581, -0.1726, -0.9786, -0.1148,  0.3047, -0.1370, -1.2486,\n",
      "         -0.0082,  1.2642],\n",
      "        [ 0.7363, -1.0554, -1.2198,  0.1757,  0.6501,  0.1162,  0.4916,  0.8010,\n",
      "          0.5142, -1.2458],\n",
      "        [ 1.0706, -0.3344, -0.2811,  0.7619, -0.2338, -0.6389,  0.8668,  0.1817,\n",
      "         -0.1951, -0.7718],\n",
      "        [ 0.1537,  0.1790, -0.4062,  0.0823,  0.2425,  0.9443, -0.2859,  0.6139,\n",
      "         -0.0501,  0.9342],\n",
      "        [ 0.4157, -0.0185, -0.1596,  0.5004,  1.3144, -1.0198,  0.6735,  0.8632,\n",
      "          0.6933, -0.1800],\n",
      "        [ 0.0273, -1.5596, -0.8260, -0.6566,  0.2447, -0.1912,  0.6333, -0.5165,\n",
      "          0.7057, -0.2742],\n",
      "        [-0.8096,  0.0310, -1.1589,  0.8551, -0.2126,  0.7085, -0.1717, -0.6420,\n",
      "          0.0095,  1.4974],\n",
      "        [ 0.4610, -0.2436, -0.2599,  0.9264,  0.3943, -0.1581,  0.8231,  0.2787,\n",
      "          0.1979, -0.0723],\n",
      "        [ 0.1962, -1.0721, -0.9161, -0.4365, -0.2829, -0.2950,  0.3255, -0.6865,\n",
      "          0.4168,  0.2679],\n",
      "        [-0.3913,  0.3830,  0.2195,  0.3993,  0.5711, -0.2029, -0.2336, -0.0846,\n",
      "          0.0195,  0.1945],\n",
      "        [ 0.2759, -0.5956, -0.4890,  0.3098, -0.2145,  0.5754,  0.8844,  0.1313,\n",
      "          0.1245,  0.6919]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "this is 4 epoch\n",
      "x is tensor([[-0.2290,  1.2786,  0.3341,  0.0985, -0.0992, -0.7334, -0.8649, -0.4964,\n",
      "         -0.4566,  1.5048],\n",
      "        [-0.3796, -0.4937, -0.8009, -0.2741,  0.1658, -0.0121,  0.4416, -0.2015,\n",
      "         -0.0962,  0.0998],\n",
      "        [ 0.3215, -0.4579, -1.1388, -0.0204,  0.2511, -0.0035,  0.6961,  0.9450,\n",
      "          0.1879, -0.3639],\n",
      "        [ 0.0036, -0.5386, -1.3375, -0.5841,  0.0916, -1.3121, -0.6515, -0.2979,\n",
      "          1.0804, -0.0919],\n",
      "        [-0.2241,  0.4265,  0.0054, -0.0826, -0.5758, -0.6925,  0.0994, -0.6229,\n",
      "         -0.8576,  0.3012],\n",
      "        [ 0.2235,  1.4409,  0.8928,  0.7223, -0.6116, -1.6856, -0.0596, -0.7095,\n",
      "         -1.2028,  0.3562],\n",
      "        [-0.0972,  0.3060,  0.6573,  0.3085, -0.3260, -0.3235,  0.5225, -1.1482,\n",
      "         -1.2006,  0.5086],\n",
      "        [-0.6056, -1.3465, -1.2345, -0.8795,  0.2474,  0.7569, -0.2256, -0.3199,\n",
      "          1.2152,  0.7675],\n",
      "        [-0.3077, -0.2454, -0.5742,  0.2105, -0.5111,  0.5800, -0.4270, -0.8334,\n",
      "          0.1129,  1.0073],\n",
      "        [-0.2244, -0.7581, -0.1726, -0.9786, -0.1148,  0.3047, -0.1370, -1.2486,\n",
      "         -0.0082,  1.2642],\n",
      "        [ 0.7363, -1.0554, -1.2198,  0.1757,  0.6501,  0.1162,  0.4916,  0.8010,\n",
      "          0.5142, -1.2458],\n",
      "        [ 1.0706, -0.3344, -0.2811,  0.7619, -0.2338, -0.6389,  0.8668,  0.1817,\n",
      "         -0.1951, -0.7718],\n",
      "        [ 0.1537,  0.1790, -0.4062,  0.0823,  0.2425,  0.9443, -0.2859,  0.6139,\n",
      "         -0.0501,  0.9342],\n",
      "        [ 0.4157, -0.0185, -0.1596,  0.5004,  1.3144, -1.0198,  0.6735,  0.8632,\n",
      "          0.6933, -0.1800],\n",
      "        [ 0.0273, -1.5596, -0.8260, -0.6566,  0.2447, -0.1912,  0.6333, -0.5165,\n",
      "          0.7057, -0.2742],\n",
      "        [-0.8096,  0.0310, -1.1589,  0.8551, -0.2126,  0.7085, -0.1717, -0.6420,\n",
      "          0.0095,  1.4974],\n",
      "        [ 0.4610, -0.2436, -0.2599,  0.9264,  0.3943, -0.1581,  0.8231,  0.2787,\n",
      "          0.1979, -0.0723],\n",
      "        [ 0.1962, -1.0721, -0.9161, -0.4365, -0.2829, -0.2950,  0.3255, -0.6865,\n",
      "          0.4168,  0.2679],\n",
      "        [-0.3913,  0.3830,  0.2195,  0.3993,  0.5711, -0.2029, -0.2336, -0.0846,\n",
      "          0.0195,  0.1945],\n",
      "        [ 0.2759, -0.5956, -0.4890,  0.3098, -0.2145,  0.5754,  0.8844,  0.1313,\n",
      "          0.1245,  0.6919]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "this is 5 epoch\n",
      "x is tensor([[-0.2290,  1.2786,  0.3341,  0.0985, -0.0992, -0.7334, -0.8649, -0.4964,\n",
      "         -0.4566,  1.5048],\n",
      "        [-0.3796, -0.4937, -0.8009, -0.2741,  0.1658, -0.0121,  0.4416, -0.2015,\n",
      "         -0.0962,  0.0998],\n",
      "        [ 0.3215, -0.4579, -1.1388, -0.0204,  0.2511, -0.0035,  0.6961,  0.9450,\n",
      "          0.1879, -0.3639],\n",
      "        [ 0.0036, -0.5386, -1.3375, -0.5841,  0.0916, -1.3121, -0.6515, -0.2979,\n",
      "          1.0804, -0.0919],\n",
      "        [-0.2241,  0.4265,  0.0054, -0.0826, -0.5758, -0.6925,  0.0994, -0.6229,\n",
      "         -0.8576,  0.3012],\n",
      "        [ 0.2235,  1.4409,  0.8928,  0.7223, -0.6116, -1.6856, -0.0596, -0.7095,\n",
      "         -1.2028,  0.3562],\n",
      "        [-0.0972,  0.3060,  0.6573,  0.3085, -0.3260, -0.3235,  0.5225, -1.1482,\n",
      "         -1.2006,  0.5086],\n",
      "        [-0.6056, -1.3465, -1.2345, -0.8795,  0.2474,  0.7569, -0.2256, -0.3199,\n",
      "          1.2152,  0.7675],\n",
      "        [-0.3077, -0.2454, -0.5742,  0.2105, -0.5111,  0.5800, -0.4270, -0.8334,\n",
      "          0.1129,  1.0073],\n",
      "        [-0.2244, -0.7581, -0.1726, -0.9786, -0.1148,  0.3047, -0.1370, -1.2486,\n",
      "         -0.0082,  1.2642],\n",
      "        [ 0.7363, -1.0554, -1.2198,  0.1757,  0.6501,  0.1162,  0.4916,  0.8010,\n",
      "          0.5142, -1.2458],\n",
      "        [ 1.0706, -0.3344, -0.2811,  0.7619, -0.2338, -0.6389,  0.8668,  0.1817,\n",
      "         -0.1951, -0.7718],\n",
      "        [ 0.1537,  0.1790, -0.4062,  0.0823,  0.2425,  0.9443, -0.2859,  0.6139,\n",
      "         -0.0501,  0.9342],\n",
      "        [ 0.4157, -0.0185, -0.1596,  0.5004,  1.3144, -1.0198,  0.6735,  0.8632,\n",
      "          0.6933, -0.1800],\n",
      "        [ 0.0273, -1.5596, -0.8260, -0.6566,  0.2447, -0.1912,  0.6333, -0.5165,\n",
      "          0.7057, -0.2742],\n",
      "        [-0.8096,  0.0310, -1.1589,  0.8551, -0.2126,  0.7085, -0.1717, -0.6420,\n",
      "          0.0095,  1.4974],\n",
      "        [ 0.4610, -0.2436, -0.2599,  0.9264,  0.3943, -0.1581,  0.8231,  0.2787,\n",
      "          0.1979, -0.0723],\n",
      "        [ 0.1962, -1.0721, -0.9161, -0.4365, -0.2829, -0.2950,  0.3255, -0.6865,\n",
      "          0.4168,  0.2679],\n",
      "        [-0.3913,  0.3830,  0.2195,  0.3993,  0.5711, -0.2029, -0.2336, -0.0846,\n",
      "          0.0195,  0.1945],\n",
      "        [ 0.2759, -0.5956, -0.4890,  0.3098, -0.2145,  0.5754,  0.8844,  0.1313,\n",
      "          0.1245,  0.6919]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "this is 6 epoch\n",
      "x is tensor([[-0.2290,  1.2786,  0.3341,  0.0985, -0.0992, -0.7334, -0.8649, -0.4964,\n",
      "         -0.4566,  1.5048],\n",
      "        [-0.3796, -0.4937, -0.8009, -0.2741,  0.1658, -0.0121,  0.4416, -0.2015,\n",
      "         -0.0962,  0.0998],\n",
      "        [ 0.3215, -0.4579, -1.1388, -0.0204,  0.2511, -0.0035,  0.6961,  0.9450,\n",
      "          0.1879, -0.3639],\n",
      "        [ 0.0036, -0.5386, -1.3375, -0.5841,  0.0916, -1.3121, -0.6515, -0.2979,\n",
      "          1.0804, -0.0919],\n",
      "        [-0.2241,  0.4265,  0.0054, -0.0826, -0.5758, -0.6925,  0.0994, -0.6229,\n",
      "         -0.8576,  0.3012],\n",
      "        [ 0.2235,  1.4409,  0.8928,  0.7223, -0.6116, -1.6856, -0.0596, -0.7095,\n",
      "         -1.2028,  0.3562],\n",
      "        [-0.0972,  0.3060,  0.6573,  0.3085, -0.3260, -0.3235,  0.5225, -1.1482,\n",
      "         -1.2006,  0.5086],\n",
      "        [-0.6056, -1.3465, -1.2345, -0.8795,  0.2474,  0.7569, -0.2256, -0.3199,\n",
      "          1.2152,  0.7675],\n",
      "        [-0.3077, -0.2454, -0.5742,  0.2105, -0.5111,  0.5800, -0.4270, -0.8334,\n",
      "          0.1129,  1.0073],\n",
      "        [-0.2244, -0.7581, -0.1726, -0.9786, -0.1148,  0.3047, -0.1370, -1.2486,\n",
      "         -0.0082,  1.2642],\n",
      "        [ 0.7363, -1.0554, -1.2198,  0.1757,  0.6501,  0.1162,  0.4916,  0.8010,\n",
      "          0.5142, -1.2458],\n",
      "        [ 1.0706, -0.3344, -0.2811,  0.7619, -0.2338, -0.6389,  0.8668,  0.1817,\n",
      "         -0.1951, -0.7718],\n",
      "        [ 0.1537,  0.1790, -0.4062,  0.0823,  0.2425,  0.9443, -0.2859,  0.6139,\n",
      "         -0.0501,  0.9342],\n",
      "        [ 0.4157, -0.0185, -0.1596,  0.5004,  1.3144, -1.0198,  0.6735,  0.8632,\n",
      "          0.6933, -0.1800],\n",
      "        [ 0.0273, -1.5596, -0.8260, -0.6566,  0.2447, -0.1912,  0.6333, -0.5165,\n",
      "          0.7057, -0.2742],\n",
      "        [-0.8096,  0.0310, -1.1589,  0.8551, -0.2126,  0.7085, -0.1717, -0.6420,\n",
      "          0.0095,  1.4974],\n",
      "        [ 0.4610, -0.2436, -0.2599,  0.9264,  0.3943, -0.1581,  0.8231,  0.2787,\n",
      "          0.1979, -0.0723],\n",
      "        [ 0.1962, -1.0721, -0.9161, -0.4365, -0.2829, -0.2950,  0.3255, -0.6865,\n",
      "          0.4168,  0.2679],\n",
      "        [-0.3913,  0.3830,  0.2195,  0.3993,  0.5711, -0.2029, -0.2336, -0.0846,\n",
      "          0.0195,  0.1945],\n",
      "        [ 0.2759, -0.5956, -0.4890,  0.3098, -0.2145,  0.5754,  0.8844,  0.1313,\n",
      "          0.1245,  0.6919]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "this is 7 epoch\n",
      "x is tensor([[-0.2290,  1.2786,  0.3341,  0.0985, -0.0992, -0.7334, -0.8649, -0.4964,\n",
      "         -0.4566,  1.5048],\n",
      "        [-0.3796, -0.4937, -0.8009, -0.2741,  0.1658, -0.0121,  0.4416, -0.2015,\n",
      "         -0.0962,  0.0998],\n",
      "        [ 0.3215, -0.4579, -1.1388, -0.0204,  0.2511, -0.0035,  0.6961,  0.9450,\n",
      "          0.1879, -0.3639],\n",
      "        [ 0.0036, -0.5386, -1.3375, -0.5841,  0.0916, -1.3121, -0.6515, -0.2979,\n",
      "          1.0804, -0.0919],\n",
      "        [-0.2241,  0.4265,  0.0054, -0.0826, -0.5758, -0.6925,  0.0994, -0.6229,\n",
      "         -0.8576,  0.3012],\n",
      "        [ 0.2235,  1.4409,  0.8928,  0.7223, -0.6116, -1.6856, -0.0596, -0.7095,\n",
      "         -1.2028,  0.3562],\n",
      "        [-0.0972,  0.3060,  0.6573,  0.3085, -0.3260, -0.3235,  0.5225, -1.1482,\n",
      "         -1.2006,  0.5086],\n",
      "        [-0.6056, -1.3465, -1.2345, -0.8795,  0.2474,  0.7569, -0.2256, -0.3199,\n",
      "          1.2152,  0.7675],\n",
      "        [-0.3077, -0.2454, -0.5742,  0.2105, -0.5111,  0.5800, -0.4270, -0.8334,\n",
      "          0.1129,  1.0073],\n",
      "        [-0.2244, -0.7581, -0.1726, -0.9786, -0.1148,  0.3047, -0.1370, -1.2486,\n",
      "         -0.0082,  1.2642],\n",
      "        [ 0.7363, -1.0554, -1.2198,  0.1757,  0.6501,  0.1162,  0.4916,  0.8010,\n",
      "          0.5142, -1.2458],\n",
      "        [ 1.0706, -0.3344, -0.2811,  0.7619, -0.2338, -0.6389,  0.8668,  0.1817,\n",
      "         -0.1951, -0.7718],\n",
      "        [ 0.1537,  0.1790, -0.4062,  0.0823,  0.2425,  0.9443, -0.2859,  0.6139,\n",
      "         -0.0501,  0.9342],\n",
      "        [ 0.4157, -0.0185, -0.1596,  0.5004,  1.3144, -1.0198,  0.6735,  0.8632,\n",
      "          0.6933, -0.1800],\n",
      "        [ 0.0273, -1.5596, -0.8260, -0.6566,  0.2447, -0.1912,  0.6333, -0.5165,\n",
      "          0.7057, -0.2742],\n",
      "        [-0.8096,  0.0310, -1.1589,  0.8551, -0.2126,  0.7085, -0.1717, -0.6420,\n",
      "          0.0095,  1.4974],\n",
      "        [ 0.4610, -0.2436, -0.2599,  0.9264,  0.3943, -0.1581,  0.8231,  0.2787,\n",
      "          0.1979, -0.0723],\n",
      "        [ 0.1962, -1.0721, -0.9161, -0.4365, -0.2829, -0.2950,  0.3255, -0.6865,\n",
      "          0.4168,  0.2679],\n",
      "        [-0.3913,  0.3830,  0.2195,  0.3993,  0.5711, -0.2029, -0.2336, -0.0846,\n",
      "          0.0195,  0.1945],\n",
      "        [ 0.2759, -0.5956, -0.4890,  0.3098, -0.2145,  0.5754,  0.8844,  0.1313,\n",
      "          0.1245,  0.6919]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "this is 8 epoch\n",
      "x is tensor([[-0.2290,  1.2786,  0.3341,  0.0985, -0.0992, -0.7334, -0.8649, -0.4964,\n",
      "         -0.4566,  1.5048],\n",
      "        [-0.3796, -0.4937, -0.8009, -0.2741,  0.1658, -0.0121,  0.4416, -0.2015,\n",
      "         -0.0962,  0.0998],\n",
      "        [ 0.3215, -0.4579, -1.1388, -0.0204,  0.2511, -0.0035,  0.6961,  0.9450,\n",
      "          0.1879, -0.3639],\n",
      "        [ 0.0036, -0.5386, -1.3375, -0.5841,  0.0916, -1.3121, -0.6515, -0.2979,\n",
      "          1.0804, -0.0919],\n",
      "        [-0.2241,  0.4265,  0.0054, -0.0826, -0.5758, -0.6925,  0.0994, -0.6229,\n",
      "         -0.8576,  0.3012],\n",
      "        [ 0.2235,  1.4409,  0.8928,  0.7223, -0.6116, -1.6856, -0.0596, -0.7095,\n",
      "         -1.2028,  0.3562],\n",
      "        [-0.0972,  0.3060,  0.6573,  0.3085, -0.3260, -0.3235,  0.5225, -1.1482,\n",
      "         -1.2006,  0.5086],\n",
      "        [-0.6056, -1.3465, -1.2345, -0.8795,  0.2474,  0.7569, -0.2256, -0.3199,\n",
      "          1.2152,  0.7675],\n",
      "        [-0.3077, -0.2454, -0.5742,  0.2105, -0.5111,  0.5800, -0.4270, -0.8334,\n",
      "          0.1129,  1.0073],\n",
      "        [-0.2244, -0.7581, -0.1726, -0.9786, -0.1148,  0.3047, -0.1370, -1.2486,\n",
      "         -0.0082,  1.2642],\n",
      "        [ 0.7363, -1.0554, -1.2198,  0.1757,  0.6501,  0.1162,  0.4916,  0.8010,\n",
      "          0.5142, -1.2458],\n",
      "        [ 1.0706, -0.3344, -0.2811,  0.7619, -0.2338, -0.6389,  0.8668,  0.1817,\n",
      "         -0.1951, -0.7718],\n",
      "        [ 0.1537,  0.1790, -0.4062,  0.0823,  0.2425,  0.9443, -0.2859,  0.6139,\n",
      "         -0.0501,  0.9342],\n",
      "        [ 0.4157, -0.0185, -0.1596,  0.5004,  1.3144, -1.0198,  0.6735,  0.8632,\n",
      "          0.6933, -0.1800],\n",
      "        [ 0.0273, -1.5596, -0.8260, -0.6566,  0.2447, -0.1912,  0.6333, -0.5165,\n",
      "          0.7057, -0.2742],\n",
      "        [-0.8096,  0.0310, -1.1589,  0.8551, -0.2126,  0.7085, -0.1717, -0.6420,\n",
      "          0.0095,  1.4974],\n",
      "        [ 0.4610, -0.2436, -0.2599,  0.9264,  0.3943, -0.1581,  0.8231,  0.2787,\n",
      "          0.1979, -0.0723],\n",
      "        [ 0.1962, -1.0721, -0.9161, -0.4365, -0.2829, -0.2950,  0.3255, -0.6865,\n",
      "          0.4168,  0.2679],\n",
      "        [-0.3913,  0.3830,  0.2195,  0.3993,  0.5711, -0.2029, -0.2336, -0.0846,\n",
      "          0.0195,  0.1945],\n",
      "        [ 0.2759, -0.5956, -0.4890,  0.3098, -0.2145,  0.5754,  0.8844,  0.1313,\n",
      "          0.1245,  0.6919]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "this is 9 epoch\n",
      "x is tensor([[-0.2290,  1.2786,  0.3341,  0.0985, -0.0992, -0.7334, -0.8649, -0.4964,\n",
      "         -0.4566,  1.5048],\n",
      "        [-0.3796, -0.4937, -0.8009, -0.2741,  0.1658, -0.0121,  0.4416, -0.2015,\n",
      "         -0.0962,  0.0998],\n",
      "        [ 0.3215, -0.4579, -1.1388, -0.0204,  0.2511, -0.0035,  0.6961,  0.9450,\n",
      "          0.1879, -0.3639],\n",
      "        [ 0.0036, -0.5386, -1.3375, -0.5841,  0.0916, -1.3121, -0.6515, -0.2979,\n",
      "          1.0804, -0.0919],\n",
      "        [-0.2241,  0.4265,  0.0054, -0.0826, -0.5758, -0.6925,  0.0994, -0.6229,\n",
      "         -0.8576,  0.3012],\n",
      "        [ 0.2235,  1.4409,  0.8928,  0.7223, -0.6116, -1.6856, -0.0596, -0.7095,\n",
      "         -1.2028,  0.3562],\n",
      "        [-0.0972,  0.3060,  0.6573,  0.3085, -0.3260, -0.3235,  0.5225, -1.1482,\n",
      "         -1.2006,  0.5086],\n",
      "        [-0.6056, -1.3465, -1.2345, -0.8795,  0.2474,  0.7569, -0.2256, -0.3199,\n",
      "          1.2152,  0.7675],\n",
      "        [-0.3077, -0.2454, -0.5742,  0.2105, -0.5111,  0.5800, -0.4270, -0.8334,\n",
      "          0.1129,  1.0073],\n",
      "        [-0.2244, -0.7581, -0.1726, -0.9786, -0.1148,  0.3047, -0.1370, -1.2486,\n",
      "         -0.0082,  1.2642],\n",
      "        [ 0.7363, -1.0554, -1.2198,  0.1757,  0.6501,  0.1162,  0.4916,  0.8010,\n",
      "          0.5142, -1.2458],\n",
      "        [ 1.0706, -0.3344, -0.2811,  0.7619, -0.2338, -0.6389,  0.8668,  0.1817,\n",
      "         -0.1951, -0.7718],\n",
      "        [ 0.1537,  0.1790, -0.4062,  0.0823,  0.2425,  0.9443, -0.2859,  0.6139,\n",
      "         -0.0501,  0.9342],\n",
      "        [ 0.4157, -0.0185, -0.1596,  0.5004,  1.3144, -1.0198,  0.6735,  0.8632,\n",
      "          0.6933, -0.1800],\n",
      "        [ 0.0273, -1.5596, -0.8260, -0.6566,  0.2447, -0.1912,  0.6333, -0.5165,\n",
      "          0.7057, -0.2742],\n",
      "        [-0.8096,  0.0310, -1.1589,  0.8551, -0.2126,  0.7085, -0.1717, -0.6420,\n",
      "          0.0095,  1.4974],\n",
      "        [ 0.4610, -0.2436, -0.2599,  0.9264,  0.3943, -0.1581,  0.8231,  0.2787,\n",
      "          0.1979, -0.0723],\n",
      "        [ 0.1962, -1.0721, -0.9161, -0.4365, -0.2829, -0.2950,  0.3255, -0.6865,\n",
      "          0.4168,  0.2679],\n",
      "        [-0.3913,  0.3830,  0.2195,  0.3993,  0.5711, -0.2029, -0.2336, -0.0846,\n",
      "          0.0195,  0.1945],\n",
      "        [ 0.2759, -0.5956, -0.4890,  0.3098, -0.2145,  0.5754,  0.8844,  0.1313,\n",
      "          0.1245,  0.6919]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10, 10)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        print('flag')\n",
    "        for i in range(10):\n",
    "            x = self.linear(input)\n",
    "            print(\"this is {} epoch\".format(i))\n",
    "            print(\"x is {}\".format(x))\n",
    "            print('\\n')\n",
    "\n",
    "tensor = torch.randn(20, 10)\n",
    "test = Test()\n",
    "test(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers import BartConfig, T5Config\n",
    "\n",
    "class PromptBartConfig(BartConfig):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.pre_seq_len = config.pre_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "[CLS]\n",
      "0\n",
      "Hello\n",
      "0\n",
      "[MASK]\n",
      "0\n",
      "world\n",
      "0\n",
      "[SEP]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['[CLS]', '[SEP]']\n",
    "print(special_tokens[0])\n",
    "seq = ['[CLS]', 'Hello', '[MASK]', 'world', '[SEP]']\n",
    "for s in seq:\n",
    "    print(s)\n",
    "    drop_mask = sum([seq == t for t in special_tokens])\n",
    "    print(drop_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Hello', '[MASK]', 'world', '[SEP]']\n",
      "0\n",
      "[True]\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['[CLS]', '[SEP]']\n",
    "seq = [['[CLS]', 'Hello', '[MASK]', 'world', '[SEP]'],]\n",
    "\n",
    "for s in seq:\n",
    "    print(s)\n",
    "    drop_mask = sum([s == t for t in special_tokens])\n",
    "    print(drop_mask)\n",
    "    drop_mask = [bool(1 - drop_mask)]\n",
    "    print(drop_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "special_token_ids = []\n",
    "config = {\n",
    "    \"max_n_segments\": 4,\n",
    "}\n",
    "\n",
    "def pad_and_segment(input_ids, config):\n",
    "    \"\"\"\n",
    "    segment input_ids into segments\n",
    "    be careful that all the segments are treated as one input sequence \n",
    "    and dealed with incurrence \n",
    "    \"\"\"\n",
    "    segmented_batch = []\n",
    "    # input_ids: [batch_size, seq_len]\n",
    "    for seq in input_ids:\n",
    "        print('this is the origin seq: {}'.format(seq))\n",
    "        drop_mask = sum([seq == t for t in special_token_ids])\n",
    "        seq = seq[(1 - drop_mask)]\n",
    "        seq = seq[:self.config.segment_size * config.max_n_segments]\n",
    "        print('this is the seq after drop_mask: {}'.format(seq))\n",
    "        \n",
    "        align = self.config.segment_alignment\n",
    "        if align in {'right', None}:\n",
    "            split_inds = (list(range(len(seq), 0, -config.segment_size)) + [0])[::-1]\n",
    "        elif align == 'left':\n",
    "            split_inds = list(range(0, len(seq), config.segment_size)) + [len(seq)]\n",
    "        elif align == 'center':\n",
    "            n_seg = math.ceil(len(seq) / config.segment_size)\n",
    "            split_inds = list(range(0, len(seq), math.ceil(len(seq) / n_seg))) + [len(seq)]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        input_segments = [seq[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "        # TODO: do the implementation\n",
    "        # input_segments = [self.pad_add_special_tokens(t, self.config.input_size) for t in input_segments]\n",
    "        print('this is the input_segments: {}'.format(input_segments))\n",
    "        \n",
    "        # add empty segment markers if needed\n",
    "        n_empty_segments = config.max_n_segments - len(input_segments)\n",
    "        \n",
    "        # input_segments:\n",
    "        input_segments = [None] * n_empty_segments + input_segments\n",
    "        print('this is the input_segments after adding empty segment markers: {}'.format(input_segments))\n",
    "        \n",
    "        # segmented_batch: \n",
    "        segmented_batch.append(input_segments)\n",
    "        print('this is the segmented_batch: {}'.format(segmented_batch))\n",
    "        print('first sample in segmented_batch: {}'.format(segmented_batch[0]))\n",
    "        \n",
    "    segmented_batch = [[sample[seg_num] for sample in segmented_batch] \\\n",
    "                        for seg_num in range(config.max_n_segments)]\n",
    "    print('this is the segmented_batch after re-arrange: {}'.format(segmented_batch))\n",
    "    return segmented_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films. Watch I-Reporter give her review of Potter\\'s latest  . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \\\"Harry Potter and the Order of the Phoenix\\\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \\\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\\\" he told an Australian interviewer earlier this month. \\\"I don't think I'll be particularly extravagant. \\\"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\\\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \\\"Hostel: Part II,\\\" currently six places below his number one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \\\"I\\'ll definitely have some sort of party,\\\" he said in an interview. \\\"Hopefully none of you will be reading about it.\\\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \\\"People are always looking to say 'kid star goes off the rails,'\\\" he told reporters last month. \\\"But I try very hard not to go that way because it would be too easy for them.\\\" His latest outing as the boy wizard in \\\"Harry Potter and the Order of the Phoenix\\\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films. Watch I-Reporter give her review of Potter's latest  . There is life beyond Potter, however. The Londoner has filmed a TV movie called \\\"My Boy Jack,\\\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \\\"December Boys,\\\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer's \\\"Equus.\\\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \\\"I just think I'm going to be more sort of fair game,\\\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.\"\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the origin seq: ['LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films. Watch I-Reporter give her review of Potter\\'s latest  . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'bool'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m batch \u001b[39m=\u001b[39m [\n\u001b[1;32m      2\u001b[0m     [\u001b[39m\"\u001b[39m\u001b[39mLONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt cast a spell on him. Daniel Radcliffe as Harry Potter in \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mHarry Potter and the Order of the Phoenix\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mI don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m he told an Australian interviewer earlier this month. \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mI don\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt think I\u001b[39m\u001b[39m'\u001b[39m\u001b[39mll be particularly extravagant. \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mThe things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mHostel: Part II,\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m currently six places below his number one movie on the UK box office chart. Details of how he\u001b[39m\u001b[39m'\u001b[39m\u001b[39mll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mI\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mll definitely have some sort of party,\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m he said in an interview. \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mHopefully none of you will be reading about it.\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m Radcliffe\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39ms earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mPeople are always looking to say \u001b[39m\u001b[39m'\u001b[39m\u001b[39mkid star goes off the rails,\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m he told reporters last month. \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mBut I try very hard not to go that way because it would be too easy for them.\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m His latest outing as the boy wizard in \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mHarry Potter and the Order of the Phoenix\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m is breaking records on both sides of the Atlantic and he will reprise the role in the last two films. Watch I-Reporter give her review of Potter\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms latest  . There is life beyond Potter, however. The Londoner has filmed a TV movie called \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mMy Boy Jack,\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m about author Rudyard Kipling and his son, due for release later this year. He will also appear in \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mDecember Boys,\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mEquus.\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m Meanwhile, he is braced for even closer media scrutiny now that he\u001b[39m\u001b[39m'\u001b[39m\u001b[39ms legally an adult: \u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39mI just think I\u001b[39m\u001b[39m'\u001b[39m\u001b[39mm going to be more sort of fair game,\u001b[39m\u001b[39m\\\"\u001b[39;00m\u001b[39m he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m ]\n\u001b[0;32m----> 5\u001b[0m pad_and_segment(batch, config)\n",
      "Cell \u001b[0;32mIn[24], line 18\u001b[0m, in \u001b[0;36mpad_and_segment\u001b[0;34m(input_ids, config)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mthis is the origin seq: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(seq))\n\u001b[1;32m     17\u001b[0m drop_mask \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m([seq \u001b[39m==\u001b[39m t \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m special_token_ids])\n\u001b[0;32m---> 18\u001b[0m seq \u001b[39m=\u001b[39m seq[(\u001b[39m1\u001b[39;49m \u001b[39m-\u001b[39;49m drop_mask)\u001b[39m.\u001b[39;49mbool()]\n\u001b[1;32m     19\u001b[0m seq \u001b[39m=\u001b[39m seq[:\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39msegment_size \u001b[39m*\u001b[39m config\u001b[39m.\u001b[39mmax_n_segments]\n\u001b[1;32m     20\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mthis is the seq after drop_mask: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(seq))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'bool'"
     ]
    }
   ],
   "source": [
    "batch = [\n",
    "    [\"LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \\\"Harry Potter and the Order of the Phoenix\\\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \\\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\\\" he told an Australian interviewer earlier this month. \\\"I don't think I'll be particularly extravagant. \\\"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\\\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \\\"Hostel: Part II,\\\" currently six places below his number one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \\\"I\\'ll definitely have some sort of party,\\\" he said in an interview. \\\"Hopefully none of you will be reading about it.\\\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \\\"People are always looking to say 'kid star goes off the rails,'\\\" he told reporters last month. \\\"But I try very hard not to go that way because it would be too easy for them.\\\" His latest outing as the boy wizard in \\\"Harry Potter and the Order of the Phoenix\\\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films. Watch I-Reporter give her review of Potter's latest  . There is life beyond Potter, however. The Londoner has filmed a TV movie called \\\"My Boy Jack,\\\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \\\"December Boys,\\\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer's \\\"Equus.\\\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \\\"I just think I'm going to be more sort of fair game,\\\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.\"]\n",
    "]\n",
    "\n",
    "pad_and_segment(batch, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq: tensor([101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 102, 111, 112, 113,\n",
      "        114, 115, 102, 116, 117, 102])\n",
      "drop_mask: tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1])\n",
      "seq: tensor([103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "        117])\n",
      "seq: tensor([101, 102, 103, 104, 105, 106, 102, 107, 108, 109, 110, 111, 102, 112,\n",
      "        113, 114, 115, 102, 116, 117])\n",
      "drop_mask: tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0])\n",
      "seq: tensor([103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "        117])\n",
      "Original input_ids:\n",
      "tensor([[101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 102, 111, 112, 113,\n",
      "         114, 115, 102, 116, 117, 102],\n",
      "        [101, 102, 103, 104, 105, 106, 102, 107, 108, 109, 110, 111, 102, 112,\n",
      "         113, 114, 115, 102, 116, 117]])\n",
      "Processed input_ids:\n",
      "tensor([[103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "         117],\n",
      "        [103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "         117]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_ids = torch.tensor([\n",
    "    [101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 102, 111, 112, 113, 114, 115, 102, 116, 117, 102],\n",
    "    [101, 102, 103, 104, 105, 106, 102, 107, 108, 109, 110, 111, 102, 112, 113, 114, 115, 102, 116, 117]\n",
    "])\n",
    "\n",
    "special_token_ids = [101, 102]\n",
    "\n",
    "\n",
    "\n",
    "class Processor:\n",
    "    def __init__(self, segment_size, rmt_config, special_token_ids):\n",
    "        self.segment_size = segment_size\n",
    "        self.rmt_config = rmt_config\n",
    "        self.special_token_ids = special_token_ids\n",
    "\n",
    "    def pad_and_segment(self, input_ids):\n",
    "        segmented_batch = []\n",
    "        for seq in input_ids:\n",
    "            print('seq:', seq)\n",
    "            drop_mask = sum([seq == t for t in self.special_token_ids])\n",
    "            print('drop_mask:', drop_mask)\n",
    "            seq = seq[(1 - drop_mask).bool()]\n",
    "            print('seq:', seq)\n",
    "            seq = seq[:self.segment_size * self.rmt_config['max_n_segments']]\n",
    "            segmented_batch.append(seq.tolist())\n",
    "        return torch.tensor(segmented_batch)\n",
    "\n",
    "# 假设我们有一个名为 rmt_config 的配置字典和 segment_size 变量\n",
    "rmt_config = {'max_n_segments': 3}\n",
    "segment_size = 5\n",
    "\n",
    "processor = Processor(segment_size, rmt_config, special_token_ids)\n",
    "output = processor.pad_and_segment(input_ids)\n",
    "\n",
    "print(\"Original input_ids:\")\n",
    "print(input_ids)\n",
    "print(\"Processed input_ids:\")\n",
    "print(output)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
