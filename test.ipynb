{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "from datasets import load_dataset, load_metric\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "!wandb login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vw970or3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e259c5475ee34e1fa7a9bdecc7283256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.013 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.227164…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▄▅▁▂▅▄▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.99713</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rare-wind-1</strong> at: <a href='https://wandb.ai/kaifan-li/my_project/runs/vw970or3' target=\"_blank\">https://wandb.ai/kaifan-li/my_project/runs/vw970or3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230804_225122-vw970or3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vw970or3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07bc6da2c3d4e70a35c6eab7030ed93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668189813693366, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/is/kaifan-l/private_room/proj-repos/prompt-for-long-text-summarization/wandb/run-20230804_225935-7shln82z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaifan-li/my_project/runs/7shln82z' target=\"_blank\">swept-dawn-2</a></strong> to <a href='https://wandb.ai/kaifan-li/my_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaifan-li/my_project' target=\"_blank\">https://wandb.ai/kaifan-li/my_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaifan-li/my_project/runs/7shln82z' target=\"_blank\">https://wandb.ai/kaifan-li/my_project/runs/7shln82z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ['WANDB_DIR'] = os.getcwd() + '/wandb/'\n",
    "os.environ['WANDB_CACHE_DIR'] = os.getcwd() + '/wandb/.cache/'\n",
    "os.environ['WANDB_CONFIG_DIR'] = os.getcwd() + '/wandb/.config/'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 初始化wandb\n",
    "wandb.init(\n",
    "    entity='kaifan-li',\n",
    "    project=\"my_project\"\n",
    ")\n",
    "\n",
    "# 构建模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i in range(100):\n",
    "        inputs = torch.randn(32, 10)  # 随机生成输入数据\n",
    "        labels = torch.randn(32, 1)   # 随机生成标签\n",
    "        \n",
    "        # 正向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # 记录训练过程和指标\n",
    "    avg_loss = running_loss / 100\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": avg_loss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BartModel, BartConfig, BartForConditionalGeneration\n",
    "from model.prefix_encoder import PrefixEncoder\n",
    "# import logger\n",
    "import os\n",
    "class BartPrefixForConditionalGeneration(BartForConditionalGeneration):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # self.model = BartModel(config)\n",
    "        # self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
    "        # self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n",
    "        \n",
    "        # MODIFIED\n",
    "        # Start\n",
    "        self.config = config\n",
    "        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        # End\n",
    "        \n",
    "        # https://github.com/huggingface/transformers/issues/4701\n",
    "        # if we use BartPrefixForConditionalGeneration.from_pretrained() to load the model, \n",
    "        # it will not overwrite the pretrained weights of the model\n",
    "        # Initialize weights and apply final processing\n",
    "        # self.post_init()\n",
    "        \n",
    "        # MODIFIED\n",
    "        # Start\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.pre_seq_len = config.pre_seq_len\n",
    "        self.n_layer = config.num_hidden_layers\n",
    "        self.n_head = config.num_attention_heads\n",
    "        self.n_embd = config.hidden_size // config.num_attention_heads\n",
    "        \n",
    "        self.prefix_tokens = torch.arange(self.pre_seq_len).long()\n",
    "        self.prefix_encoder = PrefixEncoder(config)\n",
    "        \n",
    "        bart_param = 0\n",
    "        all_param = 0\n",
    "        \n",
    "        for name, param in self.model.named_parameters():\n",
    "            bart_param += param.numel() # numel() returns the total number of elements in the input tensor\n",
    "        \n",
    "        for name, param in self.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            \n",
    "        trainable_param = all_param - bart_param\n",
    "        \n",
    "        print(\"Total parameters: {:,}\".format(all_param))\n",
    "        print(\"Trainable parameters: {:,} {:,%}\".format((trainable_param), trainable_param/all_param))\n",
    "        # End"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 406,781,952\n",
      "Trainable parameters: 491,520 0.120831%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartPrefixForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50264, 1024, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       "  (prefix_encoder): PrefixEncoder(\n",
       "    (embedding): Embedding(20, 24576)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = BartConfig.from_pretrained('facebook/bart-large-cnn')\n",
    "config.pre_seq_len=20\n",
    "config.prefix_projection=False\n",
    "bart = BartPrefixForConditionalGeneration(config)\n",
    "bart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.0116, -0.0149,  0.0162,  ..., -0.0061, -0.0113,  0.0047],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0084,  0.0047,  0.0178,  ...,  0.0010,  0.0168,  0.0475],\n",
       "        ...,\n",
       "        [ 0.0104, -0.0081, -0.0018,  ...,  0.0143, -0.0116, -0.0234],\n",
       "        [ 0.0123, -0.0171, -0.0113,  ..., -0.0192,  0.0085, -0.0006],\n",
       "        [-0.0279, -0.0205,  0.0152,  ..., -0.0041,  0.0014, -0.0242]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart.model.shared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BartPrefixForConditionalGeneration 模型成功加载了预训练的权重！\n"
     ]
    }
   ],
   "source": [
    "# 检查模型的参数是否正确加载\n",
    "state_dict = bart.state_dict()\n",
    "if any(key.startswith('model.') for key in state_dict.keys()):\n",
    "    print(\"BartPrefixForConditionalGeneration 模型成功加载了预训练的权重！\")\n",
    "else:\n",
    "    print(\"BartPrefixForConditionalGeneration 模型没有成功加载预训练的权重，请检查模型定义。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = BartConfig.from_pretrained('facebook/bart-large-cnn')\n",
    "config.pre_seq_len=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [1,2,3,4,5]\n",
    "test = test[-1:]\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flag\n",
      "this is 0 epoch\n",
      "x is tensor([[-0.2290,  1.2786,  0.3341,  0.0985, -0.0992, -0.7334, -0.8649, -0.4964,\n",
      "         -0.4566,  1.5048],\n",
      "        [-0.3796, -0.4937, -0.8009, -0.2741,  0.1658, -0.0121,  0.4416, -0.2015,\n",
      "         -0.0962,  0.0998],\n",
      "        [ 0.3215, -0.4579, -1.1388, -0.0204,  0.2511, -0.0035,  0.6961,  0.9450,\n",
      "          0.1879, -0.3639],\n",
      "        [ 0.0036, -0.5386, -1.3375, -0.5841,  0.0916, -1.3121, -0.6515, -0.2979,\n",
      "          1.0804, -0.0919],\n",
      "        [-0.2241,  0.4265,  0.0054, -0.0826, -0.5758, -0.6925,  0.0994, -0.6229,\n",
      "         -0.8576,  0.3012],\n",
      "        [ 0.2235,  1.4409,  0.8928,  0.7223, -0.6116, -1.6856, -0.0596, -0.7095,\n",
      "         -1.2028,  0.3562],\n",
      "        [-0.0972,  0.3060,  0.6573,  0.3085, -0.3260, -0.3235,  0.5225, -1.1482,\n",
      "         -1.2006,  0.5086],\n",
      "        [-0.6056, -1.3465, -1.2345, -0.8795,  0.2474,  0.7569, -0.2256, -0.3199,\n",
      "          1.2152,  0.7675],\n",
      "        [-0.3077, -0.2454, -0.5742,  0.2105, -0.5111,  0.5800, -0.4270, -0.8334,\n",
      "          0.1129,  1.0073],\n",
      "        [-0.2244, -0.7581, -0.1726, -0.9786, -0.1148,  0.3047, -0.1370, -1.2486,\n",
      "         -0.0082,  1.2642],\n",
      "        [ 0.7363, -1.0554, -1.2198,  0.1757,  0.6501,  0.1162,  0.4916,  0.8010,\n",
      "          0.5142, -1.2458],\n",
      "        [ 1.0706, -0.3344, -0.2811,  0.7619, -0.2338, -0.6389,  0.8668,  0.1817,\n",
      "         -0.1951, -0.7718],\n",
      "        [ 0.1537,  0.1790, -0.4062,  0.0823,  0.2425,  0.9443, -0.2859,  0.6139,\n",
      "         -0.0501,  0.9342],\n",
      "        [ 0.4157, -0.0185, -0.1596,  0.5004,  1.3144, -1.0198,  0.6735,  0.8632,\n",
      "          0.6933, -0.1800],\n",
      "        [ 0.0273, -1.5596, -0.8260, -0.6566,  0.2447, -0.1912,  0.6333, -0.5165,\n",
      "          0.7057, -0.2742],\n",
      "        [-0.8096,  0.0310, -1.1589,  0.8551, -0.2126,  0.7085, -0.1717, -0.6420,\n",
      "          0.0095,  1.4974],\n",
      "        [ 0.4610, -0.2436, -0.2599,  0.9264,  0.3943, -0.1581,  0.8231,  0.2787,\n",
      "          0.1979, -0.0723],\n",
      "        [ 0.1962, -1.0721, -0.9161, -0.4365, -0.2829, -0.2950,  0.3255, -0.6865,\n",
      "          0.4168,  0.2679],\n",
      "        [-0.3913,  0.3830,  0.2195,  0.3993,  0.5711, -0.2029, -0.2336, -0.0846,\n",
      "          0.0195,  0.1945],\n",
      "        [ 0.2759, -0.5956, -0.4890,  0.3098, -0.2145,  0.5754,  0.8844,  0.1313,\n",
      "          0.1245,  0.6919]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "this is 1 epoch\n",
      "x is tensor([[-0.2290,  1.2786,  0.3341,  0.0985, -0.0992, -0.7334, -0.8649, -0.4964,\n",
      "         -0.4566,  1.5048],\n",
      "        [-0.3796, -0.4937, -0.8009, -0.2741,  0.1658, -0.0121,  0.4416, -0.2015,\n",
      "         -0.0962,  0.0998],\n",
      "        [ 0.3215, -0.4579, -1.1388, -0.0204,  0.2511, -0.0035,  0.6961,  0.9450,\n",
      "          0.1879, -0.3639],\n",
      "        [ 0.0036, -0.5386, -1.3375, -0.5841,  0.0916, -1.3121, -0.6515, -0.2979,\n",
      "          1.0804, -0.0919],\n",
      "        [-0.2241,  0.4265,  0.0054, -0.0826, -0.5758, -0.6925,  0.0994, -0.6229,\n",
      "         -0.8576,  0.3012],\n",
      "        [ 0.2235,  1.4409,  0.8928,  0.7223, -0.6116, -1.6856, -0.0596, -0.7095,\n",
      "         -1.2028,  0.3562],\n",
      "        [-0.0972,  0.3060,  0.6573,  0.3085, -0.3260, -0.3235,  0.5225, -1.1482,\n",
      "         -1.2006,  0.5086],\n",
      "        [-0.6056, -1.3465, -1.2345, -0.8795,  0.2474,  0.7569, -0.2256, -0.3199,\n",
      "          1.2152,  0.7675],\n",
      "        [-0.3077, -0.2454, -0.5742,  0.2105, -0.5111,  0.5800, -0.4270, -0.8334,\n",
      "          0.1129,  1.0073],\n",
      "        [-0.2244, -0.7581, -0.1726, -0.9786, -0.1148,  0.3047, -0.1370, -1.2486,\n",
      "         -0.0082,  1.2642],\n",
      "        [ 0.7363, -1.0554, -1.2198,  0.1757,  0.6501,  0.1162,  0.4916,  0.8010,\n",
      "          0.5142, -1.2458],\n",
      "        [ 1.0706, -0.3344, -0.2811,  0.7619, -0.2338, -0.6389,  0.8668,  0.1817,\n",
      "         -0.1951, -0.7718],\n",
      "        [ 0.1537,  0.1790, -0.4062,  0.0823,  0.2425,  0.9443, -0.2859,  0.6139,\n",
      "         -0.0501,  0.9342],\n",
      "        [ 0.4157, -0.0185, -0.1596,  0.5004,  1.3144, -1.0198,  0.6735,  0.8632,\n",
      "          0.6933, -0.1800],\n",
      "        [ 0.0273, -1.5596, -0.8260, -0.6566,  0.2447, -0.1912,  0.6333, -0.5165,\n",
      "          0.7057, -0.2742],\n",
      "        [-0.8096,  0.0310, -1.1589,  0.8551, -0.2126,  0.7085, -0.1717, -0.6420,\n",
      "          0.0095,  1.4974],\n",
      "        [ 0.4610, -0.2436, -0.2599,  0.9264,  0.3943, -0.1581,  0.8231,  0.2787,\n",
      "          0.1979, -0.0723],\n",
      "        [ 0.1962, -1.0721, -0.9161, -0.4365, -0.2829, -0.2950,  0.3255, -0.6865,\n",
      "          0.4168,  0.2679],\n",
      "        [-0.3913,  0.3830,  0.2195,  0.3993,  0.5711, -0.2029, -0.2336, -0.0846,\n",
      "          0.0195,  0.1945],\n",
      "        [ 0.2759, -0.5956, -0.4890,  0.3098, -0.2145,  0.5754,  0.8844,  0.1313,\n",
      "          0.1245,  0.6919]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "this is 2 epoch\n",
      "x is tensor([[-0.2290,  1.2786,  0.3341,  0.0985, -0.0992, -0.7334, -0.8649, -0.4964,\n",
      "         -0.4566,  1.5048],\n",
      "        [-0.3796, -0.4937, -0.8009, -0.2741,  0.1658, -0.0121,  0.4416, -0.2015,\n",
      "         -0.0962,  0.0998],\n",
      "        [ 0.3215, -0.4579, -1.1388, -0.0204,  0.2511, -0.0035,  0.6961,  0.9450,\n",
      "          0.1879, -0.3639],\n",
      "        [ 0.0036, -0.5386, -1.3375, -0.5841,  0.0916, -1.3121, -0.6515, -0.2979,\n",
      "          1.0804, -0.0919],\n",
      "        [-0.2241,  0.4265,  0.0054, -0.0826, -0.5758, -0.6925,  0.0994, -0.6229,\n",
      "         -0.8576,  0.3012],\n",
      "        [ 0.2235,  1.4409,  0.8928,  0.7223, -0.6116, -1.6856, -0.0596, -0.7095,\n",
      "         -1.2028,  0.3562],\n",
      "        [-0.0972,  0.3060,  0.6573,  0.3085, -0.3260, -0.3235,  0.5225, -1.1482,\n",
      "         -1.2006,  0.5086],\n",
      "        [-0.6056, -1.3465, -1.2345, -0.8795,  0.2474,  0.7569, -0.2256, -0.3199,\n",
      "          1.2152,  0.7675],\n",
      "        [-0.3077, -0.2454, -0.5742,  0.2105, -0.5111,  0.5800, -0.4270, -0.8334,\n",
      "          0.1129,  1.0073],\n",
      "        [-0.2244, -0.7581, -0.1726, -0.9786, -0.1148,  0.3047, -0.1370, -1.2486,\n",
      "         -0.0082,  1.2642],\n",
      "        [ 0.7363, -1.0554, -1.2198,  0.1757,  0.6501,  0.1162,  0.4916,  0.8010,\n",
      "          0.5142, -1.2458],\n",
      "        [ 1.0706, -0.3344, -0.2811,  0.7619, -0.2338, -0.6389,  0.8668,  0.1817,\n",
      "         -0.1951, -0.7718],\n",
      "        [ 0.1537,  0.1790, -0.4062,  0.0823,  0.2425,  0.9443, -0.2859,  0.6139,\n",
      "         -0.0501,  0.9342],\n",
      "        [ 0.4157, -0.0185, -0.1596,  0.5004,  1.3144, -1.0198,  0.6735,  0.8632,\n",
      "          0.6933, -0.1800],\n",
      "        [ 0.0273, -1.5596, -0.8260, -0.6566,  0.2447, -0.1912,  0.6333, -0.5165,\n",
      "          0.7057, -0.2742],\n",
      "        [-0.8096,  0.0310, -1.1589,  0.8551, -0.2126,  0.7085, -0.1717, -0.6420,\n",
      "          0.0095,  1.4974],\n",
      "        [ 0.4610, -0.2436, -0.2599,  0.9264,  0.3943, -0.1581,  0.8231,  0.2787,\n",
      "          0.1979, -0.0723],\n",
      "        [ 0.1962, -1.0721, -0.9161, -0.4365, -0.2829, -0.2950,  0.3255, -0.6865,\n",
      "          0.4168,  0.2679],\n",
      "        [-0.3913,  0.3830,  0.2195,  0.3993,  0.5711, -0.2029, -0.2336, -0.0846,\n",
      "          0.0195,  0.1945],\n",
      "        [ 0.2759, -0.5956, -0.4890,  0.3098, -0.2145,  0.5754,  0.8844,  0.1313,\n",
      "          0.1245,  0.6919]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "this is 3 epoch\n",
      "x is tensor([[-0.2290,  1.2786,  0.3341,  0.0985, -0.0992, -0.7334, -0.8649, -0.4964,\n",
      "         -0.4566,  1.5048],\n",
      "        [-0.3796, -0.4937, -0.8009, -0.2741,  0.1658, -0.0121,  0.4416, -0.2015,\n",
      "         -0.0962,  0.0998],\n",
      "        [ 0.3215, -0.4579, -1.1388, -0.0204,  0.2511, -0.0035,  0.6961,  0.9450,\n",
      "          0.1879, -0.3639],\n",
      "        [ 0.0036, -0.5386, -1.3375, -0.5841,  0.0916, -1.3121, -0.6515, -0.2979,\n",
      "          1.0804, -0.0919],\n",
      "        [-0.2241,  0.4265,  0.0054, -0.0826, -0.5758, -0.6925,  0.0994, -0.6229,\n",
      "         -0.8576,  0.3012],\n",
      "        [ 0.2235,  1.4409,  0.8928,  0.7223, -0.6116, -1.6856, -0.0596, -0.7095,\n",
      "         -1.2028,  0.3562],\n",
      "        [-0.0972,  0.3060,  0.6573,  0.3085, -0.3260, -0.3235,  0.5225, -1.1482,\n",
      "         -1.2006,  0.5086],\n",
      "        [-0.6056, -1.3465, -1.2345, -0.8795,  0.2474,  0.7569, -0.2256, -0.3199,\n",
      "          1.2152,  0.7675],\n",
      "        [-0.3077, -0.2454, -0.5742,  0.2105, -0.5111,  0.5800, -0.4270, -0.8334,\n",
      "          0.1129,  1.0073],\n",
      "        [-0.2244, -0.7581, -0.1726, -0.9786, -0.1148,  0.3047, -0.1370, -1.2486,\n",
      "         -0.0082,  1.2642],\n",
      "        [ 0.7363, -1.0554, -1.2198,  0.1757,  0.6501,  0.1162,  0.4916,  0.8010,\n",
      "          0.5142, -1.2458],\n",
      "        [ 1.0706, -0.3344, -0.2811,  0.7619, -0.2338, -0.6389,  0.8668,  0.1817,\n",
      "         -0.1951, -0.7718],\n",
      "        [ 0.1537,  0.1790, -0.4062,  0.0823,  0.2425,  0.9443, -0.2859,  0.6139,\n",
      "         -0.0501,  0.9342],\n",
      "        [ 0.4157, -0.0185, -0.1596,  0.5004,  1.3144, -1.0198,  0.6735,  0.8632,\n",
      "          0.6933, -0.1800],\n",
      "        [ 0.0273, -1.5596, -0.8260, -0.6566,  0.2447, -0.1912,  0.6333, -0.5165,\n",
      "          0.7057, -0.2742],\n",
      "        [-0.8096,  0.0310, -1.1589,  0.8551, -0.2126,  0.7085, -0.1717, -0.6420,\n",
      "          0.0095,  1.4974],\n",
      "        [ 0.4610, -0.2436, -0.2599,  0.9264,  0.3943, -0.1581,  0.8231,  0.2787,\n",
      "          0.1979, -0.0723],\n",
      "        [ 0.1962, -1.0721, -0.9161, -0.4365, -0.2829, -0.2950,  0.3255, -0.6865,\n",
      "          0.4168,  0.2679],\n",
      "        [-0.3913,  0.3830,  0.2195,  0.3993,  0.5711, -0.2029, -0.2336, -0.0846,\n",
      "          0.0195,  0.1945],\n",
      "        [ 0.2759, -0.5956, -0.4890,  0.3098, -0.2145,  0.5754,  0.8844,  0.1313,\n",
      "          0.1245,  0.6919]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "this is 4 epoch\n",
      "x is tensor([[-0.2290,  1.2786,  0.3341,  0.0985, -0.0992, -0.7334, -0.8649, -0.4964,\n",
      "         -0.4566,  1.5048],\n",
      "        [-0.3796, -0.4937, -0.8009, -0.2741,  0.1658, -0.0121,  0.4416, -0.2015,\n",
      "         -0.0962,  0.0998],\n",
      "        [ 0.3215, -0.4579, -1.1388, -0.0204,  0.2511, -0.0035,  0.6961,  0.9450,\n",
      "          0.1879, -0.3639],\n",
      "        [ 0.0036, -0.5386, -1.3375, -0.5841,  0.0916, -1.3121, -0.6515, -0.2979,\n",
      "          1.0804, -0.0919],\n",
      "        [-0.2241,  0.4265,  0.0054, -0.0826, -0.5758, -0.6925,  0.0994, -0.6229,\n",
      "         -0.8576,  0.3012],\n",
      "        [ 0.2235,  1.4409,  0.8928,  0.7223, -0.6116, -1.6856, -0.0596, -0.7095,\n",
      "         -1.2028,  0.3562],\n",
      "        [-0.0972,  0.3060,  0.6573,  0.3085, -0.3260, -0.3235,  0.5225, -1.1482,\n",
      "         -1.2006,  0.5086],\n",
      "        [-0.6056, -1.3465, -1.2345, -0.8795,  0.2474,  0.7569, -0.2256, -0.3199,\n",
      "          1.2152,  0.7675],\n",
      "        [-0.3077, -0.2454, -0.5742,  0.2105, -0.5111,  0.5800, -0.4270, -0.8334,\n",
      "          0.1129,  1.0073],\n",
      "        [-0.2244, -0.7581, -0.1726, -0.9786, -0.1148,  0.3047, -0.1370, -1.2486,\n",
      "         -0.0082,  1.2642],\n",
      "        [ 0.7363, -1.0554, -1.2198,  0.1757,  0.6501,  0.1162,  0.4916,  0.8010,\n",
      "          0.5142, -1.2458],\n",
      "        [ 1.0706, -0.3344, -0.2811,  0.7619, -0.2338, -0.6389,  0.8668,  0.1817,\n",
      "         -0.1951, -0.7718],\n",
      "        [ 0.1537,  0.1790, -0.4062,  0.0823,  0.2425,  0.9443, -0.2859,  0.6139,\n",
      "         -0.0501,  0.9342],\n",
      "        [ 0.4157, -0.0185, -0.1596,  0.5004,  1.3144, -1.0198,  0.6735,  0.8632,\n",
      "          0.6933, -0.1800],\n",
      "        [ 0.0273, -1.5596, -0.8260, -0.6566,  0.2447, -0.1912,  0.6333, -0.5165,\n",
      "          0.7057, -0.2742],\n",
      "        [-0.8096,  0.0310, -1.1589,  0.8551, -0.2126,  0.7085, -0.1717, -0.6420,\n",
      "          0.0095,  1.4974],\n",
      "        [ 0.4610, -0.2436, -0.2599,  0.9264,  0.3943, -0.1581,  0.8231,  0.2787,\n",
      "          0.1979, -0.0723],\n",
      "        [ 0.1962, -1.0721, -0.9161, -0.4365, -0.2829, -0.2950,  0.3255, -0.6865,\n",
      "          0.4168,  0.2679],\n",
      "        [-0.3913,  0.3830,  0.2195,  0.3993,  0.5711, -0.2029, -0.2336, -0.0846,\n",
      "          0.0195,  0.1945],\n",
      "        [ 0.2759, -0.5956, -0.4890,  0.3098, -0.2145,  0.5754,  0.8844,  0.1313,\n",
      "          0.1245,  0.6919]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "this is 5 epoch\n",
      "x is tensor([[-0.2290,  1.2786,  0.3341,  0.0985, -0.0992, -0.7334, -0.8649, -0.4964,\n",
      "         -0.4566,  1.5048],\n",
      "        [-0.3796, -0.4937, -0.8009, -0.2741,  0.1658, -0.0121,  0.4416, -0.2015,\n",
      "         -0.0962,  0.0998],\n",
      "        [ 0.3215, -0.4579, -1.1388, -0.0204,  0.2511, -0.0035,  0.6961,  0.9450,\n",
      "          0.1879, -0.3639],\n",
      "        [ 0.0036, -0.5386, -1.3375, -0.5841,  0.0916, -1.3121, -0.6515, -0.2979,\n",
      "          1.0804, -0.0919],\n",
      "        [-0.2241,  0.4265,  0.0054, -0.0826, -0.5758, -0.6925,  0.0994, -0.6229,\n",
      "         -0.8576,  0.3012],\n",
      "        [ 0.2235,  1.4409,  0.8928,  0.7223, -0.6116, -1.6856, -0.0596, -0.7095,\n",
      "         -1.2028,  0.3562],\n",
      "        [-0.0972,  0.3060,  0.6573,  0.3085, -0.3260, -0.3235,  0.5225, -1.1482,\n",
      "         -1.2006,  0.5086],\n",
      "        [-0.6056, -1.3465, -1.2345, -0.8795,  0.2474,  0.7569, -0.2256, -0.3199,\n",
      "          1.2152,  0.7675],\n",
      "        [-0.3077, -0.2454, -0.5742,  0.2105, -0.5111,  0.5800, -0.4270, -0.8334,\n",
      "          0.1129,  1.0073],\n",
      "        [-0.2244, -0.7581, -0.1726, -0.9786, -0.1148,  0.3047, -0.1370, -1.2486,\n",
      "         -0.0082,  1.2642],\n",
      "        [ 0.7363, -1.0554, -1.2198,  0.1757,  0.6501,  0.1162,  0.4916,  0.8010,\n",
      "          0.5142, -1.2458],\n",
      "        [ 1.0706, -0.3344, -0.2811,  0.7619, -0.2338, -0.6389,  0.8668,  0.1817,\n",
      "         -0.1951, -0.7718],\n",
      "        [ 0.1537,  0.1790, -0.4062,  0.0823,  0.2425,  0.9443, -0.2859,  0.6139,\n",
      "         -0.0501,  0.9342],\n",
      "        [ 0.4157, -0.0185, -0.1596,  0.5004,  1.3144, -1.0198,  0.6735,  0.8632,\n",
      "          0.6933, -0.1800],\n",
      "        [ 0.0273, -1.5596, -0.8260, -0.6566,  0.2447, -0.1912,  0.6333, -0.5165,\n",
      "          0.7057, -0.2742],\n",
      "        [-0.8096,  0.0310, -1.1589,  0.8551, -0.2126,  0.7085, -0.1717, -0.6420,\n",
      "          0.0095,  1.4974],\n",
      "        [ 0.4610, -0.2436, -0.2599,  0.9264,  0.3943, -0.1581,  0.8231,  0.2787,\n",
      "          0.1979, -0.0723],\n",
      "        [ 0.1962, -1.0721, -0.9161, -0.4365, -0.2829, -0.2950,  0.3255, -0.6865,\n",
      "          0.4168,  0.2679],\n",
      "        [-0.3913,  0.3830,  0.2195,  0.3993,  0.5711, -0.2029, -0.2336, -0.0846,\n",
      "          0.0195,  0.1945],\n",
      "        [ 0.2759, -0.5956, -0.4890,  0.3098, -0.2145,  0.5754,  0.8844,  0.1313,\n",
      "          0.1245,  0.6919]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "this is 6 epoch\n",
      "x is tensor([[-0.2290,  1.2786,  0.3341,  0.0985, -0.0992, -0.7334, -0.8649, -0.4964,\n",
      "         -0.4566,  1.5048],\n",
      "        [-0.3796, -0.4937, -0.8009, -0.2741,  0.1658, -0.0121,  0.4416, -0.2015,\n",
      "         -0.0962,  0.0998],\n",
      "        [ 0.3215, -0.4579, -1.1388, -0.0204,  0.2511, -0.0035,  0.6961,  0.9450,\n",
      "          0.1879, -0.3639],\n",
      "        [ 0.0036, -0.5386, -1.3375, -0.5841,  0.0916, -1.3121, -0.6515, -0.2979,\n",
      "          1.0804, -0.0919],\n",
      "        [-0.2241,  0.4265,  0.0054, -0.0826, -0.5758, -0.6925,  0.0994, -0.6229,\n",
      "         -0.8576,  0.3012],\n",
      "        [ 0.2235,  1.4409,  0.8928,  0.7223, -0.6116, -1.6856, -0.0596, -0.7095,\n",
      "         -1.2028,  0.3562],\n",
      "        [-0.0972,  0.3060,  0.6573,  0.3085, -0.3260, -0.3235,  0.5225, -1.1482,\n",
      "         -1.2006,  0.5086],\n",
      "        [-0.6056, -1.3465, -1.2345, -0.8795,  0.2474,  0.7569, -0.2256, -0.3199,\n",
      "          1.2152,  0.7675],\n",
      "        [-0.3077, -0.2454, -0.5742,  0.2105, -0.5111,  0.5800, -0.4270, -0.8334,\n",
      "          0.1129,  1.0073],\n",
      "        [-0.2244, -0.7581, -0.1726, -0.9786, -0.1148,  0.3047, -0.1370, -1.2486,\n",
      "         -0.0082,  1.2642],\n",
      "        [ 0.7363, -1.0554, -1.2198,  0.1757,  0.6501,  0.1162,  0.4916,  0.8010,\n",
      "          0.5142, -1.2458],\n",
      "        [ 1.0706, -0.3344, -0.2811,  0.7619, -0.2338, -0.6389,  0.8668,  0.1817,\n",
      "         -0.1951, -0.7718],\n",
      "        [ 0.1537,  0.1790, -0.4062,  0.0823,  0.2425,  0.9443, -0.2859,  0.6139,\n",
      "         -0.0501,  0.9342],\n",
      "        [ 0.4157, -0.0185, -0.1596,  0.5004,  1.3144, -1.0198,  0.6735,  0.8632,\n",
      "          0.6933, -0.1800],\n",
      "        [ 0.0273, -1.5596, -0.8260, -0.6566,  0.2447, -0.1912,  0.6333, -0.5165,\n",
      "          0.7057, -0.2742],\n",
      "        [-0.8096,  0.0310, -1.1589,  0.8551, -0.2126,  0.7085, -0.1717, -0.6420,\n",
      "          0.0095,  1.4974],\n",
      "        [ 0.4610, -0.2436, -0.2599,  0.9264,  0.3943, -0.1581,  0.8231,  0.2787,\n",
      "          0.1979, -0.0723],\n",
      "        [ 0.1962, -1.0721, -0.9161, -0.4365, -0.2829, -0.2950,  0.3255, -0.6865,\n",
      "          0.4168,  0.2679],\n",
      "        [-0.3913,  0.3830,  0.2195,  0.3993,  0.5711, -0.2029, -0.2336, -0.0846,\n",
      "          0.0195,  0.1945],\n",
      "        [ 0.2759, -0.5956, -0.4890,  0.3098, -0.2145,  0.5754,  0.8844,  0.1313,\n",
      "          0.1245,  0.6919]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "this is 7 epoch\n",
      "x is tensor([[-0.2290,  1.2786,  0.3341,  0.0985, -0.0992, -0.7334, -0.8649, -0.4964,\n",
      "         -0.4566,  1.5048],\n",
      "        [-0.3796, -0.4937, -0.8009, -0.2741,  0.1658, -0.0121,  0.4416, -0.2015,\n",
      "         -0.0962,  0.0998],\n",
      "        [ 0.3215, -0.4579, -1.1388, -0.0204,  0.2511, -0.0035,  0.6961,  0.9450,\n",
      "          0.1879, -0.3639],\n",
      "        [ 0.0036, -0.5386, -1.3375, -0.5841,  0.0916, -1.3121, -0.6515, -0.2979,\n",
      "          1.0804, -0.0919],\n",
      "        [-0.2241,  0.4265,  0.0054, -0.0826, -0.5758, -0.6925,  0.0994, -0.6229,\n",
      "         -0.8576,  0.3012],\n",
      "        [ 0.2235,  1.4409,  0.8928,  0.7223, -0.6116, -1.6856, -0.0596, -0.7095,\n",
      "         -1.2028,  0.3562],\n",
      "        [-0.0972,  0.3060,  0.6573,  0.3085, -0.3260, -0.3235,  0.5225, -1.1482,\n",
      "         -1.2006,  0.5086],\n",
      "        [-0.6056, -1.3465, -1.2345, -0.8795,  0.2474,  0.7569, -0.2256, -0.3199,\n",
      "          1.2152,  0.7675],\n",
      "        [-0.3077, -0.2454, -0.5742,  0.2105, -0.5111,  0.5800, -0.4270, -0.8334,\n",
      "          0.1129,  1.0073],\n",
      "        [-0.2244, -0.7581, -0.1726, -0.9786, -0.1148,  0.3047, -0.1370, -1.2486,\n",
      "         -0.0082,  1.2642],\n",
      "        [ 0.7363, -1.0554, -1.2198,  0.1757,  0.6501,  0.1162,  0.4916,  0.8010,\n",
      "          0.5142, -1.2458],\n",
      "        [ 1.0706, -0.3344, -0.2811,  0.7619, -0.2338, -0.6389,  0.8668,  0.1817,\n",
      "         -0.1951, -0.7718],\n",
      "        [ 0.1537,  0.1790, -0.4062,  0.0823,  0.2425,  0.9443, -0.2859,  0.6139,\n",
      "         -0.0501,  0.9342],\n",
      "        [ 0.4157, -0.0185, -0.1596,  0.5004,  1.3144, -1.0198,  0.6735,  0.8632,\n",
      "          0.6933, -0.1800],\n",
      "        [ 0.0273, -1.5596, -0.8260, -0.6566,  0.2447, -0.1912,  0.6333, -0.5165,\n",
      "          0.7057, -0.2742],\n",
      "        [-0.8096,  0.0310, -1.1589,  0.8551, -0.2126,  0.7085, -0.1717, -0.6420,\n",
      "          0.0095,  1.4974],\n",
      "        [ 0.4610, -0.2436, -0.2599,  0.9264,  0.3943, -0.1581,  0.8231,  0.2787,\n",
      "          0.1979, -0.0723],\n",
      "        [ 0.1962, -1.0721, -0.9161, -0.4365, -0.2829, -0.2950,  0.3255, -0.6865,\n",
      "          0.4168,  0.2679],\n",
      "        [-0.3913,  0.3830,  0.2195,  0.3993,  0.5711, -0.2029, -0.2336, -0.0846,\n",
      "          0.0195,  0.1945],\n",
      "        [ 0.2759, -0.5956, -0.4890,  0.3098, -0.2145,  0.5754,  0.8844,  0.1313,\n",
      "          0.1245,  0.6919]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "this is 8 epoch\n",
      "x is tensor([[-0.2290,  1.2786,  0.3341,  0.0985, -0.0992, -0.7334, -0.8649, -0.4964,\n",
      "         -0.4566,  1.5048],\n",
      "        [-0.3796, -0.4937, -0.8009, -0.2741,  0.1658, -0.0121,  0.4416, -0.2015,\n",
      "         -0.0962,  0.0998],\n",
      "        [ 0.3215, -0.4579, -1.1388, -0.0204,  0.2511, -0.0035,  0.6961,  0.9450,\n",
      "          0.1879, -0.3639],\n",
      "        [ 0.0036, -0.5386, -1.3375, -0.5841,  0.0916, -1.3121, -0.6515, -0.2979,\n",
      "          1.0804, -0.0919],\n",
      "        [-0.2241,  0.4265,  0.0054, -0.0826, -0.5758, -0.6925,  0.0994, -0.6229,\n",
      "         -0.8576,  0.3012],\n",
      "        [ 0.2235,  1.4409,  0.8928,  0.7223, -0.6116, -1.6856, -0.0596, -0.7095,\n",
      "         -1.2028,  0.3562],\n",
      "        [-0.0972,  0.3060,  0.6573,  0.3085, -0.3260, -0.3235,  0.5225, -1.1482,\n",
      "         -1.2006,  0.5086],\n",
      "        [-0.6056, -1.3465, -1.2345, -0.8795,  0.2474,  0.7569, -0.2256, -0.3199,\n",
      "          1.2152,  0.7675],\n",
      "        [-0.3077, -0.2454, -0.5742,  0.2105, -0.5111,  0.5800, -0.4270, -0.8334,\n",
      "          0.1129,  1.0073],\n",
      "        [-0.2244, -0.7581, -0.1726, -0.9786, -0.1148,  0.3047, -0.1370, -1.2486,\n",
      "         -0.0082,  1.2642],\n",
      "        [ 0.7363, -1.0554, -1.2198,  0.1757,  0.6501,  0.1162,  0.4916,  0.8010,\n",
      "          0.5142, -1.2458],\n",
      "        [ 1.0706, -0.3344, -0.2811,  0.7619, -0.2338, -0.6389,  0.8668,  0.1817,\n",
      "         -0.1951, -0.7718],\n",
      "        [ 0.1537,  0.1790, -0.4062,  0.0823,  0.2425,  0.9443, -0.2859,  0.6139,\n",
      "         -0.0501,  0.9342],\n",
      "        [ 0.4157, -0.0185, -0.1596,  0.5004,  1.3144, -1.0198,  0.6735,  0.8632,\n",
      "          0.6933, -0.1800],\n",
      "        [ 0.0273, -1.5596, -0.8260, -0.6566,  0.2447, -0.1912,  0.6333, -0.5165,\n",
      "          0.7057, -0.2742],\n",
      "        [-0.8096,  0.0310, -1.1589,  0.8551, -0.2126,  0.7085, -0.1717, -0.6420,\n",
      "          0.0095,  1.4974],\n",
      "        [ 0.4610, -0.2436, -0.2599,  0.9264,  0.3943, -0.1581,  0.8231,  0.2787,\n",
      "          0.1979, -0.0723],\n",
      "        [ 0.1962, -1.0721, -0.9161, -0.4365, -0.2829, -0.2950,  0.3255, -0.6865,\n",
      "          0.4168,  0.2679],\n",
      "        [-0.3913,  0.3830,  0.2195,  0.3993,  0.5711, -0.2029, -0.2336, -0.0846,\n",
      "          0.0195,  0.1945],\n",
      "        [ 0.2759, -0.5956, -0.4890,  0.3098, -0.2145,  0.5754,  0.8844,  0.1313,\n",
      "          0.1245,  0.6919]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n",
      "this is 9 epoch\n",
      "x is tensor([[-0.2290,  1.2786,  0.3341,  0.0985, -0.0992, -0.7334, -0.8649, -0.4964,\n",
      "         -0.4566,  1.5048],\n",
      "        [-0.3796, -0.4937, -0.8009, -0.2741,  0.1658, -0.0121,  0.4416, -0.2015,\n",
      "         -0.0962,  0.0998],\n",
      "        [ 0.3215, -0.4579, -1.1388, -0.0204,  0.2511, -0.0035,  0.6961,  0.9450,\n",
      "          0.1879, -0.3639],\n",
      "        [ 0.0036, -0.5386, -1.3375, -0.5841,  0.0916, -1.3121, -0.6515, -0.2979,\n",
      "          1.0804, -0.0919],\n",
      "        [-0.2241,  0.4265,  0.0054, -0.0826, -0.5758, -0.6925,  0.0994, -0.6229,\n",
      "         -0.8576,  0.3012],\n",
      "        [ 0.2235,  1.4409,  0.8928,  0.7223, -0.6116, -1.6856, -0.0596, -0.7095,\n",
      "         -1.2028,  0.3562],\n",
      "        [-0.0972,  0.3060,  0.6573,  0.3085, -0.3260, -0.3235,  0.5225, -1.1482,\n",
      "         -1.2006,  0.5086],\n",
      "        [-0.6056, -1.3465, -1.2345, -0.8795,  0.2474,  0.7569, -0.2256, -0.3199,\n",
      "          1.2152,  0.7675],\n",
      "        [-0.3077, -0.2454, -0.5742,  0.2105, -0.5111,  0.5800, -0.4270, -0.8334,\n",
      "          0.1129,  1.0073],\n",
      "        [-0.2244, -0.7581, -0.1726, -0.9786, -0.1148,  0.3047, -0.1370, -1.2486,\n",
      "         -0.0082,  1.2642],\n",
      "        [ 0.7363, -1.0554, -1.2198,  0.1757,  0.6501,  0.1162,  0.4916,  0.8010,\n",
      "          0.5142, -1.2458],\n",
      "        [ 1.0706, -0.3344, -0.2811,  0.7619, -0.2338, -0.6389,  0.8668,  0.1817,\n",
      "         -0.1951, -0.7718],\n",
      "        [ 0.1537,  0.1790, -0.4062,  0.0823,  0.2425,  0.9443, -0.2859,  0.6139,\n",
      "         -0.0501,  0.9342],\n",
      "        [ 0.4157, -0.0185, -0.1596,  0.5004,  1.3144, -1.0198,  0.6735,  0.8632,\n",
      "          0.6933, -0.1800],\n",
      "        [ 0.0273, -1.5596, -0.8260, -0.6566,  0.2447, -0.1912,  0.6333, -0.5165,\n",
      "          0.7057, -0.2742],\n",
      "        [-0.8096,  0.0310, -1.1589,  0.8551, -0.2126,  0.7085, -0.1717, -0.6420,\n",
      "          0.0095,  1.4974],\n",
      "        [ 0.4610, -0.2436, -0.2599,  0.9264,  0.3943, -0.1581,  0.8231,  0.2787,\n",
      "          0.1979, -0.0723],\n",
      "        [ 0.1962, -1.0721, -0.9161, -0.4365, -0.2829, -0.2950,  0.3255, -0.6865,\n",
      "          0.4168,  0.2679],\n",
      "        [-0.3913,  0.3830,  0.2195,  0.3993,  0.5711, -0.2029, -0.2336, -0.0846,\n",
      "          0.0195,  0.1945],\n",
      "        [ 0.2759, -0.5956, -0.4890,  0.3098, -0.2145,  0.5754,  0.8844,  0.1313,\n",
      "          0.1245,  0.6919]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Test(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10, 10)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        print('flag')\n",
    "        for i in range(10):\n",
    "            x = self.linear(input)\n",
    "            print(\"this is {} epoch\".format(i))\n",
    "            print(\"x is {}\".format(x))\n",
    "            print('\\n')\n",
    "\n",
    "tensor = torch.randn(20, 10)\n",
    "test = Test()\n",
    "test(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers import BartConfig, T5Config\n",
    "\n",
    "class PromptBartConfig(BartConfig):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.pre_seq_len = config.pre_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]\n",
      "[CLS]\n",
      "0\n",
      "Hello\n",
      "0\n",
      "[MASK]\n",
      "0\n",
      "world\n",
      "0\n",
      "[SEP]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['[CLS]', '[SEP]']\n",
    "print(special_tokens[0])\n",
    "seq = ['[CLS]', 'Hello', '[MASK]', 'world', '[SEP]']\n",
    "for s in seq:\n",
    "    print(s)\n",
    "    drop_mask = sum([seq == t for t in special_tokens])\n",
    "    print(drop_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'Hello', '[MASK]', 'world', '[SEP]']\n",
      "0\n",
      "[True]\n"
     ]
    }
   ],
   "source": [
    "special_tokens = ['[CLS]', '[SEP]']\n",
    "seq = [['[CLS]', 'Hello', '[MASK]', 'world', '[SEP]'],]\n",
    "\n",
    "for s in seq:\n",
    "    print(s)\n",
    "    drop_mask = sum([s == t for t in special_tokens])\n",
    "    print(drop_mask)\n",
    "    drop_mask = [bool(1 - drop_mask)]\n",
    "    print(drop_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "special_token_ids = []\n",
    "config = {\n",
    "    \"max_n_segments\": 4,\n",
    "}\n",
    "\n",
    "def pad_and_segment(input_ids, config):\n",
    "    \"\"\"\n",
    "    segment input_ids into segments\n",
    "    be careful that all the segments are treated as one input sequence \n",
    "    and dealed with incurrence \n",
    "    \"\"\"\n",
    "    segmented_batch = []\n",
    "    # input_ids: [batch_size, seq_len]\n",
    "    for seq in input_ids:\n",
    "        print('this is the origin seq: {}'.format(seq))\n",
    "        drop_mask = sum([seq == t for t in special_token_ids])\n",
    "        seq = seq[(1 - drop_mask)]\n",
    "        seq = seq[:self.config.segment_size * config.max_n_segments]\n",
    "        print('this is the seq after drop_mask: {}'.format(seq))\n",
    "        \n",
    "        align = self.config.segment_alignment\n",
    "        if align in {'right', None}:\n",
    "            split_inds = (list(range(len(seq), 0, -config.segment_size)) + [0])[::-1]\n",
    "        elif align == 'left':\n",
    "            split_inds = list(range(0, len(seq), config.segment_size)) + [len(seq)]\n",
    "        elif align == 'center':\n",
    "            n_seg = math.ceil(len(seq) / config.segment_size)\n",
    "            split_inds = list(range(0, len(seq), math.ceil(len(seq) / n_seg))) + [len(seq)]\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        input_segments = [seq[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "        # TODO: do the implementation\n",
    "        # input_segments = [self.pad_add_special_tokens(t, self.config.input_size) for t in input_segments]\n",
    "        print('this is the input_segments: {}'.format(input_segments))\n",
    "        \n",
    "        # add empty segment markers if needed\n",
    "        n_empty_segments = config.max_n_segments - len(input_segments)\n",
    "        \n",
    "        # input_segments:\n",
    "        input_segments = [None] * n_empty_segments + input_segments\n",
    "        print('this is the input_segments after adding empty segment markers: {}'.format(input_segments))\n",
    "        \n",
    "        # segmented_batch: \n",
    "        segmented_batch.append(input_segments)\n",
    "        print('this is the segmented_batch: {}'.format(segmented_batch))\n",
    "        print('first sample in segmented_batch: {}'.format(segmented_batch[0]))\n",
    "        \n",
    "    segmented_batch = [[sample[seg_num] for sample in segmented_batch] \\\n",
    "                        for seg_num in range(config.max_n_segments)]\n",
    "    print('this is the segmented_batch after re-arrange: {}'.format(segmented_batch))\n",
    "    return segmented_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won\\'t cast a spell on him. Daniel Radcliffe as Harry Potter in \"Harry Potter and the Order of the Phoenix\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \"I don\\'t plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\" he told an Australian interviewer earlier this month. \"I don\\'t think I\\'ll be particularly extravagant. \"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \"Hostel: Part II,\" currently six places below his number one movie on the UK box office chart. Details of how he\\'ll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \"I\\'ll definitely have some sort of party,\" he said in an interview. \"Hopefully none of you will be reading about it.\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \"People are always looking to say \\'kid star goes off the rails,\\'\" he told reporters last month. \"But I try very hard not to go that way because it would be too easy for them.\" His latest outing as the boy wizard in \"Harry Potter and the Order of the Phoenix\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films. Watch I-Reporter give her review of Potter\\'s latest  . There is life beyond Potter, however. The Londoner has filmed a TV movie called \"My Boy Jack,\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \"December Boys,\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer\\'s \"Equus.\" Meanwhile, he is braced for even closer media scrutiny now that he\\'s legally an adult: \"I just think I\\'m going to be more sort of fair game,\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = \"LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \\\"Harry Potter and the Order of the Phoenix\\\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \\\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\\\" he told an Australian interviewer earlier this month. \\\"I don't think I'll be particularly extravagant. \\\"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\\\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \\\"Hostel: Part II,\\\" currently six places below his number one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \\\"I\\'ll definitely have some sort of party,\\\" he said in an interview. \\\"Hopefully none of you will be reading about it.\\\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \\\"People are always looking to say 'kid star goes off the rails,'\\\" he told reporters last month. \\\"But I try very hard not to go that way because it would be too easy for them.\\\" His latest outing as the boy wizard in \\\"Harry Potter and the Order of the Phoenix\\\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films. Watch I-Reporter give her review of Potter's latest  . There is life beyond Potter, however. The Londoner has filmed a TV movie called \\\"My Boy Jack,\\\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \\\"December Boys,\\\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer's \\\"Equus.\\\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \\\"I just think I'm going to be more sort of fair game,\\\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.\"\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [\n",
    "    [\"LONDON, England (Reuters) -- Harry Potter star Daniel Radcliffe gains access to a reported £20 million ($41.1 million) fortune as he turns 18 on Monday, but he insists the money won't cast a spell on him. Daniel Radcliffe as Harry Potter in \\\"Harry Potter and the Order of the Phoenix\\\" To the disappointment of gossip columnists around the world, the young actor says he has no plans to fritter his cash away on fast cars, drink and celebrity parties. \\\"I don't plan to be one of those people who, as soon as they turn 18, suddenly buy themselves a massive sports car collection or something similar,\\\" he told an Australian interviewer earlier this month. \\\"I don't think I'll be particularly extravagant. \\\"The things I like buying are things that cost about 10 pounds -- books and CDs and DVDs.\\\" At 18, Radcliffe will be able to gamble in a casino, buy a drink in a pub or see the horror film \\\"Hostel: Part II,\\\" currently six places below his number one movie on the UK box office chart. Details of how he'll mark his landmark birthday are under wraps. His agent and publicist had no comment on his plans. \\\"I\\'ll definitely have some sort of party,\\\" he said in an interview. \\\"Hopefully none of you will be reading about it.\\\" Radcliffe\\'s earnings from the first five Potter films have been held in a trust fund which he has not been able to touch. Despite his growing fame and riches, the actor says he is keeping his feet firmly on the ground. \\\"People are always looking to say 'kid star goes off the rails,'\\\" he told reporters last month. \\\"But I try very hard not to go that way because it would be too easy for them.\\\" His latest outing as the boy wizard in \\\"Harry Potter and the Order of the Phoenix\\\" is breaking records on both sides of the Atlantic and he will reprise the role in the last two films. Watch I-Reporter give her review of Potter's latest  . There is life beyond Potter, however. The Londoner has filmed a TV movie called \\\"My Boy Jack,\\\" about author Rudyard Kipling and his son, due for release later this year. He will also appear in \\\"December Boys,\\\" an Australian film about four boys who escape an orphanage. Earlier this year, he made his stage debut playing a tortured teenager in Peter Shaffer's \\\"Equus.\\\" Meanwhile, he is braced for even closer media scrutiny now that he's legally an adult: \\\"I just think I'm going to be more sort of fair game,\\\" he told Reuters. E-mail to a friend . Copyright 2007 Reuters. All rights reserved.This material may not be published, broadcast, rewritten, or redistributed.\"]\n",
    "]\n",
    "\n",
    "pad_and_segment(batch, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一步是将处理后的数据集 `segmented_batch` 进行转置，使得每个子句在第一维度，即 `batch_size = 1`。原始的 `segmented_batch` 是一个列表的列表，其中每个列表代表一个样本，而每个样本可能被分成多个子句。而在这个步骤中，我们将所有样本的第 `seg_num` 个子句提取出来，形成一个新的列表，并将这些列表组成一个新的 `segmented_batch` 列表。\n",
    "\n",
    "举个例子，假设原始的 `segmented_batch` 如下：\n",
    "\n",
    "```python\n",
    "segmented_batch = [[sample1_seg1, sample1_seg2, sample1_seg3],\n",
    "                   [sample2_seg1, sample2_seg2],\n",
    "                   [sample3_seg1, sample3_seg2, sample3_seg3, sample3_seg4]]\n",
    "```\n",
    "\n",
    "其中 `sample1_seg1` 表示第一个样本的第一个子句，`sample1_seg2` 表示第一个样本的第二个子句，以此类推。\n",
    "\n",
    "经过转置后的 `segmented_batch` 如下：\n",
    "\n",
    "```python\n",
    "segmented_batch = [[sample1_seg1, sample2_seg1, sample3_seg1],\n",
    "                   [sample1_seg2, sample2_seg2, sample3_seg2],\n",
    "                   [sample1_seg3, None, sample3_seg3],\n",
    "                   [None, None, sample3_seg4]]\n",
    "```\n",
    "\n",
    "注意，这里可能会出现 `None`，因为不同样本可能具有不同数量的子句。这种转置操作通常用于将一个批次的数据转换为序列模型的输入，使得每个子句在第一维度，方便进行后续的处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq: tensor([101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 102, 111, 112, 113,\n",
      "        114, 115, 102, 116, 117, 102])\n",
      "drop_mask: tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1])\n",
      "seq: tensor([103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "        117])\n",
      "input_segments flag 1: [tensor([103, 104, 105, 106, 107]), tensor([108, 109, 110, 111, 112]), tensor([113, 114, 115, 116, 117])]\n",
      "input_segments flag 2: [None, None, None, None, None, None, None, tensor([103, 104, 105, 106, 107]), tensor([108, 109, 110, 111, 112]), tensor([113, 114, 115, 116, 117])]\n",
      "segmented_batch: [[None, None, None, None, None, None, None, tensor([103, 104, 105, 106, 107]), tensor([108, 109, 110, 111, 112]), tensor([113, 114, 115, 116, 117])]]\n",
      "seq: tensor([101, 102, 103, 104, 105, 106, 102, 107, 108, 109, 110, 111, 102, 112,\n",
      "        113, 114, 115, 102, 116, 117])\n",
      "drop_mask: tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0])\n",
      "seq: tensor([103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "        117])\n",
      "input_segments flag 1: [tensor([103, 104, 105, 106, 107]), tensor([108, 109, 110, 111, 112]), tensor([113, 114, 115, 116, 117])]\n",
      "input_segments flag 2: [None, None, None, None, None, None, None, tensor([103, 104, 105, 106, 107]), tensor([108, 109, 110, 111, 112]), tensor([113, 114, 115, 116, 117])]\n",
      "segmented_batch: [[None, None, None, None, None, None, None, tensor([103, 104, 105, 106, 107]), tensor([108, 109, 110, 111, 112]), tensor([113, 114, 115, 116, 117])], [None, None, None, None, None, None, None, tensor([103, 104, 105, 106, 107]), tensor([108, 109, 110, 111, 112]), tensor([113, 114, 115, 116, 117])]]\n",
      "=============\n",
      "segmented_batch: [[None, None], [None, None], [None, None], [None, None], [None, None], [None, None], [None, None], [tensor([103, 104, 105, 106, 107]), tensor([103, 104, 105, 106, 107])], [tensor([108, 109, 110, 111, 112]), tensor([108, 109, 110, 111, 112])], [tensor([113, 114, 115, 116, 117]), tensor([113, 114, 115, 116, 117])]]\n",
      "Original input_ids:\n",
      "tensor([[101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 102, 111, 112, 113,\n",
      "         114, 115, 102, 116, 117, 102],\n",
      "        [101, 102, 103, 104, 105, 106, 102, 107, 108, 109, 110, 111, 102, 112,\n",
      "         113, 114, 115, 102, 116, 117]])\n",
      "Processed input_ids:\n",
      "[[None, None], [None, None], [None, None], [None, None], [None, None], [None, None], [None, None], [tensor([103, 104, 105, 106, 107]), tensor([103, 104, 105, 106, 107])], [tensor([108, 109, 110, 111, 112]), tensor([108, 109, 110, 111, 112])], [tensor([113, 114, 115, 116, 117]), tensor([113, 114, 115, 116, 117])]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_ids = torch.tensor([\n",
    "    [101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 102, 111, 112, 113, 114, 115, 102, 116, 117, 102],\n",
    "    [101, 102, 103, 104, 105, 106, 102, 107, 108, 109, 110, 111, 102, 112, 113, 114, 115, 102, 116, 117]\n",
    "])\n",
    "\n",
    "special_token_ids = [101, 102]\n",
    "\n",
    "\n",
    "\n",
    "class Processor:\n",
    "    def __init__(self, segment_size, rmt_config, special_token_ids):\n",
    "        self.segment_size = segment_size\n",
    "        self.rmt_config = rmt_config\n",
    "        self.special_token_ids = special_token_ids\n",
    "\n",
    "    def pad_and_segment(self, input_ids):\n",
    "        segmented_batch = []\n",
    "        for seq in input_ids:\n",
    "            print('seq:', seq)\n",
    "            drop_mask = sum([seq == t for t in self.special_token_ids])\n",
    "            print('drop_mask:', drop_mask)\n",
    "            seq = seq[(1 - drop_mask).bool()]\n",
    "            print('seq:', seq)\n",
    "            seq = seq[:self.segment_size * self.rmt_config['max_n_segments']]\n",
    "            \n",
    "            align = self.rmt_config['segment_alignment']\n",
    "            if align in {'right', None}:\n",
    "                split_inds = (list(range(len(seq), 0, -self.rmt_config['segment_size'])) + [0])[::-1]\n",
    "            elif align == 'left':\n",
    "                split_inds = list(range(0, len(seq), self.rmt_config['segment_size'])) + [len(seq)]\n",
    "            elif align == 'center':\n",
    "                n_seg = math.ceil(len(seq) / self.rmt_config['segment_size'])\n",
    "                split_inds = list(range(0, len(seq), math.ceil(len(seq) / n_seg))) + [len(seq)]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            input_segments = [seq[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "            print('input_segments flag 1:', input_segments)\n",
    "            # TODO: do the implementation\n",
    "            # input_segments = [self.pad_add_special_tokens(t, self.config.input_size) for t in input_segments]\n",
    "            \n",
    "            # add empty segment markers if needed\n",
    "            n_empty_segments = self.rmt_config['max_n_segments'] - len(input_segments)\n",
    "            # input_segments:\n",
    "            input_segments = [None] * n_empty_segments + input_segments\n",
    "            print('input_segments flag 2:', input_segments)\n",
    "            # segmented_batch: \n",
    "            segmented_batch.append(input_segments)\n",
    "            print('segmented_batch:', segmented_batch)\n",
    "            \n",
    "        print('=============')\n",
    "        segmented_batch = [[sample[seg_num] for sample in segmented_batch] \\\n",
    "                    for seg_num in range(self.rmt_config['max_n_segments'])]\n",
    "        print('segmented_batch:', segmented_batch)\n",
    "        return segmented_batch\n",
    "\n",
    "# 假设我们有一个名为 rmt_config 的配置字典和 segment_size 变量\n",
    "rmt_config = {'max_n_segments': 10,\n",
    "              'segment_size': 5,\n",
    "              'segment_alignment': 'right',}\n",
    "segment_size = 5\n",
    "\n",
    "processor = Processor(segment_size, rmt_config, special_token_ids)\n",
    "output = processor.pad_and_segment(input_ids)\n",
    "\n",
    "print(\"Original input_ids:\")\n",
    "print(input_ids)\n",
    "print(\"Processed input_ids:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0, 42891, 301, 4, 99, 10, 4613, 232, 2], [101, 7592, 2026, 2166, 102])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BartTokenizerFast\n",
    "\n",
    "bart_tokenizer = BartTokenizerFast.from_pretrained('facebook/bart-large-cnn')\n",
    "bart_tokenizer('hello life. what a wonderful world')['input_ids'], tokenizer('hello my life')['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [31414, 232], 'attention_mask': [1, 1]}"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = bart_tokenizer('Hello world', add_special_tokens=False)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Key input_ids is not a special token",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[119], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test \u001b[39m=\u001b[39m bart_tokenizer\u001b[39m.\u001b[39;49madd_special_tokens(test)\n\u001b[1;32m      2\u001b[0m test\n",
      "File \u001b[0;32m~/miniconda3/envs/summarization/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:904\u001b[0m, in \u001b[0;36mSpecialTokensMixin.add_special_tokens\u001b[0;34m(self, special_tokens_dict, replace_additional_special_tokens)\u001b[0m\n\u001b[1;32m    902\u001b[0m added_tokens \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m    903\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m special_tokens_dict\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 904\u001b[0m     \u001b[39massert\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSPECIAL_TOKENS_ATTRIBUTES, \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mKey \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m is not a special token\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    906\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose:\n\u001b[1;32m    907\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAssigning \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m to the \u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m key of the tokenizer\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: Key input_ids is not a special token"
     ]
    }
   ],
   "source": [
    "test = bart_tokenizer.add_special_tokens(test)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>Hello world</s>'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoded = bart_tokenizer.decode(test)\n",
    "decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 7592, 2088,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_test = tokenizer('Hello world', return_tensors='pt', add_special_tokens=True)\n",
    "bert_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] hello world [SEP]'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_test_decoded = tokenizer.decode(bert_test['input_ids'][0])\n",
    "bert_test_decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'sep_token': '</s>',\n",
       " 'pad_token': '<pad>',\n",
       " 'cls_token': '<s>',\n",
       " 'mask_token': '<mask>'}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_tokenizer.pad_token_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   713,    16,     5,    78, 48652,     4,   152,    16,     5,\n",
       "         15636,  1397, 48652,     4,   152,    16,     5,   371, 48652,     4,\n",
       "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
       "             1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0]])}"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sentence = 'This is the first seq. This is the secend seq. This is the third seq.'\n",
    "\n",
    "encoded_test_sentence = bart_tokenizer(test_sentence, return_tensors='pt')\n",
    "encoded_test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    0,   713,    16,     5,    78, 48652,     4,   152,    16,     5,\n",
      "         15636,  1397, 48652,     4,   152,    16,     5,   371, 48652,     4,\n",
      "             2,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0]])}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    0,   713,    16,     5,    78, 48652,     4,   152,    16,     5,\n",
       "         15636,  1397, 48652,     4,   152,    16,     5,   371, 48652,     4,\n",
       "             2]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(encoded_test_sentence)\n",
    "encoded_test_sentence = bart_tokenizer(test_sentence, return_tensors='pt', add_special_tokens=True)\n",
    "encoded_test_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[159], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m encoded_test_sentence[\u001b[39m'\u001b[39;49m\u001b[39minput_ids\u001b[39;49m\u001b[39m'\u001b[39;49m]\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 2"
     ]
    }
   ],
   "source": [
    "encoded_test_sentence['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_tokenizer.num_special_tokens_to_add()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = \"Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13014"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(long_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = bart_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 21001, 2339, 6, 101, 557, 6665, 6, 6441, 7201, 6, 50, 2799, 6, 747, 5585, 10, 4764, 9, 335, 8, 8418, 4, 1216, 2339, 64, 1719, 2632, 7614, 6, 1455, 4271, 7576, 6, 8, 694, 4935, 1283, 7, 323, 49, 1449, 4, 635, 6, 5516, 215, 251, 14301, 64, 28, 4087, 6, 941, 77, 447, 19, 3092, 14, 33, 8135, 5933, 11948, 4, 282, 1632, 2777, 5774, 8558, 6, 101, 2788, 20257, 50, 2777, 2706, 6, 24, 16, 1537, 7, 304, 11857, 26492, 40878, 3092, 101, 163, 18854, 6, 272, 10311, 12, 246, 6, 50, 255, 245, 4, 1216, 3092, 3700, 33, 10, 4532, 8135, 5933, 9, 29600, 22121, 6, 61, 839, 51, 1395, 2024, 3679, 2339, 14, 11514, 42, 3000, 4, 3972, 609, 251, 2339, 19, 11857, 26492, 3092, 6, 10, 1537, 1548, 16, 7, 3462, 5, 2788, 88, 2735, 5561, 50, 26521, 6, 609, 349, 2835, 12672, 19, 5, 1421, 6, 8, 172, 9637, 5, 775, 4, 870, 26667, 5, 3780, 88, 2735, 1667, 6, 349, 2835, 64, 2564, 624, 5, 1421, 18, 8135, 5933, 16311, 4, 10462, 6, 42, 1548, 3441, 7316, 5516, 7, 1306, 14, 5, 9696, 109, 45, 12414, 5, 1029, 40584, 8, 5377, 9, 5, 3780, 4, 993, 4964, 6877, 634, 15599, 6410, 6, 1271, 780, 22121, 7, 2458, 5, 1786, 8, 253, 9, 5561, 6, 50, 634, 41, 27573, 227, 5561, 7, 8415, 5377, 4, 40450, 8, 5485, 747, 5731, 6777, 2643, 7, 3692, 251, 3780, 5774, 6, 6122, 15, 49, 2167, 304, 403, 8, 1421, 3471, 4, 21001, 2339, 6, 101, 557, 6665, 6, 6441, 7201, 6, 50, 2799, 6, 747, 5585, 10, 4764, 9, 335, 8, 8418, 4, 1216, 2339, 64, 1719, 2632, 7614, 6, 1455, 4271, 7576, 6, 8, 694, 4935, 1283, 7, 323, 49, 1449, 4, 635, 6, 5516, 215, 251, 14301, 64, 28, 4087, 6, 941, 77, 447, 19, 3092, 14, 33, 8135, 5933, 11948, 4, 282, 1632, 2777, 5774, 8558, 6, 101, 2788, 20257, 50, 2777, 2706, 6, 24, 16, 1537, 7, 304, 11857, 26492, 40878, 3092, 101, 163, 18854, 6, 272, 10311, 12, 246, 6, 50, 255, 245, 4, 1216, 3092, 3700, 33, 10, 4532, 8135, 5933, 9, 29600, 22121, 6, 61, 839, 51, 1395, 2024, 3679, 2339, 14, 11514, 42, 3000, 4, 3972, 609, 251, 2339, 19, 11857, 26492, 3092, 6, 10, 1537, 1548, 16, 7, 3462, 5, 2788, 88, 2735, 5561, 50, 26521, 6, 609, 349, 2835, 12672, 19, 5, 1421, 6, 8, 172, 9637, 5, 775, 4, 870, 26667, 5, 3780, 88, 2735, 1667, 6, 349, 2835, 64, 2564, 624, 5, 1421, 18, 8135, 5933, 16311, 4, 10462, 6, 42, 1548, 3441, 7316, 5516, 7, 1306, 14, 5, 9696, 109, 45, 12414, 5, 1029, 40584, 8, 5377, 9, 5, 3780, 4, 993, 4964, 6877, 634, 15599, 6410, 6, 1271, 780, 22121, 7, 2458, 5, 1786, 8, 253, 9, 5561, 6, 50, 634, 41, 27573, 227, 5561, 7, 8415, 5377, 4, 40450, 8, 5485, 747, 5731, 6777, 2643, 7, 3692, 251, 3780, 5774, 6, 6122, 15, 49, 2167, 304, 403, 8, 1421, 3471, 4, 21001, 2339, 6, 101, 557, 6665, 6, 6441, 7201, 6, 50, 2799, 6, 747, 5585, 10, 4764, 9, 335, 8, 8418, 4, 1216, 2339, 64, 1719, 2632, 7614, 6, 1455, 4271, 7576, 6, 8, 694, 4935, 1283, 7, 323, 49, 1449, 4, 635, 6, 5516, 215, 251, 14301, 64, 28, 4087, 6, 941, 77, 447, 19, 3092, 14, 33, 8135, 5933, 11948, 4, 282, 1632, 2777, 5774, 8558, 6, 101, 2788, 20257, 50, 2777, 2706, 6, 24, 16, 1537, 7, 304, 11857, 26492, 40878, 3092, 101, 163, 18854, 6, 272, 10311, 12, 246, 6, 50, 255, 245, 4, 1216, 3092, 3700, 33, 10, 4532, 8135, 5933, 9, 29600, 22121, 6, 61, 839, 51, 1395, 2024, 3679, 2339, 14, 11514, 42, 3000, 4, 3972, 609, 251, 2339, 19, 11857, 26492, 3092, 6, 10, 1537, 1548, 16, 7, 3462, 5, 2788, 88, 2735, 5561, 50, 26521, 6, 609, 349, 2835, 12672, 19, 5, 1421, 6, 8, 172, 9637, 5, 775, 4, 870, 26667, 5, 3780, 88, 2735, 1667, 6, 349, 2835, 64, 2564, 624, 5, 1421, 18, 8135, 5933, 16311, 4, 10462, 6, 42, 1548, 3441, 7316, 5516, 7, 1306, 14, 5, 9696, 109, 45, 12414, 5, 1029, 40584, 8, 5377, 9, 5, 3780, 4, 993, 4964, 6877, 634, 15599, 6410, 6, 1271, 780, 22121, 7, 2458, 5, 1786, 8, 253, 9, 5561, 6, 50, 634, 41, 27573, 227, 5561, 7, 8415, 5377, 4, 40450, 8, 5485, 747, 5731, 6777, 2643, 7, 3692, 251, 3780, 5774, 6, 6122, 15, 49, 2167, 304, 403, 8, 1421, 3471, 4, 21001, 2339, 6, 101, 557, 6665, 6, 6441, 7201, 6, 50, 2799, 6, 747, 5585, 10, 4764, 9, 335, 8, 8418, 4, 1216, 2339, 64, 1719, 2632, 7614, 6, 1455, 4271, 7576, 6, 8, 694, 4935, 1283, 7, 323, 49, 1449, 4, 635, 6, 5516, 215, 251, 14301, 64, 28, 4087, 6, 941, 77, 447, 19, 3092, 14, 33, 8135, 5933, 11948, 4, 282, 1632, 2777, 5774, 8558, 6, 101, 2788, 20257, 50, 2777, 2706, 6, 24, 16, 1537, 7, 304, 11857, 26492, 40878, 3092, 101, 163, 18854, 6, 272, 10311, 12, 246, 6, 50, 255, 245, 4, 1216, 3092, 3700, 33, 10, 4532, 8135, 5933, 9, 29600, 22121, 6, 61, 839, 51, 1395, 2024, 3679, 2339, 14, 11514, 42, 3000, 4, 3972, 609, 251, 2339, 19, 11857, 26492, 3092, 6, 10, 1537, 1548, 16, 7, 3462, 5, 2788, 88, 2735, 5561, 50, 26521, 6, 609, 349, 2835, 12672, 19, 5, 1421, 6, 8, 172, 9637, 5, 775, 4, 870, 26667, 5, 3780, 88, 2735, 1667, 6, 349, 2835, 64, 2564, 624, 5, 1421, 18, 8135, 5933, 16311, 4, 10462, 6, 42, 1548, 3441, 7316, 5516, 7, 1306, 14, 5, 9696, 109, 45, 12414, 5, 1029, 40584, 8, 5377, 9, 5, 3780, 4, 993, 4964, 6877, 634, 15599, 6410, 6, 1271, 780, 22121, 7, 2458, 5, 1786, 8, 253, 9, 5561, 6, 50, 634, 41, 27573, 227, 5561, 7, 8415, 5377, 4, 40450, 8, 5485, 747, 5731, 6777, 2643, 7, 3692, 251, 3780, 5774, 6, 6122, 15, 49, 2167, 304, 403, 8, 1421, 3471, 4, 21001, 2339, 6, 101, 557, 6665, 6, 6441, 7201, 6, 50, 2799, 6, 747, 5585, 10, 4764, 9, 335, 8, 8418, 4, 1216, 2339, 64, 1719, 2632, 7614, 6, 1455, 4271, 7576, 6, 8, 694, 4935, 1283, 7, 323, 49, 1449, 4, 635, 6, 5516, 215, 251, 14301, 64, 28, 4087, 6, 941, 77, 447, 19, 3092, 14, 33, 8135, 5933, 11948, 4, 282, 1632, 2777, 5774, 8558, 6, 101, 2788, 20257, 50, 2777, 2706, 6, 24, 16, 1537, 7, 304, 11857, 26492, 40878, 3092, 101, 163, 18854, 6, 272, 10311, 12, 246, 6, 50, 255, 245, 4, 1216, 3092, 3700, 33, 10, 4532, 8135, 5933, 9, 29600, 22121, 6, 61, 839, 51, 1395, 2024, 3679, 2339, 14, 11514, 42, 3000, 4, 3972, 609, 251, 2339, 19, 11857, 26492, 3092, 6, 10, 1537, 1548, 16, 7, 3462, 5, 2788, 88, 2735, 5561, 50, 26521, 6, 609, 349, 2835, 12672, 19, 5, 1421, 6, 8, 172, 9637, 5, 775, 4, 870, 26667, 5, 3780, 88, 2735, 1667, 6, 349, 2835, 64, 2564, 624, 5, 1421, 18, 8135, 5933, 16311, 4, 10462, 6, 42, 1548, 3441, 7316, 5516, 7, 1306, 14, 5, 9696, 109, 45, 12414, 5, 1029, 40584, 8, 5377, 9, 5, 3780, 4, 993, 4964, 6877, 634, 15599, 6410, 6, 1271, 780, 22121, 7, 2458, 5, 1786, 8, 253, 9, 5561, 6, 50, 634, 41, 27573, 227, 5561, 7, 8415, 5377, 4, 40450, 8, 5485, 747, 5731, 6777, 2643, 7, 3692, 251, 3780, 5774, 6, 6122, 15, 49, 2167, 304, 403, 8, 1421, 3471, 4, 21001, 2339, 6, 101, 557, 6665, 6, 6441, 7201, 6, 50, 2799, 6, 747, 5585, 10, 4764, 9, 335, 8, 8418, 4, 1216, 2339, 64, 1719, 2632, 7614, 6, 1455, 4271, 7576, 6, 8, 694, 4935, 1283, 7, 323, 49, 1449, 4, 635, 6, 5516, 215, 251, 14301, 64, 28, 4087, 6, 941, 77, 447, 19, 3092, 14, 33, 8135, 5933, 11948, 4, 282, 1632, 2777, 5774, 8558, 6, 101, 2788, 20257, 50, 2777, 2706, 6, 24, 16, 1537, 7, 304, 11857, 26492, 40878, 3092, 101, 163, 18854, 6, 272, 10311, 12, 246, 6, 50, 255, 245, 4, 1216, 3092, 3700, 33, 10, 4532, 8135, 5933, 9, 29600, 22121, 6, 61, 839, 51, 1395, 2024, 3679, 2339, 14, 11514, 42, 3000, 4, 3972, 609, 251, 2339, 19, 11857, 26492, 3092, 6, 10, 1537, 1548, 16, 7, 3462, 5, 2788, 88, 2735, 5561, 50, 26521, 6, 609, 349, 2835, 12672, 19, 5, 1421, 6, 8, 172, 9637, 5, 775, 4, 870, 26667, 5, 3780, 88, 2735, 1667, 6, 349, 2835, 64, 2564, 624, 5, 1421, 18, 8135, 5933, 16311, 4, 10462, 6, 42, 1548, 3441, 7316, 5516, 7, 1306, 14, 5, 9696, 109, 45, 12414, 5, 1029, 40584, 8, 5377, 9, 5, 3780, 4, 993, 4964, 6877, 634, 15599, 6410, 6, 1271, 780, 22121, 7, 2458, 5, 1786, 8, 253, 9, 5561, 6, 50, 634, 41, 27573, 227, 5561, 7, 8415, 5377, 4, 40450, 8, 5485, 747, 5731, 6777, 2643, 7, 3692, 251, 3780, 5774, 6, 6122, 15, 49, 2167, 304, 403, 8, 1421, 3471, 4, 21001, 2339, 6, 101, 557, 6665, 6, 6441, 7201, 6, 50, 2799, 6, 747, 5585, 10, 4764, 9, 335, 8, 8418, 4, 1216, 2339, 64, 1719, 2632, 7614, 6, 1455, 4271, 7576, 6, 8, 694, 4935, 1283, 7, 323, 49, 1449, 4, 635, 6, 5516, 215, 251, 14301, 64, 28, 4087, 6, 941, 77, 447, 19, 3092, 14, 33, 8135, 5933, 11948, 4, 282, 1632, 2777, 5774, 8558, 6, 101, 2788, 20257, 50, 2777, 2706, 6, 24, 16, 1537, 7, 304, 11857, 26492, 40878, 3092, 101, 163, 18854, 6, 272, 10311, 12, 246, 6, 50, 255, 245, 4, 1216, 3092, 3700, 33, 10, 4532, 8135, 5933, 9, 29600, 22121, 6, 61, 839, 51, 1395, 2024, 3679, 2339, 14, 11514, 42, 3000, 4, 3972, 609, 251, 2339, 19, 11857, 26492, 3092, 6, 10, 1537, 1548, 16, 7, 3462, 5, 2788, 88, 2735, 5561, 50, 26521, 6, 609, 349, 2835, 12672, 19, 5, 1421, 6, 8, 172, 9637, 5, 775, 4, 870, 26667, 5, 3780, 88, 2735, 1667, 6, 349, 2835, 64, 2564, 624, 5, 1421, 18, 8135, 5933, 16311, 4, 10462, 6, 42, 1548, 3441, 7316, 5516, 7, 1306, 14, 5, 9696, 109, 45, 12414, 5, 1029, 40584, 8, 5377, 9, 5, 3780, 4, 993, 4964, 6877, 634, 15599, 6410, 6, 1271, 780, 22121, 7, 2458, 5, 1786, 8, 253, 9, 5561, 6, 50, 634, 41, 27573, 227, 5561, 7, 8415, 5377, 4, 40450, 8, 5485, 747, 5731, 6777, 2643, 7, 3692, 251, 3780, 5774, 6, 6122, 15, 49, 2167, 304, 403, 8, 1421, 3471, 4, 21001, 2339, 6, 101, 557, 6665, 6, 6441, 7201, 6, 50, 2799, 6, 747, 5585, 10, 4764, 9, 335, 8, 8418, 4, 1216, 2339, 64, 1719, 2632, 7614, 6, 1455, 4271, 7576, 6, 8, 694, 4935, 1283, 7, 323, 49, 1449, 4, 635, 6, 5516, 215, 251, 14301, 64, 28, 4087, 6, 941, 77, 447, 19, 3092, 14, 33, 8135, 5933, 11948, 4, 282, 1632, 2777, 5774, 8558, 6, 101, 2788, 20257, 50, 2777, 2706, 6, 24, 16, 1537, 7, 304, 11857, 26492, 40878, 3092, 101, 163, 18854, 6, 272, 10311, 12, 246, 6, 50, 255, 245, 4, 1216, 3092, 3700, 33, 10, 4532, 8135, 5933, 9, 29600, 22121, 6, 61, 839, 51, 1395, 2024, 3679, 2339, 14, 11514, 42, 3000, 4, 3972, 609, 251, 2339, 19, 11857, 26492, 3092, 6, 10, 1537, 1548, 16, 7, 3462, 5, 2788, 88, 2735, 5561, 50, 26521, 6, 609, 349, 2835, 12672, 19, 5, 1421, 6, 8, 172, 9637, 5, 775, 4, 870, 26667, 5, 3780, 88, 2735, 1667, 6, 349, 2835, 64, 2564, 624, 5, 1421, 18, 8135, 5933, 16311, 4, 10462, 6, 42, 1548, 3441, 7316, 5516, 7, 1306, 14, 5, 9696, 109, 45, 12414, 5, 1029, 40584, 8, 5377, 9, 5, 3780, 4, 993, 4964, 6877, 634, 15599, 6410, 6, 1271, 780, 22121, 7, 2458, 5, 1786, 8, 253, 9, 5561, 6, 50, 634, 41, 27573, 227, 5561, 7, 8415, 5377, 4, 40450, 8, 5485, 747, 5731, 6777, 2643, 7, 3692, 251, 3780, 5774, 6, 6122, 15, 49, 2167, 304, 403, 8, 1421, 3471, 4, 21001, 2339, 6, 101, 557, 6665, 6, 6441, 7201, 6, 50, 2799, 6, 747, 5585, 10, 4764, 9, 335, 8, 8418, 4, 1216, 2339, 64, 1719, 2632, 7614, 6, 1455, 4271, 7576, 6, 8, 694, 4935, 1283, 7, 323, 49, 1449, 4, 635, 6, 5516, 215, 251, 14301, 64, 28, 4087, 6, 941, 77, 447, 19, 3092, 14, 33, 8135, 5933, 11948, 4, 282, 1632, 2777, 5774, 8558, 6, 101, 2788, 20257, 50, 2777, 2706, 6, 24, 16, 1537, 7, 304, 11857, 26492, 40878, 3092, 101, 163, 18854, 6, 272, 10311, 12, 246, 6, 50, 255, 245, 4, 1216, 3092, 3700, 33, 10, 4532, 8135, 5933, 9, 29600, 22121, 6, 61, 839, 51, 1395, 2024, 3679, 2339, 14, 11514, 42, 3000, 4, 3972, 609, 251, 2339, 19, 11857, 26492, 3092, 6, 10, 1537, 1548, 16, 7, 3462, 5, 2788, 88, 2735, 5561, 50, 26521, 6, 609, 349, 2835, 12672, 19, 5, 1421, 6, 8, 172, 9637, 5, 775, 4, 870, 26667, 5, 3780, 88, 2735, 1667, 6, 349, 2835, 64, 2564, 624, 5, 1421, 18, 8135, 5933, 16311, 4, 10462, 6, 42, 1548, 3441, 7316, 5516, 7, 1306, 14, 5, 9696, 109, 45, 12414, 5, 1029, 40584, 8, 5377, 9, 5, 3780, 4, 993, 4964, 6877, 634, 15599, 6410, 6, 1271, 780, 22121, 7, 2458, 5, 1786, 8, 253, 9, 5561, 6, 50, 634, 41, 27573, 227, 5561, 7, 8415, 5377, 4, 40450, 8, 5485, 747, 5731, 6777, 2643, 7, 3692, 251, 3780, 5774, 6, 6122, 15, 49, 2167, 304, 403, 8, 1421, 3471, 4, 2], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = tokenizer(long_text, truncation=False)\n",
    "text"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
