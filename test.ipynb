{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "from datasets import load_dataset, load_metric\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "!wandb login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vw970or3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e259c5475ee34e1fa7a9bdecc7283256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.013 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.227164…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▄▅▁▂▅▄▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.99713</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rare-wind-1</strong> at: <a href='https://wandb.ai/kaifan-li/my_project/runs/vw970or3' target=\"_blank\">https://wandb.ai/kaifan-li/my_project/runs/vw970or3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230804_225122-vw970or3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vw970or3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07bc6da2c3d4e70a35c6eab7030ed93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668189813693366, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/is/kaifan-l/private_room/proj-repos/prompt-for-long-text-summarization/wandb/run-20230804_225935-7shln82z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaifan-li/my_project/runs/7shln82z' target=\"_blank\">swept-dawn-2</a></strong> to <a href='https://wandb.ai/kaifan-li/my_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaifan-li/my_project' target=\"_blank\">https://wandb.ai/kaifan-li/my_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaifan-li/my_project/runs/7shln82z' target=\"_blank\">https://wandb.ai/kaifan-li/my_project/runs/7shln82z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ['WANDB_DIR'] = os.getcwd() + '/wandb/'\n",
    "os.environ['WANDB_CACHE_DIR'] = os.getcwd() + '/wandb/.cache/'\n",
    "os.environ['WANDB_CONFIG_DIR'] = os.getcwd() + '/wandb/.config/'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 初始化wandb\n",
    "wandb.init(\n",
    "    entity='kaifan-li',\n",
    "    project=\"my_project\"\n",
    ")\n",
    "\n",
    "# 构建模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i in range(100):\n",
    "        inputs = torch.randn(32, 10)  # 随机生成输入数据\n",
    "        labels = torch.randn(32, 1)   # 随机生成标签\n",
    "        \n",
    "        # 正向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # 记录训练过程和指标\n",
    "    avg_loss = running_loss / 100\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": avg_loss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers import BartConfig, T5Config\n",
    "\n",
    "class PromptBartConfig(BartConfig):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.pre_seq_len = config.pre_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一步是将处理后的数据集 `segmented_batch` 进行转置，使得每个子句在第一维度，即 `batch_size = 1`。原始的 `segmented_batch` 是一个列表的列表，其中每个列表代表一个样本，而每个样本可能被分成多个子句。而在这个步骤中，我们将所有样本的第 `seg_num` 个子句提取出来，形成一个新的列表，并将这些列表组成一个新的 `segmented_batch` 列表。\n",
    "\n",
    "举个例子，假设原始的 `segmented_batch` 如下：\n",
    "\n",
    "```python\n",
    "segmented_batch = [[sample1_seg1, sample1_seg2, sample1_seg3],\n",
    "                   [sample2_seg1, sample2_seg2],\n",
    "                   [sample3_seg1, sample3_seg2, sample3_seg3, sample3_seg4]]\n",
    "```\n",
    "\n",
    "其中 `sample1_seg1` 表示第一个样本的第一个子句，`sample1_seg2` 表示第一个样本的第二个子句，以此类推。\n",
    "\n",
    "经过转置后的 `segmented_batch` 如下：\n",
    "\n",
    "```python\n",
    "segmented_batch = [[sample1_seg1, sample2_seg1, sample3_seg1],\n",
    "                   [sample1_seg2, sample2_seg2, sample3_seg2],\n",
    "                   [sample1_seg3, None, sample3_seg3],\n",
    "                   [None, None, sample3_seg4]]\n",
    "```\n",
    "\n",
    "注意，这里可能会出现 `None`，因为不同样本可能具有不同数量的子句。这种转置操作通常用于将一个批次的数据转换为序列模型的输入，使得每个子句在第一维度，方便进行后续的处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq: tensor([101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 102, 111, 112, 113,\n",
      "        114, 115, 102, 116, 117, 102])\n",
      "drop_mask: tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1])\n",
      "seq: tensor([103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "        117])\n",
      "input_segments flag 1: [tensor([103, 104, 105, 106, 107]), tensor([108, 109, 110, 111, 112]), tensor([113, 114, 115, 116, 117])]\n",
      "input_segments flag 2: [None, None, None, None, None, None, None, tensor([103, 104, 105, 106, 107]), tensor([108, 109, 110, 111, 112]), tensor([113, 114, 115, 116, 117])]\n",
      "segmented_batch: [[None, None, None, None, None, None, None, tensor([103, 104, 105, 106, 107]), tensor([108, 109, 110, 111, 112]), tensor([113, 114, 115, 116, 117])]]\n",
      "seq: tensor([101, 102, 103, 104, 105, 106, 102, 107, 108, 109, 110, 111, 102, 112,\n",
      "        113, 114, 115, 102, 116, 117])\n",
      "drop_mask: tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0])\n",
      "seq: tensor([103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "        117])\n",
      "input_segments flag 1: [tensor([103, 104, 105, 106, 107]), tensor([108, 109, 110, 111, 112]), tensor([113, 114, 115, 116, 117])]\n",
      "input_segments flag 2: [None, None, None, None, None, None, None, tensor([103, 104, 105, 106, 107]), tensor([108, 109, 110, 111, 112]), tensor([113, 114, 115, 116, 117])]\n",
      "segmented_batch: [[None, None, None, None, None, None, None, tensor([103, 104, 105, 106, 107]), tensor([108, 109, 110, 111, 112]), tensor([113, 114, 115, 116, 117])], [None, None, None, None, None, None, None, tensor([103, 104, 105, 106, 107]), tensor([108, 109, 110, 111, 112]), tensor([113, 114, 115, 116, 117])]]\n",
      "=============\n",
      "segmented_batch: [[None, None], [None, None], [None, None], [None, None], [None, None], [None, None], [None, None], [tensor([103, 104, 105, 106, 107]), tensor([103, 104, 105, 106, 107])], [tensor([108, 109, 110, 111, 112]), tensor([108, 109, 110, 111, 112])], [tensor([113, 114, 115, 116, 117]), tensor([113, 114, 115, 116, 117])]]\n",
      "Original input_ids:\n",
      "tensor([[101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 102, 111, 112, 113,\n",
      "         114, 115, 102, 116, 117, 102],\n",
      "        [101, 102, 103, 104, 105, 106, 102, 107, 108, 109, 110, 111, 102, 112,\n",
      "         113, 114, 115, 102, 116, 117]])\n",
      "Processed input_ids:\n",
      "[[None, None], [None, None], [None, None], [None, None], [None, None], [None, None], [None, None], [tensor([103, 104, 105, 106, 107]), tensor([103, 104, 105, 106, 107])], [tensor([108, 109, 110, 111, 112]), tensor([108, 109, 110, 111, 112])], [tensor([113, 114, 115, 116, 117]), tensor([113, 114, 115, 116, 117])]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_ids = torch.tensor([\n",
    "    [101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 102, 111, 112, 113, 114, 115, 102, 116, 117, 102],\n",
    "    [101, 102, 103, 104, 105, 106, 102, 107, 108, 109, 110, 111, 102, 112, 113, 114, 115, 102, 116, 117]\n",
    "])\n",
    "\n",
    "special_token_ids = [101, 102]\n",
    "\n",
    "\n",
    "\n",
    "class Processor:\n",
    "    def __init__(self, segment_size, rmt_config, special_token_ids):\n",
    "        self.segment_size = segment_size\n",
    "        self.rmt_config = rmt_config\n",
    "        self.special_token_ids = special_token_ids\n",
    "\n",
    "    def pad_and_segment(self, input_ids):\n",
    "        segmented_batch = []\n",
    "        for seq in input_ids:\n",
    "            print('seq:', seq)\n",
    "            drop_mask = sum([seq == t for t in self.special_token_ids])\n",
    "            print('drop_mask:', drop_mask)\n",
    "            seq = seq[(1 - drop_mask).bool()]\n",
    "            print('seq:', seq)\n",
    "            seq = seq[:self.segment_size * self.rmt_config['max_n_segments']]\n",
    "            \n",
    "            align = self.rmt_config['segment_alignment']\n",
    "            if align in {'right', None}:\n",
    "                split_inds = (list(range(len(seq), 0, -self.rmt_config['segment_size'])) + [0])[::-1]\n",
    "            elif align == 'left':\n",
    "                split_inds = list(range(0, len(seq), self.rmt_config['segment_size'])) + [len(seq)]\n",
    "            elif align == 'center':\n",
    "                n_seg = math.ceil(len(seq) / self.rmt_config['segment_size'])\n",
    "                split_inds = list(range(0, len(seq), math.ceil(len(seq) / n_seg))) + [len(seq)]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            input_segments = [seq[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "            print('input_segments flag 1:', input_segments)\n",
    "            # TODO: do the implementation\n",
    "            # input_segments = [self.pad_add_special_tokens(t, self.config.input_size) for t in input_segments]\n",
    "            \n",
    "            # add empty segment markers if needed\n",
    "            n_empty_segments = self.rmt_config['max_n_segments'] - len(input_segments)\n",
    "            # input_segments:\n",
    "            input_segments = [None] * n_empty_segments + input_segments\n",
    "\n",
    "            # segmented_batch: \n",
    "            segmented_batch.append(input_segments)\n",
    "\n",
    "            \n",
    "        print('=============')\n",
    "        segmented_batch = [[sample[seg_num] for sample in segmented_batch] \\\n",
    "                    for seg_num in range(self.rmt_config['max_n_segments'])]\n",
    "        print('segmented_batch:', segmented_batch)\n",
    "        return segmented_batch\n",
    "\n",
    "# 假设我们有一个名为 rmt_config 的配置字典和 segment_size 变量\n",
    "rmt_config = {'max_n_segments': 10,\n",
    "              'segment_size': 5,\n",
    "              'segment_alignment': 'right',}\n",
    "segment_size = 5\n",
    "\n",
    "processor = Processor(segment_size, rmt_config, special_token_ids)\n",
    "output = processor.pad_and_segment(input_ids)\n",
    "\n",
    "print(\"Original input_ids:\")\n",
    "print(input_ids)\n",
    "print(\"Processed input_ids:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = \"Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13014"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(long_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/is/kaifan-l/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "\n",
    "import numpy as np\n",
    "import psutil\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "\n",
    "from peft import PrefixTuningConfig, TaskType, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator() # device_placement=\"cuda:0\"\n",
    "model_name_or_path = \"facebook/bart-large\"\n",
    "dataset_name = \"cnn_dailymail\"\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    num_virtual_tokens=20,\n",
    ")\n",
    "text_column = 'article'\n",
    "label_column = 'highlights'\n",
    "lr = 3e-3\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "do_test = True\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/home/is/kaifan-l/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1d9366c0e4492695f192c66a459242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn_dataset = load_dataset(dataset_name, \"3.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "target_max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples[text_column]\n",
    "    targets = examples[label_column]\n",
    "    model_inputs = tokenizer(inputs, truncation=True) # 这里暂时不padding\n",
    "    targets = tokenizer(\n",
    "        targets,\n",
    "        max_length=target_max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    targets = targets['input_ids']\n",
    "    targets[targets == tokenizer.pad_token_id] = -100\n",
    "    model_inputs['labels'] = targets\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/is/kaifan-l/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-032269279110a08c.arrow\n",
      "Loading cached processed dataset at /home/is/kaifan-l/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-39ca471479d521d4.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d05f5b90074d9a98b8c4beef10ee6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with accelerator.main_process_first():\n",
    "    cnn_dataset = cnn_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        remove_columns=cnn_dataset[\"train\"].column_names,\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/is/kaifan-l/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-99312ead8dbc8f26.arrow\n",
      "Loading cached shuffled indices for dataset at /home/is/kaifan-l/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0044f3d14da28a04.arrow\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(cnn_dataset[\"train\"]) * 0.01)\n",
    "eval_size = int(len(cnn_dataset[\"validation\"]) * 0.01)\n",
    "test_size = int(len(cnn_dataset[\"test\"]) * 0.01)\n",
    "\n",
    "# 从打乱后的数据集中随机抽取指定数量的数据\n",
    "train_dataset = cnn_dataset[\"train\"].shuffle(seed=42).select(range(train_size))\n",
    "eval_dataset = cnn_dataset[\"validation\"].shuffle(seed=42).select(range(eval_size))\n",
    "test_dataset = cnn_dataset[\"test\"].shuffle(seed=42).select(range(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    return tokenizer.pad(examples, padding='longest', return_tensors='pt')\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True, # 将数据加载到固定的内存中，可以加速数据加载\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  9518,  3579,  ...,  -100,  -100,  -100],\n",
      "        [    0, 29178,   260,  ...,  -100,  -100,  -100],\n",
      "        [    0, 10993,  4645,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [    0,  3762,     9,  ...,  -100,  -100,  -100],\n",
      "        [    0,  4993,    10,  ...,  -100,  -100,  -100],\n",
      "        [    0,   133,  1275,  ...,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    print(batch['labels'])\n",
    "    break\n",
    "\n",
    "# \"input_ids\" | \"attention_mask\" | \"labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.summarization import BartPrefixForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"pre_seq_len\": 20,\n",
    "    \"input_size\": 512,\n",
    "    \"segment_size\": 512,\n",
    "    \"max_n_segments\": 3,\n",
    "    \"bptt_depth\": 2,\n",
    "    \"prefix_projection\" : False,\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"hidden_size\": 1024\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_config(config, kwargs):\n",
    "    if config is not None:\n",
    "        config.pre_seq_len = kwargs['pre_seq_len']\n",
    "        config.segment_size = kwargs['segment_size']\n",
    "        config.input_size = kwargs['input_size']\n",
    "        config.max_n_segments = kwargs['max_n_segments']\n",
    "        config.bptt_depth = kwargs['bptt_depth']\n",
    "        config.prefix_projection = kwargs['prefix_projection']\n",
    "        config.hidden_dropout_prob = kwargs['hidden_dropout_prob']\n",
    "        config.hidden_size = kwargs['hidden_size']\n",
    "    else:\n",
    "        raise Exception('config is None!')\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 458,252,288\n",
      "Trainable parameters: 491,520 0.107260%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartPrefixForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-large-cnn and are newly initialized: ['bos_token', 'cls_token', 'sep_token', 'prefix_encoder.embedding.weight', 'eos_token']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(BartPrefixForConditionalGeneration(\n",
       "   (model): BartModel(\n",
       "     (shared): Embedding(50264, 1024, padding_idx=1)\n",
       "     (encoder): BartEncoder(\n",
       "       (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "       (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "       (layers): ModuleList(\n",
       "         (0-11): 12 x BartEncoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (activation_fn): GELUActivation()\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "       (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "     (decoder): BartDecoder(\n",
       "       (embed_tokens): Embedding(50264, 1024, padding_idx=1)\n",
       "       (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
       "       (layers): ModuleList(\n",
       "         (0-11): 12 x BartDecoderLayer(\n",
       "           (self_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (activation_fn): GELUActivation()\n",
       "           (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (encoder_attn): BartAttention(\n",
       "             (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "             (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "           )\n",
       "           (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "           (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "           (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "           (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "         )\n",
       "       )\n",
       "       (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "     )\n",
       "   )\n",
       "   (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
       "   (prefix_encoder): PrefixEncoder(\n",
       "     (embedding): Embedding(20, 24576)\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       " ),\n",
       " BartConfig {\n",
       "   \"_name_or_path\": \"facebook/bart-large-cnn\",\n",
       "   \"_num_labels\": 3,\n",
       "   \"activation_dropout\": 0.0,\n",
       "   \"activation_function\": \"gelu\",\n",
       "   \"add_final_layer_norm\": false,\n",
       "   \"architectures\": [\n",
       "     \"BartForConditionalGeneration\"\n",
       "   ],\n",
       "   \"attention_dropout\": 0.0,\n",
       "   \"bos_token_id\": 0,\n",
       "   \"bptt_depth\": 2,\n",
       "   \"classif_dropout\": 0.0,\n",
       "   \"classifier_dropout\": 0.0,\n",
       "   \"d_model\": 1024,\n",
       "   \"decoder_attention_heads\": 16,\n",
       "   \"decoder_ffn_dim\": 4096,\n",
       "   \"decoder_layerdrop\": 0.0,\n",
       "   \"decoder_layers\": 12,\n",
       "   \"decoder_start_token_id\": 2,\n",
       "   \"dropout\": 0.1,\n",
       "   \"early_stopping\": true,\n",
       "   \"encoder_attention_heads\": 16,\n",
       "   \"encoder_ffn_dim\": 4096,\n",
       "   \"encoder_layerdrop\": 0.0,\n",
       "   \"encoder_layers\": 12,\n",
       "   \"eos_token_id\": 2,\n",
       "   \"force_bos_token_to_be_generated\": true,\n",
       "   \"forced_bos_token_id\": 0,\n",
       "   \"forced_eos_token_id\": 2,\n",
       "   \"gradient_checkpointing\": false,\n",
       "   \"hidden_dropout_prob\": 0.1,\n",
       "   \"id2label\": {\n",
       "     \"0\": \"LABEL_0\",\n",
       "     \"1\": \"LABEL_1\",\n",
       "     \"2\": \"LABEL_2\"\n",
       "   },\n",
       "   \"init_std\": 0.02,\n",
       "   \"input_size\": 512,\n",
       "   \"is_encoder_decoder\": true,\n",
       "   \"label2id\": {\n",
       "     \"LABEL_0\": 0,\n",
       "     \"LABEL_1\": 1,\n",
       "     \"LABEL_2\": 2\n",
       "   },\n",
       "   \"length_penalty\": 2.0,\n",
       "   \"max_length\": 142,\n",
       "   \"max_n_segments\": 3,\n",
       "   \"max_position_embeddings\": 1024,\n",
       "   \"min_length\": 56,\n",
       "   \"model_type\": \"bart\",\n",
       "   \"no_repeat_ngram_size\": 3,\n",
       "   \"normalize_before\": false,\n",
       "   \"num_beams\": 4,\n",
       "   \"num_hidden_layers\": 12,\n",
       "   \"output_past\": true,\n",
       "   \"pad_token_id\": 1,\n",
       "   \"pre_seq_len\": 20,\n",
       "   \"prefix\": \" \",\n",
       "   \"prefix_projection\": false,\n",
       "   \"scale_embedding\": false,\n",
       "   \"segment_size\": 512,\n",
       "   \"task_specific_params\": {\n",
       "     \"summarization\": {\n",
       "       \"early_stopping\": true,\n",
       "       \"length_penalty\": 2.0,\n",
       "       \"max_length\": 142,\n",
       "       \"min_length\": 56,\n",
       "       \"no_repeat_ngram_size\": 3,\n",
       "       \"num_beams\": 4\n",
       "     }\n",
       "   },\n",
       "   \"transformers_version\": \"4.28.0\",\n",
       "   \"use_cache\": true,\n",
       "   \"vocab_size\": 50264\n",
       " })"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "bart_config = AutoConfig.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "bart_config = set_config(bart_config, kwargs)\n",
    "bart_generator = BartPrefixForConditionalGeneration.from_pretrained(\n",
    "    \"facebook/bart-large-cnn\",\n",
    "    config=bart_config,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "bart_generator, bart_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1024\n"
     ]
    }
   ],
   "source": [
    "print(bart_config.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "test = torch.arange(20).long()\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]), tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
      "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]]))\n",
      "<zip object at 0x2baa848e5dc0>\n",
      "0 10\n",
      "0 10\n",
      "1 10\n",
      "1 10\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "                [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],])\n",
    "attention_mask = torch.tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                    [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],])\n",
    "labels = torch.tensor([[0, 1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "            [10, 11, 12, 13, 14, 15, 16, 17, 18, 19],])\n",
    "seg = (input_ids, attention_mask, labels)\n",
    "print(seg)\n",
    "print(zip(*seg))\n",
    "for seg_num, s in enumerate(zip(*seg)):\n",
    "    print(seg_num, s[0].shape[0])\n",
    "    print(seg_num, s[0].shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for param in bart_generator.model.parameters():\n",
    "    if param.requires_grad:\n",
    "        print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 1\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "trainable_parameters = [param for param in bart_generator.parameters() if param.requires_grad]\n",
    "\n",
    "# 打印可训练参数的数量\n",
    "print(\"Total trainable parameters:\", len(trainable_parameters))\n",
    "\n",
    "# 打印每个可训练参数的名称\n",
    "for param in trainable_parameters:\n",
    "    print(param.data.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrefixEncoder(\n",
       "  (embedding): Embedding(20, 24576)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_generator.prefix_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaPreTrainedModel, AutoModel, AutoConfig\n",
    "class RobertaPrefixModelForQuestionAnswering(RobertaPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        # self.roberta = RobertaModel(config, add_pooling_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import logging\n",
    "import copy\n",
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "from transformers import (\n",
    "    BartForConditionalGeneration, \n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "from transformers import BartConfig, T5Config\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "from model.prefix_encoder import PrefixEncoder\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from transformers.modeling_bart.py\n",
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    if pad_token_id is None:\n",
    "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
    "    # replace possible -100 values in labels by `pad_token_id`\n",
    "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "\n",
    "    return shifted_input_ids\n",
    "# prefix-tuning/p-tuning v2 version\n",
    "class BartPrefixForConditionalGeneration(BartForConditionalGeneration):\n",
    "    def __init__(self, config: BartConfig, tokenizer):\n",
    "        super().__init__(config)\n",
    "        # copied from BartForConditionalGeneration.__init__()\n",
    "        # self.model = BartModel(config)\n",
    "        # self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
    "        # self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n",
    "        # self.post_init() will not overwrite the pretrained parameters when using from_pretrained()\n",
    "        \n",
    "        self.config = config\n",
    "        self.segment_alignment = config.segment_alignment\n",
    "        self.extract_special_tokens(tokenizer)\n",
    "        self.pre_seq_len = config.pre_seq_len\n",
    "        self.n_layer = config.num_hidden_layers\n",
    "        self.n_head = config.num_attention_heads\n",
    "        self.n_embd = config.hidden_size // config.num_attention_heads\n",
    "        # self.extend_word_embeddings(config.pre_seq_len, tokenizer)\n",
    "        \n",
    "        # tokenizer.num_special_tokens_to_add()cal the number of special tokens needed to add except [SEP]\n",
    "        self.segment_size = config.input_size - self.pre_seq_len - tokenizer.num_special_tokens_to_add()\n",
    "        if 'sep_token' in tokenizer.special_tokens_map:\n",
    "            self.segment_size -= 1\n",
    "        \n",
    "        # TODO: forget some part of long range memory and add new memory\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for param in self.lm_head.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.prefix_tokens = torch.arange(self.pre_seq_len).long()\n",
    "        self.prefix_encoder = PrefixEncoder(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "        bart_param = 0\n",
    "        lm_head_param = 0\n",
    "        all_param = 0\n",
    "        \n",
    "        # count the number of trainable parameters in bart\n",
    "        for name, param in self.model.named_parameters():\n",
    "            bart_param += param.numel() # numel() returns the total number of elements in the input tensor\n",
    "        for name, param in self.lm_head.named_parameters():\n",
    "            lm_head_param += param.numel()\n",
    "            \n",
    "        for name, param in self.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            \n",
    "        trainable_param = all_param - bart_param - lm_head_param\n",
    "        \n",
    "        print(\"Total parameters: {:,}\".format(all_param))\n",
    "        print(\"Trainable parameters: {:,} {:,%}\".format((trainable_param), trainable_param/all_param))\n",
    "\n",
    "    def get_prompt(self, batch_size):\n",
    "        prefix_tokens = self.prefix_tokens.unsqueeze(0).expand(batch_size, -1).to(self.model.device)\n",
    "        past_key_values = self.prefix_encoder(prefix_tokens)\n",
    "        bsz, seqlen, _ = past_key_values.shape\n",
    "        past_key_values = past_key_values.view(\n",
    "            bsz,\n",
    "            seqlen,\n",
    "            self.n_layer * 2,\n",
    "            self.n_head,\n",
    "            self.n_embd\n",
    "        )        \n",
    "        past_key_values = self.dropout(past_key_values)\n",
    "        # (2,batch_size,n_head,seq_len,head_dim)\n",
    "        past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(2)\n",
    "        return past_key_values\n",
    "    \n",
    "    # TODO：labels按照比例切分\n",
    "    # TODO: 25% -> 50% -> 75% -> 100% -> 100% -> 100% -> 100% -> 100% -> 100% -> 100%\n",
    "    def pad_and_segment(self, input_ids, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        segment input_ids into segments\n",
    "        \n",
    "        input sample:\n",
    "        segmented_batch = [\n",
    "            [sample1_seg1, sample1_seg2, sample1_seg3],\n",
    "            [sample2_seg1, sample2_seg2],\n",
    "            [sample3_seg1, sample3_seg2, sample3_seg3, sample3_seg4]\n",
    "        ]\n",
    "                   \n",
    "        output sample:\n",
    "        segmented_batch = [\n",
    "            [sample1_seg1, sample2_seg1, sample3_seg1],\n",
    "            [sample1_seg2, sample2_seg2, sample3_seg2],\n",
    "            [sample1_seg3, None, sample3_seg3],\n",
    "            [None, None, sample3_seg4]\n",
    "        ]\n",
    "        \"\"\"\n",
    "        segmented_batch = []\n",
    "        segmented_batch_attention_masks = []\n",
    "        segmented_batch_labels = []\n",
    "        \n",
    "        if attention_mask is None:\n",
    "            attention_mask = [None] * input_ids.shape[0]\n",
    "        batch_attention_mask = attention_mask\n",
    "            \n",
    "        # inference mode\n",
    "        if labels is None:\n",
    "            labels = [None] * input_ids.shape[0]\n",
    "        batch_labels = labels\n",
    "        \n",
    "        # input_ids: [batch_size, seq_len]\n",
    "        for seq, attn_mask, label in zip(input_ids, batch_attention_mask, batch_labels):\n",
    "\n",
    "            # pytorch syntax: element-wise operation\n",
    "            drop_mask = sum([seq == t for t in self.special_token_ids])\n",
    "            drop_mask = torch.tensor([1 if t != 0 else 0 for t in drop_mask])\n",
    "\n",
    "            # bool type slice for tensor type\n",
    "            # remove special tokens\n",
    "            seq = seq[(1 - drop_mask).bool()]\n",
    "            \n",
    "            if attn_mask is not None:\n",
    "                attn_mask_drop_mask = sum([attn_mask == self.pad_token_id])\n",
    "                attn_mask = attn_mask[attn_mask_drop_mask.bool()]\n",
    "                attn_mask = attn_mask[:self.segment_size * self.config.max_n_segments]\n",
    "            if label is not None:\n",
    "                label_drop_mask = sum([label == t for t in self.special_token_ids + [-100]])\n",
    "                label_drop_mask = torch.tensor([1 if t != 0 else 0 for t in label_drop_mask])\n",
    "                label = label[(1-label_drop_mask).bool()]\n",
    "                # TODO：label = label[:self.config.sum_max_size * self.config.max_n_segments]\n",
    "                label = label[:self.segment_size * self.config.max_n_segments]\n",
    "            \n",
    "            align = self.segment_alignment\n",
    "            if align in {'right', None}:\n",
    "                split_inds = (list(range(len(seq), 0, -self.segment_size)) + [0])[::-1]\n",
    "            elif align == 'left':\n",
    "                split_inds = list(range(0, len(seq), self.segment_size)) + [len(seq)]\n",
    "            elif align == 'center':\n",
    "                n_seg = math.ceil(len(seq) / self.segment_size)\n",
    "                split_inds = list(range(0, len(seq), math.ceil(len(seq) / n_seg))) + [len(seq)]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            input_segments = [seq[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "            input_segments = [self.pad_add_special_tokens(t, self.config.input_size) for t in input_segments]\n",
    "            \n",
    "            # add empty segment markers if needed\n",
    "            n_empty_segments = self.config.max_n_segments - len(input_segments)\n",
    "            # input_segments:\n",
    "            input_segments = input_segments + [None] * n_empty_segments\n",
    "\n",
    "            # segmented_batch: \n",
    "            segmented_batch.append(input_segments)\n",
    "\n",
    "            if attn_mask is not None:\n",
    "                attn_mask_segments = [attn_mask[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "                attn_mask_segments = [self.pad_add_special_tokens(t, self.config.input_size, add_to='attention_mask') for t in attn_mask_segments]\n",
    "                attn_mask_segments = attn_mask_segments + [None] * n_empty_segments\n",
    "                segmented_batch_attention_masks.append(attn_mask_segments)\n",
    "            \n",
    "            # TODO: labels need to be segmented by other rules\n",
    "            if label is not None:\n",
    "                labels_segments = [label[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "                labels_segments = [self.pad_add_special_tokens(t, self.config.input_size, add_to='labels') for t in labels_segments]\n",
    "                labels_segments = labels_segments + [None] * n_empty_segments\n",
    "                segmented_batch_labels.append(labels_segments)\n",
    "                \n",
    "        segmented_batch = [[sample[seg_num] for sample in segmented_batch] \n",
    "                            for seg_num in range(self.config.max_n_segments)]\n",
    "        segmented_batch_attention_masks = [[sample[seg_num] for sample in segmented_batch_attention_masks]\n",
    "                                           for seg_num in range(self.config.max_n_segments)]\n",
    "        segmented_batch_labels = [[sample[seg_num] for sample in segmented_batch_labels]\n",
    "                                  for seg_num in range(self.config.max_n_segments)]\n",
    "\n",
    "        return segmented_batch, segmented_batch_attention_masks, segmented_batch_labels\n",
    "        \n",
    "    def extract_special_tokens(self, tokenizer):\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.special_token_ids = [tokenizer.pad_token_id]\n",
    "        for token in ['cls_token', 'sep_token', 'eos_token', 'bos_token']:\n",
    "            token_id = getattr(tokenizer, f'{token}_id')\n",
    "            if token_id is not None:\n",
    "                self.register_buffer(token, torch.tensor([token_id]))\n",
    "                self.special_token_ids.append(token_id)\n",
    "            else:\n",
    "                setattr(self, token, None)\n",
    "                \n",
    "    # def extend_word_embeddings(self, tokenizer):\n",
    "    #     vocab_size = self.model.config.vocab_size\n",
    "    #     # NOTE: Really necessary???\n",
    "    #     extended_vocab_size = vocab_size + self.config.pre_seq_len\n",
    "    #     self.pre_seq_len = self.config.pre_seq_len\n",
    "\n",
    "    # Memory mechanism like RNN\n",
    "    def forget_and_memory(self,):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    #  prefix-tuning don't need to concat prefix and input sequence\n",
    "    def pad_add_special_tokens(self, tensor, segment_size, \n",
    "                               prompts=None, prompt_attention_mask=None, # maybe better to use pre_seq_len and generate prompts attention mask?\n",
    "                               add_to='input_ids'):\n",
    "        \"\"\"\n",
    "        bart tokenizer:\n",
    "        {'bos_token': '<s>', 0\n",
    "         'eos_token': '</s>', 2\n",
    "         'unk_token': '<unk>', 3\n",
    "         'sep_token': '</s>', 0\n",
    "         'pad_token': '<pad>', 1\n",
    "         'cls_token': '<s>', 0\n",
    "         'mask_token': '<mask>' 50264\n",
    "        }\n",
    "        \"\"\"\n",
    "        input_elements = []\n",
    "        # Add special tokens: <s> and </s> to the input sequence\n",
    "        # For prefix-prop\n",
    "        if prompts is not None:\n",
    "            if add_to == 'inputs':\n",
    "                input_elements += [self.cls_token, prompts, self.sep_token, tensor, self.sep_token]\n",
    "            # For Bart, only the pad token is 0 in attention_mask\n",
    "            elif add_to == 'attention_mask':\n",
    "                mask_value = torch.ones((1), device=tensor.device)\n",
    "                input_elements += [mask_value, prompt_attention_mask, mask_value, tensor, mask_value]\n",
    "            # As a encoder-decoder model：is not needed to add prompt to labels\n",
    "            elif add_to == 'labels':\n",
    "                input_elements += [self.eos_token, tensor, self.sep_token]\n",
    "        # For prefix-tuning/p-tuning v2\n",
    "        else:\n",
    "            if add_to == 'input_ids':\n",
    "                input_elements += [self.sep_token, tensor, self.sep_token]\n",
    "            elif add_to == 'attention_mask':\n",
    "                mask_value = torch.ones((1), device=tensor.device)\n",
    "                input_elements += [mask_value, tensor, mask_value]\n",
    "            elif add_to == 'labels':\n",
    "                input_elements += [self.eos_token, tensor, self.sep_token]\n",
    "        tensor = torch.cat(input_elements)\n",
    "        print(tensor.tolist())\n",
    "        # print(tensor[0])\n",
    "        # Add padding tokens\n",
    "        # TODO: implement summary module\n",
    "        #       now self.config.sum_size default = 0\n",
    "        pad_size = segment_size - tensor.shape[0] - self.config.sum_token_size\n",
    "        if pad_size > 0:\n",
    "            if add_to == 'input_ids':\n",
    "                tensor = F.pad(tensor, (0, pad_size), value=self.pad_token_id)\n",
    "            # TODO: 显然有错误\n",
    "            elif add_to == 'attention_mask':\n",
    "                tensor = F.pad(tensor, (0, pad_size), value=0)\n",
    "            elif add_to == 'labels':\n",
    "                # TODO: dynamic padding labels??\n",
    "                # pad_size = min(pad_size, self.config.label_max_size)\n",
    "                # for Seq2Seq labels need to be pad by -100\n",
    "                tensor = F.pad(tensor, (0, pad_size), value=-100)\n",
    "                pass\n",
    "        # print(\"padded tokens:\",tensor)\n",
    "        return tensor\n",
    "\n",
    "        # TODO: this implementation just add <s> and </s> to the input sequence\n",
    "        #       maybe need to add other special tokens\n",
    "    \n",
    "    def prepare_kwargs(self, segment, kwargs):\n",
    "        segment_input_ids, segment_attention_mask, segment_label = segment\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "        \n",
    "        # [sample1_seg1, sample2_seg1, sample3_seg1,....] up to batch_size\n",
    "        # Some of the segments are None like: [sample1_seg3, None, sample3_seg3]\n",
    "        non_empty_mask = [s is not None for s in segment_input_ids]\n",
    "        # all the segments are None, due to the max_n_segments >> the number of segments        \n",
    "        if sum(non_empty_mask) == 0:\n",
    "            return None, non_empty_mask\n",
    "        \n",
    "        # convert list to tensor\n",
    "        input_ids = torch.stack([s for s in segment_input_ids if s is not None])\n",
    "        # input_embeds = self.model.embeddings(input_ids)\n",
    "\n",
    "        \n",
    "        seg_kwargs['input_ids'] = input_ids\n",
    "        # seg_kwargs['inputs_embeds'] = input_embeds\n",
    "        \n",
    "        # if seg_kwargs.get('token_type_ids') is not None:\n",
    "        #     seg_kwargs['token_type_ids'] = self.get_token_type_ids(input_ids)\n",
    "        if seg_kwargs['decoder_input_ids'] is not None:\n",
    "            seg_kwargs['decoder_input_ids'] = torch.stack([el for el, m in zip(segment_label, non_empty_mask) if m])\n",
    "        print(\"seg_kwargs['decoder_input_ids']: \", seg_kwargs['decoder_input_ids'].shape)\n",
    "        # if seg_kwargs['labels_mask'] is not None:\n",
    "        # seg_kwargs['labels_mask'] = torch.stack([el for el, m in zip(segment_labels_mask, non_empty_mask) if m])\n",
    "        # if seg_kwargs.get('token_type_ids') is not None:\n",
    "        #     seg_kwargs['token_type_ids'] = self.get_token_type_ids(input_ids)\n",
    "        # seg_kwargs['output_hidden_states'] = True\n",
    "        \n",
    "        # generate prompts\n",
    "        batch_size = input_ids.shape[0]\n",
    "        past_key_values = self.get_prompt(batch_size)\n",
    "        prefix_attention_mask = torch.ones(batch_size, self.pre_seq_len)\n",
    "        attention_mask = torch.stack([s for s in segment_attention_mask if s is not None])\n",
    "        print(\"attention_mask: \", attention_mask.shape)\n",
    "        attn_mask = torch.cat([prefix_attention_mask, attention_mask], dim=1)\n",
    "        seg_kwargs['past_key_values'] = past_key_values\n",
    "        seg_kwargs['attention_mask'] = attention_mask\n",
    "        \n",
    "        return seg_kwargs, non_empty_mask\n",
    "        \n",
    "    def get_attention_mask(self, tensor):\n",
    "        mask = torch.ones_like(tensor)\n",
    "        mask[tensor == self.pad_token_id] = 0\n",
    "        return mask\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = True,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, Seq2SeqLMOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "        \"\"\" \n",
    "        kwargs = {\n",
    "            'attention_mask': attention_mask, \n",
    "            # 'token_type_ids': token_type_ids,\n",
    "            # 'position_ids': position_ids, \n",
    "            'inputs_embeds': inputs_embeds,\n",
    "            'decoder_input_ids': labels, \n",
    "            'output_attentions': output_attentions,\n",
    "            'output_hidden_states': output_hidden_states, 'return_dict': return_dict,\n",
    "        }\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        if labels is not None:\n",
    "            if use_cache:\n",
    "                logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n",
    "            use_cache = False\n",
    "            if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "                decoder_input_ids = shift_tokens_right(\n",
    "                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
    "                )\n",
    "            print(\"decoder_input_ids: \", decoder_input_ids)\n",
    "        # MODIFIED: add prefix encoder\n",
    "        # batch_size = input_ids.shape[0]\n",
    "        # past_key_values = self.get_prompt(batch_size)\n",
    "        # prefix_attention_mask = torch.ones(batch_size, self.pre_seq_len)\n",
    "        # attention_mask = torch.cat([prefix_attention_mask, attention_mask], dim=1)\n",
    "        \n",
    "        # segmented: [max_n_segments, batch_size, segment_size]\n",
    "        # !!! Note: the batch_size is not the same as the input batch_size\n",
    "        segmented = self.pad_and_segment(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        # NOTE: why???\n",
    "        # if self.pre_seq_len == 0:\n",
    "        #     segmented = segmented[-1:]\n",
    "        \n",
    "        model_outputs = []\n",
    "        for seg_num, segment in enumerate(zip(*segmented)):\n",
    "            in_ids, attn_mask, l = segment\n",
    "            print(l)\n",
    "            # TODO: can't control the number of gradient accumulation steps now\n",
    "            if self.config.bptt_depth != -1:\n",
    "                raise NotImplementedError\n",
    "            \n",
    "            seg_kwargs, non_empty_mask = self.prepare_kwargs(segment, kwargs)\n",
    "            print(\"!!!!\")\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            print(\"input_ids \", seg_kwargs['input_ids'].shape)\n",
    "            print(\"attention_mask \", seg_kwargs['attention_mask'].shape)\n",
    "            # print(\"past_key_values \", seg_kwargs['past_key_values'][0].shape)\n",
    "            # out = self.model(**seg_kwargs)\n",
    "            print(\"decoder_input_ids \", seg_kwargs['decoder_input_ids'].shape)\n",
    "            out = self.model(\n",
    "                input_ids=seg_kwargs['input_ids'],\n",
    "                attention_mask=seg_kwargs['attention_mask'],\n",
    "                # TODO:\n",
    "                past_key_values=seg_kwargs['past_key_values'],\n",
    "                decoder_input_ids=decoder_input_ids,\n",
    "            )\n",
    "            # self.prefix_tokens = out.encoder_hidden_state[-1][:, 1:self.pre_seq_len+1]\n",
    "            # self.prefix_tokens = out.last_hidden_state[:, :self.pre_seq_len]\n",
    "            print(\"query matrix: \", out.last_hidden_state.shape)\n",
    "            print(\"prefix_tokens: \", self.prefix_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomBartConfig(BartConfig):\n",
    "    def __init__(self,\n",
    "                 pre_seq_len=20,\n",
    "                 input_size=512,\n",
    "                 max_n_segments=3,\n",
    "                 bptt_depth=-1,\n",
    "                 prefix_projection=False, \n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 segment_alignment='left',\n",
    "                 sum_token_size=0,\n",
    "                 label_max_size=142,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pre_seq_len = pre_seq_len\n",
    "        self.input_size = input_size\n",
    "        self.max_n_segments = max_n_segments\n",
    "        self.bptt_depth = bptt_depth\n",
    "        self.prefix_projection = prefix_projection # whether to use reparametrization trick\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob # dropout for prefix encoder\n",
    "        self.segment_alignment = segment_alignment\n",
    "        self.sum_token_size = sum_token_size\n",
    "        self.label_max_size = label_max_size # the max size of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_config = BartConfig.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomBartConfig {\n",
       "  \"_name_or_path\": \"bart-base\",\n",
       "  \"activation_dropout\": 0.1,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartModel\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"bptt_depth\": -1,\n",
       "  \"classif_dropout\": 0.1,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 12,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"dropout\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 12,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"forced_bos_token_id\": 0,\n",
       "  \"forced_eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"input_size\": 512,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"label_max_size\": 142,\n",
       "  \"max_n_segments\": 3,\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_beams\": 4,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"pre_seq_len\": 20,\n",
       "  \"prefix_projection\": false,\n",
       "  \"scale_embedding\": false,\n",
       "  \"segment_alignment\": \"left\",\n",
       "  \"sum_token_size\": 0,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"length_penalty\": 1.0,\n",
       "      \"max_length\": 128,\n",
       "      \"min_length\": 12,\n",
       "      \"num_beams\": 4\n",
       "    },\n",
       "    \"summarization_cnn\": {\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 142,\n",
       "      \"min_length\": 56,\n",
       "      \"num_beams\": 4\n",
       "    },\n",
       "    \"summarization_xsum\": {\n",
       "      \"length_penalty\": 1.0,\n",
       "      \"max_length\": 62,\n",
       "      \"min_length\": 11,\n",
       "      \"num_beams\": 6\n",
       "    }\n",
       "  },\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.28.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_config = CustomBartConfig(**bart_config.to_dict())\n",
    "custom_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import logging\n",
    "import copy\n",
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "from transformers import (\n",
    "    BartForConditionalGeneration, \n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "from transformers import BartConfig, T5Config\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "from model.prefix_encoder import PrefixEncoder\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 178,208,256\n",
      "Trainable parameters: 184,320 0.103430%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartPrefixForConditionalGeneration were not initialized from the model checkpoint at facebook/bart-base and are newly initialized: ['sep_token', 'cls_token', 'eos_token', 'bos_token', 'prefix_encoder.embedding.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartPrefixForConditionalGeneration(\n",
       "  (model): BartModel(\n",
       "    (shared): Embedding(50265, 768, padding_idx=1)\n",
       "    (encoder): BartEncoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartEncoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): BartDecoder(\n",
       "      (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "      (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-5): 6 x BartDecoderLayer(\n",
       "          (self_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): BartAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       "  (prefix_encoder): PrefixEncoder(\n",
       "    (embedding): Embedding(20, 9216)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BartPrefixForConditionalGeneration.from_pretrained('facebook/bart-base', \n",
    "                                                           config=custom_config,\n",
    "                                                           tokenizer=tokenizer,)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([\n",
    "    [101, 2054, 2062, 2024, 2003, 2022, 103, 102, 102, 0],\n",
    "    [101, 2110, 2033, 102, 0, 0, 0, 0, 0, 0]\n",
    "])\n",
    "attention_mask = torch.tensor([\n",
    "    [1, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
    "    [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "])\n",
    "labels = torch.tensor([\n",
    "    [0, 1, 2, 3, 4, 5, 6, -100, -100, -100],\n",
    "    [0, 1, 2, -100, -100, -100, -100, -100, -100, -100]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 2, 2, 0]"
      ]
     },
     "execution_count": 439,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.special_token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 101, 2054, 2062, 2024, 2003, 2022, 103, 102, 102, 2]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[2, 3, 4, 5, 6, 2]\n",
      "[2, 101, 2110, 2033, 102, 2]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[2, 2]\n"
     ]
    }
   ],
   "source": [
    "seg, att, label = model.pad_and_segment(input_ids=input_ids,\n",
    "                      attention_mask=attention_mask,\n",
    "                      labels=labels,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_token_ids = torch.tensor([0, 1, 2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 101, 2054, 2062, 2024, 2003, 2022,  103,  102,  102])"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq = torch.tensor([101, 2054, 2062, 2024, 2003, 2022, 103, 102, 102, 0])\n",
    "# pytorch syntax: element-wise operation\n",
    "drop_mask = sum([seq == t for t in special_token_ids])\n",
    "# Convert non-zero elements to 1\n",
    "drop_mask = torch.tensor([1 if t != 0 else 0 for t in drop_mask])\n",
    "print(drop_mask)\n",
    "# bool type slice for tensor type\n",
    "# remove special tokens\n",
    "seq = seq[(1 - drop_mask).bool()]\n",
    "seq "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input_ids:  tensor([[2, 0, 1, 2, 3, 4, 5, 6, 1, 1],\n",
      "        [2, 0, 1, 2, 1, 1, 1, 1, 1, 1]])\n",
      "[2, 101, 2054, 2062, 2024, 2003, 2022, 103, 102, 102, 2]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[2, 3, 4, 5, 6, 2]\n",
      "[2, 101, 2110, 2033, 102, 2]\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "[2, 2]\n",
      "[tensor([   2,    3,    4,    5,    6,    2, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100]), tensor([   2,    2, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "        -100, -100, -100, -100, -100, -100, -100, -100])]\n",
      "seg_kwargs['decoder_input_ids']:  torch.Size([2, 512])\n",
      "attention_mask:  torch.Size([2, 512])\n",
      "!!!!\n",
      "input_ids  torch.Size([2, 512])\n",
      "attention_mask  torch.Size([2, 512])\n",
      "decoder_input_ids  torch.Size([2, 512])\n",
      "query matrix:  torch.Size([2, 10, 768])\n",
      "prefix_tokens:  torch.Size([20])\n",
      "[None, None]\n",
      "!!!!\n",
      "[None, None]\n",
      "!!!!\n"
     ]
    }
   ],
   "source": [
    "model(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    labels=labels,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.bos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
