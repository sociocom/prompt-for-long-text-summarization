{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ['WANDB_DIR'] = os.getcwd() + '/wandb/'\n",
    "os.environ['WANDB_CACHE_DIR'] = os.getcwd() + '/wandb/.cache/'\n",
    "os.environ['WANDB_CONFIG_DIR'] = os.getcwd() + '/wandb/.config/'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 初始化wandb\n",
    "wandb.init(\n",
    "    entity='kaifan-li',\n",
    "    project=\"my_project\"\n",
    ")\n",
    "\n",
    "# 构建模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i in range(100):\n",
    "        inputs = torch.randn(32, 10)  # 随机生成输入数据\n",
    "        labels = torch.randn(32, 1)   # 随机生成标签\n",
    "        \n",
    "        # 正向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # 记录训练过程和指标\n",
    "    avg_loss = running_loss / 100\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": avg_loss})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = \"Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(long_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "\n",
    "import numpy as np\n",
    "import psutil\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "\n",
    "from peft import PrefixTuningConfig, TaskType, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator() # device_placement=\"cuda:0\"\n",
    "model_name_or_path = \"facebook/bart-base\"\n",
    "dataset_name = \"cnn_dailymail\"\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    num_virtual_tokens=20,\n",
    ")\n",
    "text_column = 'article'\n",
    "label_column = 'highlights'\n",
    "lr = 3e-3\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "do_test = True\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_dataset = load_dataset(dataset_name, \"3.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "target_max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples[text_column]\n",
    "    targets = examples[label_column]\n",
    "    model_inputs = tokenizer(inputs, truncation=True) # 这里暂时不padding\n",
    "    targets = tokenizer(\n",
    "        targets,\n",
    "        max_length=target_max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    targets = targets['input_ids']\n",
    "    targets[targets == tokenizer.pad_token_id] = -100\n",
    "    model_inputs['labels'] = targets\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with accelerator.main_process_first():\n",
    "    cnn_dataset = cnn_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        remove_columns=cnn_dataset[\"train\"].column_names,\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(cnn_dataset[\"train\"]) * 0.1)\n",
    "eval_size = int(len(cnn_dataset[\"validation\"]) * 0.01)\n",
    "test_size = int(len(cnn_dataset[\"test\"]) * 0.01)\n",
    "\n",
    "# 从打乱后的数据集中随机抽取指定数量的数据\n",
    "train_dataset = cnn_dataset[\"train\"].shuffle(seed=42).select(range(train_size))\n",
    "eval_dataset = cnn_dataset[\"validation\"].shuffle(seed=42).select(range(eval_size))\n",
    "test_dataset = cnn_dataset[\"test\"].shuffle(seed=42).select(range(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    return tokenizer.pad(examples, padding='longest', return_tensors='pt')\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True, # 将数据加载到固定的内存中，可以加速数据加载\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num, batch in enumerate(train_dataloader):\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import logging\n",
    "import copy\n",
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "from transformers import (\n",
    "    BartForConditionalGeneration, \n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "from transformers import BartConfig\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "from model.prefix_encoder import PrefixEncoder\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "class CustomBartConfig(BartConfig):\n",
    "    def __init__(self,\n",
    "                 pre_seq_len=20,\n",
    "                 input_size=512,\n",
    "                 max_n_segments=2,\n",
    "                 bptt_depth=-1,\n",
    "                 prefix_projection=False, \n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 segment_alignment='left',\n",
    "                 sum_token_size=0,\n",
    "                 label_max_size=142,\n",
    "                 sum_loss=True,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pre_seq_len = pre_seq_len\n",
    "        self.input_size = input_size\n",
    "        self.max_n_segments = max_n_segments\n",
    "        self.bptt_depth = bptt_depth\n",
    "        self.prefix_projection = prefix_projection # whether to use reparametrization trick\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob # dropout for prefix encoder\n",
    "        self.segment_alignment = segment_alignment\n",
    "        self.sum_token_size = sum_token_size\n",
    "        self.label_max_size = label_max_size # the max size of labels\n",
    "        self.sum_loss = sum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_config = BartConfig.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_config = CustomBartConfig(**bart_config.to_dict())\n",
    "custom_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import logging\n",
    "import copy\n",
    "import math\n",
    "from typing import List, Optional, Tuple, Union, Iterable\n",
    "\n",
    "from transformers import (\n",
    "    BartPretrainedModel,\n",
    "    BartForConditionalGeneration, \n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "from transformers import BartConfig, T5Config\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "from model.prefix_encoder import PrefixEncoder\n",
    "from peft import PrefixTuningConfig, TaskType, get_peft_model\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from transformers.modeling_bart.py\n",
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    if pad_token_id is None:\n",
    "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
    "    # replace possible -100 values in labels by `pad_token_id`\n",
    "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "\n",
    "    return shifted_input_ids\n",
    "# prefix-tuning/p-tuning v2 version\n",
    "class BartPrefixForConditionalGeneration(BartPretrainedModel):\n",
    "    def __init__(self, config, checkpoint, peft_config):\n",
    "        super().__init__(config)\n",
    "        # copied from BartForConditionalGeneration.__init__()\n",
    "        # self.model = BartModel(config)\n",
    "        # self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
    "        # self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n",
    "        # self.post_init() will not overwrite the pretrained parameters when using from_pretrained()\n",
    "        \n",
    "        bart_model = BartForConditionalGeneration.from_pretrained(checkpoint)\n",
    "        self.model = get_peft_model(bart_model, peft_config)\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(checkpoint)\n",
    "        self.config = config\n",
    "\n",
    "        self.segment_alignment = config.segment_alignment\n",
    "        self.extract_special_tokens(tokenizer)\n",
    "        self.pre_seq_len = config.pre_seq_len\n",
    "        self.n_layer = config.num_hidden_layers\n",
    "        self.n_head = config.num_attention_heads\n",
    "        self.n_embd = config.hidden_size // config.num_attention_heads\n",
    "        # self.extend_word_embeddings(config.pre_seq_len, tokenizer)\n",
    "        \n",
    "        # tokenizer.num_special_tokens_to_add()cal the number of special tokens needed to add except [SEP]\n",
    "        # bart-base: 489\n",
    "        self.segment_size = config.input_size - self.pre_seq_len - tokenizer.num_special_tokens_to_add()\n",
    "        if 'sep_token' in tokenizer.special_tokens_map:\n",
    "            self.segment_size -= 1\n",
    "        \n",
    "        # TODO: forget some part of long range memory and add new memory\n",
    "\n",
    "        # for param in self.model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        self.prefix_tokens = torch.arange(self.pre_seq_len).long()\n",
    "        self.prefix_encoder = PrefixEncoder(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "        bart_param = 0\n",
    "        all_param = 0\n",
    "        \n",
    "        # count the number of trainable parameters in bart\n",
    "        for name, param in self.model.named_parameters():\n",
    "            bart_param += param.numel() # numel() returns the total number of elements in the input tensor\n",
    "            \n",
    "        for name, param in self.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            \n",
    "        trainable_param = all_param - bart_param\n",
    "        \n",
    "        print(\"Total parameters: {:,}\".format(all_param))\n",
    "        print(\"Trainable parameters: {:,} {:,%}\".format((trainable_param), trainable_param/all_param))\n",
    "\n",
    "    def get_prompt(self, batch_size):\n",
    "        prefix_tokens = self.prefix_tokens.unsqueeze(0).expand(batch_size, -1).to(self.model.device)\n",
    "        past_key_values = self.prefix_encoder(prefix_tokens)\n",
    "        bsz, seqlen, _ = past_key_values.shape\n",
    "        past_key_values = past_key_values.view(\n",
    "            bsz,\n",
    "            seqlen,\n",
    "            self.n_layer * 2,\n",
    "            self.n_head,\n",
    "            self.n_embd\n",
    "        )        \n",
    "        past_key_values = self.dropout(past_key_values)\n",
    "        # (2,batch_size,n_head,seq_len,head_dim)\n",
    "        past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(2)\n",
    "        return past_key_values\n",
    "    \n",
    "    # TODO：split labels other warys\n",
    "    # TODO: 25% -> 50% -> 75% -> 100% -> 100% -> 100% -> 100% -> 100% -> 100% -> 100%\n",
    "    def pad_and_segment(self, input_ids, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        segment input_ids into segments\n",
    "        \n",
    "        input sample:\n",
    "        segmented_batch = [\n",
    "            [sample1_seg1, sample1_seg2, sample1_seg3],\n",
    "            [sample2_seg1, sample2_seg2],\n",
    "            [sample3_seg1, sample3_seg2, sample3_seg3, sample3_seg4]\n",
    "        ]\n",
    "                   \n",
    "        output sample:\n",
    "        segmented_batch = [\n",
    "            [sample1_seg1, sample2_seg1, sample3_seg1],\n",
    "            [sample1_seg2, sample2_seg2, sample3_seg2],\n",
    "            [sample1_seg3, None, sample3_seg3],\n",
    "            [None, None, sample3_seg4]\n",
    "        ]\n",
    "        \"\"\"\n",
    "        segmented_batch = []\n",
    "        segmented_batch_attention_masks = []\n",
    "        segmented_batch_labels = []\n",
    "        \n",
    "        if attention_mask is None:\n",
    "            attention_mask = [None] * input_ids.shape[0]\n",
    "        batch_attention_mask = attention_mask\n",
    "            \n",
    "        # inference mode\n",
    "        if labels is None:\n",
    "            labels = [None] * input_ids.shape[0]\n",
    "        batch_labels = labels\n",
    "        \n",
    "        # input_ids: [batch_size, seq_len]\n",
    "        for seq, attn_mask, label in zip(input_ids, batch_attention_mask, batch_labels):\n",
    "\n",
    "            # pytorch syntax: element-wise operation\n",
    "            drop_mask = sum([seq == t for t in self.special_token_ids])\n",
    "            drop_mask = torch.tensor([1 if t != 0 else 0 for t in drop_mask])\n",
    "\n",
    "            # bool type slice for tensor type\n",
    "            # remove special tokens\n",
    "            seq = seq[(1 - drop_mask).bool()]\n",
    "            seq = seq[:self.segment_size * self.config.max_n_segments]\n",
    "            \n",
    "            if attn_mask is not None:\n",
    "                attn_mask_drop_mask = sum([attn_mask == self.pad_token_id])\n",
    "                attn_mask = attn_mask[attn_mask_drop_mask.bool()]\n",
    "                attn_mask = attn_mask[:self.segment_size * self.config.max_n_segments]\n",
    "            if label is not None:\n",
    "                label_drop_mask = sum([label == t for t in self.special_token_ids + [-100]])\n",
    "                label_drop_mask = torch.tensor([1 if t != 0 else 0 for t in label_drop_mask])\n",
    "                label = label[(1-label_drop_mask).bool()]\n",
    "                # TODO：label = label[:self.config.sum_max_size * self.config.max_n_segments]\n",
    "                label = label[:self.segment_size * self.config.max_n_segments]\n",
    "            \n",
    "            align = self.segment_alignment\n",
    "            if align in {'right', None}:\n",
    "                split_inds = (list(range(len(seq), 0, -self.segment_size)) + [0])[::-1]\n",
    "            elif align == 'left':\n",
    "                split_inds = list(range(0, len(seq), self.segment_size)) + [len(seq)]\n",
    "            elif align == 'center':\n",
    "                n_seg = math.ceil(len(seq) / self.segment_size)\n",
    "                split_inds = list(range(0, len(seq), math.ceil(len(seq) / n_seg))) + [len(seq)]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            input_segments = [seq[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "            input_segments = [self.pad_add_special_tokens(t, self.config.input_size) for t in input_segments]\n",
    "            \n",
    "            # add empty segment markers if needed\n",
    "            n_empty_segments = self.config.max_n_segments - len(input_segments)\n",
    "            # input_segments:\n",
    "            # print(\"input_segments:\", len(input_segments))\n",
    "            input_segments = input_segments + [self.get_full_padding_segment()] * n_empty_segments\n",
    "            \n",
    "            # segmented_batch: \n",
    "            segmented_batch.append(input_segments)\n",
    "\n",
    "            if attn_mask is not None:\n",
    "                attn_mask_segments = [attn_mask[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "                attn_mask_segments = [self.pad_add_special_tokens(t, self.config.input_size, add_to='attention_mask') for t in attn_mask_segments]\n",
    "                attn_mask_segments = attn_mask_segments + [self.get_full_padding_segment()] * n_empty_segments\n",
    "                segmented_batch_attention_masks.append(attn_mask_segments)\n",
    "            \n",
    "            # TODO: labels need to be segmented by other rules\n",
    "            if label is not None:\n",
    "                print(\"label:\", len(label))\n",
    "                end_index = math.ceil(len(label) // (full_segment_size := len(input_segments)))\n",
    "                # # for i in range(full_segment_size):\n",
    "                # #     end = (i+1) * end_index\n",
    "                # #     labels_segments.append(label[:end])\n",
    "                labels_segments = [label[:(end_index*(i+1))] for i in range(full_segment_size)]\n",
    "                for i in range(len(labels_segments)):\n",
    "                    print(\"labels_segments:\", i, len(labels_segments[i]))\n",
    "            # TODO: labels need to be segmented by other rules\n",
    "\n",
    "                # full_segment_size = len(input_segments)\n",
    "                # labels_segments = [label[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "                labels_segments = [self.pad_add_special_tokens(t, self.config.input_size, add_to='labels') for t in labels_segments]\n",
    "                labels_segments = labels_segments + [self.get_full_padding_segment()] * n_empty_segments\n",
    "                segmented_batch_labels.append(labels_segments)\n",
    "                for i in range(len(labels_segments)):\n",
    "                    print(\"labels_segments:\", i, len(labels_segments[i]))\n",
    "                \n",
    "        segmented_batch = [[sample[seg_num] for sample in segmented_batch] \n",
    "                            for seg_num in range(self.config.max_n_segments)]\n",
    "        segmented_batch_attention_masks = [[sample[seg_num] for sample in segmented_batch_attention_masks]\n",
    "                                           for seg_num in range(self.config.max_n_segments)]\n",
    "        segmented_batch_labels = [[sample[seg_num] for sample in segmented_batch_labels]\n",
    "                                  for seg_num in range(self.config.max_n_segments)]\n",
    "\n",
    "        return segmented_batch, segmented_batch_attention_masks, segmented_batch_labels\n",
    "        \n",
    "    def extract_special_tokens(self, tokenizer):\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.special_token_ids = [tokenizer.pad_token_id]\n",
    "        for token in ['cls_token', 'sep_token', 'eos_token', 'bos_token']:\n",
    "            token_id = getattr(tokenizer, f'{token}_id')\n",
    "            if token_id is not None:\n",
    "                self.register_buffer(token, torch.tensor([token_id]))\n",
    "                self.special_token_ids.append(token_id)\n",
    "            else:\n",
    "                setattr(self, token, None)\n",
    "                \n",
    "    # def extend_word_embeddings(self, tokenizer):\n",
    "    #     vocab_size = self.model.config.vocab_size\n",
    "    #     # NOTE: Really necessary???\n",
    "    #     extended_vocab_size = vocab_size + self.config.pre_seq_len\n",
    "    #     self.pre_seq_len = self.config.pre_seq_len\n",
    "    \n",
    "    def get_full_padding_segment(self,):\n",
    "        padding_segment = torch.tensor([self.pad_token_id for _ in range(self.config.input_size)])\n",
    "        return padding_segment\n",
    "    \n",
    "    # Memory mechanism like RNN\n",
    "    def forget_and_memory(self,):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    #  prefix-tuning don't need to concat prefix and input sequence\n",
    "    def pad_add_special_tokens(self, tensor, segment_size, \n",
    "                               prompts=None, prompt_attention_mask=None, # maybe better to use pre_seq_len and generate prompts attention mask?\n",
    "                               add_to='input_ids'):\n",
    "        \"\"\"\n",
    "        bart tokenizer:\n",
    "        {'bos_token': '<s>', 0\n",
    "         'eos_token': '</s>', 2\n",
    "         'unk_token': '<unk>', 3\n",
    "         'sep_token': '</s>', 0\n",
    "         'pad_token': '<pad>', 1\n",
    "         'cls_token': '<s>', 0\n",
    "         'mask_token': '<mask>' 50264\n",
    "        }\n",
    "        \"\"\"\n",
    "        input_elements = []\n",
    "        # Add special tokens: <s> and </s> to the input sequence\n",
    "        # For prefix-prop\n",
    "        if prompts is not None:\n",
    "            if add_to == 'inputs':\n",
    "                input_elements += [self.cls_token, prompts, self.sep_token, tensor, self.sep_token]\n",
    "            # For Bart, only the pad token is 0 in attention_mask\n",
    "            elif add_to == 'attention_mask':\n",
    "                mask_value = torch.ones((1), device=tensor.device)\n",
    "                input_elements += [mask_value, prompt_attention_mask, mask_value, tensor, mask_value]\n",
    "            # As a encoder-decoder model：is not needed to add prompt to labels\n",
    "            elif add_to == 'labels':\n",
    "                input_elements += [self.eos_token, tensor, self.sep_token]\n",
    "        # For prefix-tuning/p-tuning v2\n",
    "        else:\n",
    "            if add_to == 'input_ids':\n",
    "                input_elements += [self.sep_token, tensor, self.sep_token]\n",
    "            elif add_to == 'attention_mask':\n",
    "                mask_value = torch.ones((1), device=tensor.device)\n",
    "                input_elements += [mask_value, tensor, mask_value]\n",
    "            elif add_to == 'labels':\n",
    "                input_elements += [self.eos_token, tensor, self.sep_token]\n",
    "        tensor = torch.cat(input_elements)\n",
    "\n",
    "        # Add padding tokens\n",
    "        # TODO: implement summary module\n",
    "        #       now self.config.sum_size default = 0\n",
    "        pad_size = segment_size - tensor.shape[0] - self.config.sum_token_size\n",
    "        if pad_size > 0:\n",
    "            if add_to == 'input_ids':\n",
    "                tensor = F.pad(tensor, (0, pad_size), value=self.pad_token_id)\n",
    "            elif add_to == 'attention_mask':\n",
    "                tensor = F.pad(tensor, (0, pad_size), value=0)\n",
    "            elif add_to == 'labels':\n",
    "                # for Seq2Seq labels need to be pad by -100\n",
    "                tensor = F.pad(tensor, (0, pad_size), value=-100)\n",
    "        return tensor\n",
    "\n",
    "        # TODO: this implementation just add <s> and </s> to the input sequence\n",
    "        #       maybe need to add other special tokens\n",
    "    \n",
    "    def prepare_kwargs(self, segment, kwargs):\n",
    "        segment_input_ids, segment_attention_mask, segment_label = segment\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "        \n",
    "        # [sample1_seg1, sample2_seg1, sample3_seg1,....] up to batch_size\n",
    "        # Some of the segments are None like: [sample1_seg3, None, sample3_seg3]\n",
    "        non_empty_mask = [s is not None for s in segment_input_ids]\n",
    "        print(\"non_empty_mask:\", non_empty_mask)\n",
    "        # all the segments are None, due to the max_n_segments >> the number of segments        \n",
    "        if sum(non_empty_mask) == 0:\n",
    "            return None, non_empty_mask\n",
    "        \n",
    "        # convert list to tensor\n",
    "        # print(\"segment_input_ids:\", segment_input_ids)\n",
    "\n",
    "        input_ids = torch.stack([s for s in segment_input_ids if s is not None])\n",
    "        # print(\"input_ids:\", input_ids.shape)\n",
    "        # input_embeds = self.model.embeddings(input_ids)\n",
    "\n",
    "        seg_kwargs['input_ids'] = input_ids\n",
    "    \n",
    "        # seg_kwargs['inputs_embeds'] = input_embeds\n",
    "        \n",
    "        # if seg_kwargs.get('token_type_ids') is not None:\n",
    "        #     seg_kwargs['token_type_ids'] = self.get_token_type_ids(input_ids)\n",
    "\n",
    "        # seg_kwargs['decoder_input_ids'] = torch.stack([el for el, m in zip(segment_label, non_empty_mask) if m])\n",
    "        # seg_kwargs['decoder_input_ids'] = seg_kwargs['decoder_input_ids'][non_empty_mask]\n",
    "        # print(\"decoder_input_ids:\", seg_kwargs['decoder_input_ids'].shape)\n",
    "        # if seg_kwargs['labels_mask'] is not None:\n",
    "        # seg_kwargs['labels_mask'] = torch.stack([el for el, m in zip(segment_labels_mask, non_empty_mask) if m])\n",
    "        # if seg_kwargs.get('token_type_ids') is not None:\n",
    "        #     seg_kwargs['token_type_ids'] = self.get_token_type_ids(input_ids)\n",
    "        # seg_kwargs['output_hidden_states'] = True\n",
    "        \n",
    "        # generate prompts\n",
    "        batch_size = seg_kwargs['input_ids'].shape[0]\n",
    "        # print('batch_size:', batch_size)\n",
    "        past_key_values = self.get_prompt(batch_size)\n",
    "        prefix_attention_mask = torch.ones(batch_size, self.pre_seq_len)\n",
    "        \n",
    "        if seg_kwargs['labels'] is not None:\n",
    "            seg_kwargs['labels'] = torch.stack([el for el, m in zip(segment_label, non_empty_mask) if m])\n",
    "        \n",
    "        # print(\"labels:\", seg_kwargs['labels'].shape, seg_kwargs['labels'])\n",
    "        # attn_mask = torch.cat([prefix_attention_mask, attention_mask], dim=1)\n",
    "        # seg_kwargs['past_key_values'] = past_key_values\n",
    "        if seg_kwargs['attention_mask'] is not None:\n",
    "            seg_kwargs['attention_mask'] = self.get_attention_mask(input_ids)\n",
    "        # seg_kwargs['attention_mask'] = attn_mask\n",
    "        return seg_kwargs, non_empty_mask\n",
    "        \n",
    "    def get_attention_mask(self, tensor):\n",
    "        mask = torch.ones_like(tensor)\n",
    "        mask[tensor == self.pad_token_id] = 0\n",
    "        return mask\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = True,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, Seq2SeqLMOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "        \"\"\" \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        kwargs = {\n",
    "            'attention_mask': attention_mask, \n",
    "            # 'token_type_ids': token_type_ids,\n",
    "            # 'position_ids': position_ids, \n",
    "            'inputs_embeds': inputs_embeds,\n",
    "            'labels': labels,\n",
    "            'output_attentions': output_attentions,\n",
    "            'output_hidden_states': output_hidden_states, \n",
    "            'return_dict': return_dict,\n",
    "        }\n",
    "        # segmented: [max_n_segments, batch_size, segment_size]\n",
    "        # !!! Note: the batch_size is not the same as the input batch_size\n",
    "        segmented = self.pad_and_segment(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        # NOTE: why???\n",
    "        # if self.pre_seq_len == 0:\n",
    "        #     segmented = segmented[-1:]\n",
    "        \n",
    "        model_outputs = []\n",
    "        for seg_num, segment in enumerate(zip(*segmented)):\n",
    "            print(\"seg_num:\", seg_num)\n",
    "            \n",
    "            in_ids, attn_mask, l = segment\n",
    "            print(\"batch_size:\", len(in_ids))\n",
    "            # TODO: can't control the number of gradient accumulation steps now\n",
    "            if self.config.bptt_depth != -1:\n",
    "                raise NotImplementedError\n",
    "            \n",
    "            seg_kwargs, non_empty_mask = self.prepare_kwargs(segment, kwargs)\n",
    "            # print(\"in_ids|attn_mask|l:\", seg_kwargs['input_ids'].shape, seg_kwargs['attention_mask'].shape, seg_kwargs['decoder_input_ids'].shape)\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "\n",
    "            out = self.model(**seg_kwargs)\n",
    "            print('decoder_input_ids:', decoder_input_ids)\n",
    "            # out = self.model(\n",
    "            #     input_ids=seg_kwargs['input_ids'],\n",
    "            #     attention_mask=seg_kwargs['attention_mask'],\n",
    "            #     # TODO: 只能concat attention到decoder attention mask\n",
    "            #     # past_key_values=seg_kwargs['past_key_values'],\n",
    "            #     # TODO: decoder_attention_mask: https://github.com/huggingface/transformers/issues/25271\n",
    "            #     decoder_input_ids=decoder_input_ids\n",
    "            # )\n",
    "            # self.prefix_tokens = out.encoder_last_hidden_state[-1][:, 1:self.pre_seq_len+1]\n",
    "            # self.prefix_tokens = out.last_hidden_state[:, :self.pre_seq_len]\n",
    "            out['seg_kwargs'] = seg_kwargs\n",
    "            model_outputs.append(out)\n",
    "            print('past_key_values:', out.past_key_values)\n",
    "            print('out:', out)\n",
    "        out = self.process_outputs(model_outputs, output_attentions, output_hidden_states)\n",
    "        print('model is finished')\n",
    "        return out\n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        min_length: Optional[int] = None,\n",
    "        do_sample: Optional[bool] = None,\n",
    "        early_stopping: Optional[bool] = None,\n",
    "        num_beams: Optional[int] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "        top_k: Optional[int] = None,\n",
    "        top_p: Optional[float] = None,\n",
    "        repetition_penalty: Optional[float] = None,\n",
    "        bad_words_ids: Optional[Iterable[int]] = None,\n",
    "        bos_token_id: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "        length_penalty: Optional[float] = None,\n",
    "        no_repeat_ngram_size: Optional[int] = None,\n",
    "        num_return_sequences: Optional[int] = None,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        decoder_start_token_id: Optional[int] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        **model_specific_kwargs\n",
    "    ) -> torch.LongTensor:\n",
    "\n",
    "\n",
    "        kwargs = {\n",
    "            'input_ids': input_ids,\n",
    "            'num_beams': num_beams,\n",
    "            'min_length': min_length,\n",
    "            'max_length': max_length,\n",
    "            'labels': None,\n",
    "            'attention_mask': None\n",
    "        }\n",
    "        \n",
    "        segmented = self.pad_and_segment(\n",
    "            input_ids=input_ids,\n",
    "        )\n",
    "        \n",
    "        model_outputs = []\n",
    "        for seg_num, segment in enumerate(zip(*segmented)):\n",
    "            in_ids, attn_mask, l = segment\n",
    "            \n",
    "            if self.config.bptt_depth != -1:\n",
    "                raise NotImplementedError\n",
    "            \n",
    "            seg_kwargs, non_empty_mask = self.prepare_kwargs(segment, kwargs)\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            \n",
    "            out = self.model.generate(**seg_kwargs)\n",
    "            \n",
    "            model_outputs.append(out)\n",
    "            print('out:', out)\n",
    "        print(\"model_outputs: \", self.tokenizer.decode(model_outputs[-1][-1], skip_special_tokens=True))\n",
    "        \n",
    "    def process_outputs(self, input_ids, model_outputs, output_attentions, output_hidden_states):\n",
    "        out = model_outputs[-1] # get the last segment output\n",
    "        \n",
    "        bs, seq_len = input_ids.shape\n",
    "        \n",
    "        losses = []\n",
    "        logits = []\n",
    "        labels_segm = []\n",
    "        \n",
    "        for out in model_outputs:\n",
    "            losses.append(out['loss'])\n",
    "            logits.append(out['logits'].detach())\n",
    "            labels_segm += [out['seg_kwargs']['labels']]\n",
    "        \n",
    "        if not output_hidden_states:\n",
    "            for key in out.keys():\n",
    "                if 'hidden_state' in key:\n",
    "                    out[key] = None\n",
    "                    \n",
    "        for i, l in enumerate(losses):\n",
    "            out[f'loss_{i}'] = l.mean()\n",
    "            \n",
    "        out['loss'] = torch.stack(losses).mean()\n",
    "        \n",
    "        for i in range(len(logits)):\n",
    "            logits[i] = F.pad(logits[i], (0, 0, 0, 0, 0, bs - logits[i].shape[0]))\n",
    "            labels_segm[i] = F.pad(labels_segm[i], (0, 0, 0, bs - labels_segm[i].shape[0]), value=-100)\n",
    "        \n",
    "        out['logits'] = torch.cat(logits, dim=1)\n",
    "        # Warning: rmt logits, labels, masks are not in the same order as in input data:\n",
    "        # the first dimension is number of segments!\n",
    "        # so, torch.cat will result in segm0, segm0,.. and only after all segm0 will come segm1, ... .\n",
    "        # not segm0, segm1, segm0, segm1 as in input data\n",
    "        out['logits_segm'] = [logits]\n",
    "        out['labels_segm'] = [labels_segm]\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg = math.ceil(10 / 4)\n",
    "seg \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model.summarization import BartPrefixForConditionalGeneration\n",
    "checkpoint = 'facebook/bart-base'\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    num_virtual_tokens=20,\n",
    ")\n",
    "model = BartPrefixForConditionalGeneration(    \n",
    "    checkpoint=checkpoint,\n",
    "    config=custom_config,\n",
    "    peft_config=peft_config\n",
    ")\n",
    "model.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, batch in enumerate(train_dataloader):\n",
    "    input_ids = batch[\"input_ids\"].to(model.device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "    labels = batch[\"labels\"].to(model.device)\n",
    "    \n",
    "    print(\"input_ids.shape:\", input_ids.shape)\n",
    "    print(\"attention_mask.shape:\", attention_mask.shape)\n",
    "    print(\"labels.shape:\", labels.shape)\n",
    "\n",
    "    out = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels,\n",
    "    )\n",
    "    print(out.keys())\n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        # attention_mask=attention_mask,\n",
    "        min_length=0,\n",
    "        max_length=142,\n",
    "        num_beams=4\n",
    "    )\n",
    "    # print(\"out:\", tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_tokens = torch.arange(20).long()\n",
    "prefix_tokens\n",
    "prefix_tokens = prefix_tokens.unsqueeze(0)\n",
    "prefix_tokens\n",
    "prefix_tokens = prefix_tokens.expand(32, -1)\n",
    "prefix_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
    "segment_size = 20\n",
    "def get_full_padding_segment():\n",
    "    padding_segment = [tokenizer.pad_token_id for _ in range(segment_size)]\n",
    "    return padding_segment\n",
    "\n",
    "test = get_full_padding_segment()\n",
    "test\n",
    "[test]\n",
    "[[12312414]] + [test] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceil = math.ceil(1024/3)\n",
    "ceil, 1024//3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartPrefixForConditionalGeneration(\n",
       "  (model): PeftModelForSeq2SeqLM(\n",
       "    (base_model): BartForConditionalGeneration(\n",
       "      (model): BartModel(\n",
       "        (shared): Embedding(50265, 768, padding_idx=1)\n",
       "        (encoder): BartEncoder(\n",
       "          (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "          (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "          (layers): ModuleList(\n",
       "            (0-5): 6 x BartEncoderLayer(\n",
       "              (self_attn): BartAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): GELUActivation()\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): BartDecoder(\n",
       "          (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "          (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "          (layers): ModuleList(\n",
       "            (0-5): 6 x BartDecoderLayer(\n",
       "              (self_attn): BartAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (activation_fn): GELUActivation()\n",
       "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): BartAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       "    )\n",
       "    (prompt_encoder): ModuleDict(\n",
       "      (default): PrefixEncoder(\n",
       "        (embedding): Embedding(20, 9216)\n",
       "      )\n",
       "    )\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from config import PromptBartConfig\n",
    "from transformers import AutoConfig\n",
    "from transformers import AutoModel\n",
    "from model import BartPrefixForConditionalGeneration\n",
    "from peft import PrefixTuningConfig\n",
    "\n",
    "bart_config = AutoConfig.from_pretrained('facebook/bart-base')\n",
    "custom_config = PromptBartConfig(**bart_config.to_dict())\n",
    "peft_config = PrefixTuningConfig.from_pretrained(\"kaifanli/prefix_rmt_bart_cnn-dm\")\n",
    "model = BartPrefixForConditionalGeneration(\n",
    "    checkpoint='facebook/bart-base',\n",
    "    config=custom_config,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.register_for_auto_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BartModel were not initialized from the model checkpoint at kaifanli/prefix_rmt_bart_cnn-dm and are newly initialized: ['model.decoder.layers.5.encoder_attn.out_proj.weight', 'model.decoder.layers.0.encoder_attn_layer_norm.bias', 'model.decoder.layers.4.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.fc1.weight', 'model.decoder.layers.0.encoder_attn.out_proj.weight', 'model.decoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.1.fc1.bias', 'model.decoder.layers.0.encoder_attn.v_proj.bias', 'model.encoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.3.fc1.bias', 'model.decoder.layers.5.fc2.bias', 'model.decoder.layers.3.self_attn.out_proj.bias', 'model.decoder.layers.2.encoder_attn.out_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.layers.1.fc1.weight', 'model.encoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.bias', 'model.decoder.layers.3.fc2.weight', 'model.decoder.layers.4.final_layer_norm.bias', 'model.decoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.1.encoder_attn.k_proj.weight', 'model.decoder.layers.1.encoder_attn.out_proj.weight', 'model.decoder.layers.1.fc1.bias', 'model.decoder.layers.3.encoder_attn.out_proj.weight', 'model.decoder.layers.1.encoder_attn.v_proj.weight', 'model.encoder.layers.2.self_attn.k_proj.weight', 'model.decoder.layers.3.self_attn.out_proj.weight', 'model.decoder.layers.2.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.weight', 'model.decoder.layers.3.encoder_attn.k_proj.bias', 'model.encoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.3.encoder_attn.k_proj.weight', 'model.encoder.layers.1.fc1.weight', 'model.encoder.layers.3.fc2.weight', 'model.decoder.layers.1.self_attn.out_proj.weight', 'model.decoder.layers.1.fc2.bias', 'model.shared.weight', 'model.encoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.1.fc2.weight', 'model.encoder.layers.3.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.4.self_attn.q_proj.bias', 'model.encoder.layers.4.final_layer_norm.weight', 'model.decoder.layers.0.self_attn.k_proj.bias', 'model.decoder.layers.0.fc2.bias', 'model.decoder.layers.5.encoder_attn.out_proj.bias', 'model.decoder.layers.5.fc1.bias', 'model.decoder.layers.3.encoder_attn.q_proj.weight', 'model.encoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.5.encoder_attn_layer_norm.weight', 'model.encoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.3.self_attn.v_proj.weight', 'model.decoder.layers.5.encoder_attn.v_proj.weight', 'model.decoder.layers.0.final_layer_norm.weight', 'model.encoder.layers.0.self_attn.k_proj.weight', 'model.encoder.layers.4.fc1.bias', 'model.decoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.3.encoder_attn.out_proj.bias', 'model.decoder.layers.5.self_attn_layer_norm.bias', 'model.decoder.layers.5.self_attn.v_proj.weight', 'model.encoder.layers.4.self_attn.out_proj.weight', 'model.decoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layernorm_embedding.weight', 'model.encoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.3.self_attn.k_proj.weight', 'model.decoder.layers.5.encoder_attn.v_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.k_proj.bias', 'model.encoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.2.fc1.bias', 'model.decoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.weight', 'model.decoder.layers.4.encoder_attn.k_proj.weight', 'model.encoder.layers.0.fc1.bias', 'model.encoder.layers.1.final_layer_norm.weight', 'model.encoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.3.fc2.bias', 'model.encoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.bias', 'model.encoder.layers.3.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.0.final_layer_norm.bias', 'model.encoder.layers.2.final_layer_norm.weight', 'model.encoder.layers.3.fc1.bias', 'model.decoder.layers.1.fc2.weight', 'model.encoder.layers.3.fc1.weight', 'model.decoder.layers.0.self_attn.v_proj.weight', 'model.decoder.layers.5.encoder_attn_layer_norm.bias', 'model.encoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.0.fc1.weight', 'model.decoder.layers.0.fc1.bias', 'model.encoder.layers.5.self_attn.out_proj.bias', 'model.encoder.layers.5.fc1.weight', 'model.decoder.layers.2.encoder_attn.k_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.bias', 'model.decoder.layers.2.self_attn.k_proj.weight', 'model.encoder.layers.3.self_attn.out_proj.bias', 'model.encoder.layers.5.self_attn.k_proj.weight', 'model.decoder.layers.0.encoder_attn_layer_norm.weight', 'model.decoder.layers.2.self_attn_layer_norm.weight', 'model.decoder.embed_positions.weight', 'model.decoder.layers.2.encoder_attn_layer_norm.weight', 'model.encoder.embed_positions.weight', 'model.encoder.layers.5.fc2.weight', 'model.encoder.layers.5.final_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.out_proj.bias', 'model.decoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.5.self_attn.k_proj.bias', 'model.encoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.bias', 'model.decoder.layers.5.encoder_attn.k_proj.weight', 'model.decoder.layers.2.encoder_attn.out_proj.weight', 'model.decoder.layers.1.self_attn.q_proj.bias', 'model.encoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.0.encoder_attn.k_proj.weight', 'model.encoder.layers.0.self_attn_layer_norm.bias', 'model.decoder.layers.1.encoder_attn.v_proj.bias', 'model.decoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.3.self_attn.k_proj.bias', 'model.decoder.layers.4.encoder_attn_layer_norm.bias', 'model.decoder.layers.4.final_layer_norm.weight', 'model.encoder.layers.5.self_attn.k_proj.bias', 'model.decoder.layers.2.encoder_attn.k_proj.bias', 'model.decoder.layers.3.encoder_attn.v_proj.bias', 'model.encoder.layers.4.final_layer_norm.bias', 'model.encoder.layernorm_embedding.bias', 'model.decoder.layers.3.encoder_attn_layer_norm.weight', 'model.encoder.layers.4.self_attn.out_proj.bias', 'model.encoder.layers.2.final_layer_norm.bias', 'model.decoder.layers.2.final_layer_norm.bias', 'model.encoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.0.fc1.weight', 'model.encoder.layers.4.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.0.encoder_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn.q_proj.bias', 'model.decoder.layers.5.fc1.weight', 'model.decoder.layers.2.self_attn.q_proj.bias', 'model.decoder.layers.2.encoder_attn.q_proj.weight', 'model.decoder.layers.3.encoder_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.out_proj.bias', 'model.encoder.layers.3.self_attn.out_proj.weight', 'model.encoder.layers.1.fc2.bias', 'model.encoder.layers.4.fc1.weight', 'model.encoder.layers.1.self_attn.out_proj.bias', 'model.decoder.layers.0.self_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.weight', 'model.encoder.layers.0.fc2.weight', 'model.encoder.layers.2.fc2.bias', 'model.decoder.layernorm_embedding.bias', 'model.encoder.layers.0.final_layer_norm.bias', 'model.decoder.layers.2.encoder_attn_layer_norm.bias', 'model.encoder.layers.1.self_attn.q_proj.bias', 'model.decoder.layers.5.encoder_attn.q_proj.bias', 'model.decoder.layers.3.fc1.weight', 'model.decoder.layers.1.self_attn.out_proj.bias', 'model.decoder.embed_tokens.weight', 'model.decoder.layernorm_embedding.weight', 'model.decoder.layers.3.final_layer_norm.weight', 'model.decoder.layers.5.final_layer_norm.weight', 'model.decoder.layers.1.self_attn_layer_norm.weight', 'model.encoder.layers.0.self_attn.q_proj.bias', 'model.encoder.layers.2.self_attn.k_proj.bias', 'model.encoder.layers.3.self_attn_layer_norm.bias', 'model.encoder.layers.4.fc2.bias', 'model.decoder.layers.2.encoder_attn.v_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.out_proj.weight', 'model.encoder.layers.2.self_attn.v_proj.weight', 'model.decoder.layers.2.encoder_attn.v_proj.weight', 'model.encoder.layers.5.fc2.bias', 'model.decoder.layers.5.final_layer_norm.bias', 'model.encoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.2.fc2.bias', 'model.decoder.layers.1.self_attn_layer_norm.bias', 'model.decoder.layers.4.encoder_attn.v_proj.weight', 'model.decoder.layers.4.self_attn.v_proj.weight', 'model.decoder.layers.4.self_attn.q_proj.bias', 'model.decoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.3.self_attn.q_proj.bias', 'model.decoder.layers.4.self_attn.k_proj.weight', 'model.decoder.layers.1.self_attn.k_proj.bias', 'model.decoder.layers.5.self_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn.k_proj.bias', 'model.decoder.layers.1.encoder_attn.q_proj.bias', 'model.decoder.layers.3.final_layer_norm.bias', 'model.decoder.layers.0.encoder_attn.k_proj.bias', 'model.decoder.layers.0.encoder_attn.q_proj.weight', 'model.decoder.layers.2.self_attn.q_proj.weight', 'model.encoder.layers.1.final_layer_norm.bias', 'model.decoder.layers.2.fc2.weight', 'model.decoder.layers.3.self_attn.v_proj.bias', 'model.encoder.layers.2.fc2.weight', 'model.decoder.layers.0.self_attn.k_proj.weight', 'model.decoder.layers.2.fc1.bias', 'model.decoder.layers.1.self_attn.k_proj.weight', 'model.decoder.layers.4.encoder_attn.v_proj.bias', 'model.decoder.layers.0.self_attn.q_proj.bias', 'model.decoder.layers.5.encoder_attn.k_proj.bias', 'model.decoder.layers.4.encoder_attn.q_proj.weight', 'model.decoder.layers.4.self_attn.out_proj.bias', 'model.decoder.layers.5.encoder_attn.q_proj.weight', 'model.encoder.layers.2.fc1.weight', 'model.decoder.layers.5.fc2.weight', 'model.decoder.layers.4.encoder_attn.out_proj.weight', 'model.encoder.layers.3.self_attn.v_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.weight', 'model.encoder.layers.5.self_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.out_proj.weight', 'model.decoder.layers.0.self_attn.out_proj.weight', 'model.decoder.layers.1.encoder_attn.k_proj.bias', 'model.encoder.embed_tokens.weight', 'model.decoder.layers.1.final_layer_norm.weight', 'model.decoder.layers.4.fc1.bias', 'model.decoder.layers.5.self_attn.k_proj.weight', 'model.encoder.layers.5.self_attn.q_proj.bias', 'model.encoder.layers.4.self_attn.q_proj.weight', 'model.encoder.layers.0.final_layer_norm.weight', 'model.decoder.layers.0.fc2.weight', 'model.decoder.layers.2.encoder_attn.q_proj.bias', 'model.decoder.layers.4.encoder_attn.out_proj.bias', 'model.encoder.layers.2.self_attn.v_proj.bias', 'model.decoder.layers.1.encoder_attn_layer_norm.weight', 'model.encoder.layers.5.self_attn.v_proj.weight', 'model.decoder.layers.4.fc1.weight', 'model.decoder.layers.3.encoder_attn_layer_norm.bias', 'model.encoder.layers.2.self_attn.q_proj.weight', 'model.decoder.layers.2.self_attn_layer_norm.bias', 'model.encoder.layers.0.self_attn.q_proj.weight', 'model.decoder.layers.5.self_attn.out_proj.bias', 'model.decoder.layers.0.encoder_attn.v_proj.weight', 'model.decoder.layers.1.encoder_attn.q_proj.weight', 'model.encoder.layers.4.self_attn.v_proj.bias', 'model.encoder.layers.1.self_attn.v_proj.weight', 'model.encoder.layers.4.fc2.weight', 'model.decoder.layers.5.self_attn.v_proj.bias', 'model.decoder.layers.4.fc2.bias', 'model.encoder.layers.3.fc2.bias', 'model.decoder.layers.1.self_attn.q_proj.weight', 'model.encoder.layers.2.self_attn.out_proj.weight', 'model.decoder.layers.2.final_layer_norm.weight', 'model.decoder.layers.3.self_attn_layer_norm.weight', 'model.encoder.layers.1.self_attn.k_proj.weight', 'model.encoder.layers.0.self_attn.v_proj.bias', 'model.encoder.layers.2.self_attn.q_proj.bias', 'model.encoder.layers.5.fc1.bias', 'model.decoder.layers.3.encoder_attn.v_proj.weight', 'model.decoder.layers.4.self_attn_layer_norm.bias', 'model.decoder.layers.3.self_attn.q_proj.weight', 'model.decoder.layers.0.encoder_attn.out_proj.bias', 'model.decoder.layers.2.self_attn.out_proj.weight', 'model.encoder.layers.1.self_attn_layer_norm.weight', 'model.decoder.layers.3.self_attn_layer_norm.bias', 'model.decoder.layers.5.self_attn_layer_norm.weight', 'model.decoder.layers.4.fc2.weight', 'model.decoder.layers.1.self_attn.v_proj.bias', 'model.encoder.layers.5.self_attn_layer_norm.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BartModel(\n",
       "  (shared): Embedding(50265, 768, padding_idx=1)\n",
       "  (encoder): BartEncoder(\n",
       "    (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x BartEncoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (activation_fn): GELUActivation()\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (decoder): BartDecoder(\n",
       "    (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "    (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-5): 6 x BartDecoderLayer(\n",
       "        (self_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (activation_fn): GELUActivation()\n",
       "        (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder_attn): BartAttention(\n",
       "          (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained('kaifanli/prefix_rmt_bart_cnn-dm', trust_remote_code=True)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import BartPrefixForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BartPrefixForConditionalGeneration.__init__() missing 2 required positional arguments: 'config' and 'peft_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_model \u001b[39m=\u001b[39m BartPrefixForConditionalGeneration\u001b[39m.\u001b[39;49mfrom_pretrained(\u001b[39m'\u001b[39;49m\u001b[39mkaifanli/prefix_rmt_bart_cnn-dm\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m test_model\n",
      "File \u001b[0;32m~/miniconda3/envs/summarization/lib/python3.11/site-packages/transformers/modeling_utils.py:2876\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2873\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2875\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2876\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   2878\u001b[0m \u001b[39m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[1;32m   2879\u001b[0m \u001b[39mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "\u001b[0;31mTypeError\u001b[0m: BartPrefixForConditionalGeneration.__init__() missing 2 required positional arguments: 'config' and 'peft_config'"
     ]
    }
   ],
   "source": [
    "test_model = BartPrefixForConditionalGeneration.from_pretrained('kaifanli/prefix_rmt_bart_cnn-dm')\n",
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_config = AutoConfig.from_pretrained('facebook/bart-base')\n",
    "custom_config = PromptBartConfig(**bart_config.to_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/transformers/models/bart/modeling_bart.py:549: FutureWarning: The class `PretrainedBartModel` has been depreciated, please use `BartPreTrainedModel` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import logging\n",
    "import copy\n",
    "import math\n",
    "from typing import List, Optional, Tuple, Union, Iterable\n",
    "\n",
    "from transformers import (\n",
    "    BartPretrainedModel,\n",
    "    BartForConditionalGeneration,\n",
    "    BartTokenizer,\n",
    ")\n",
    "from transformers import BartConfig\n",
    "from transformers import PreTrainedModel\n",
    "from peft import PrefixTuningConfig, TaskType, get_peft_model\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "from model.prefix_encoder import PrefixEncoder\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# copied from transformers.modeling_bart.py\n",
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    if pad_token_id is None:\n",
    "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
    "    # replace possible -100 values in labels by `pad_token_id`\n",
    "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "\n",
    "    return shifted_input_ids\n",
    "\n",
    "# ============================================\n",
    "# =============== BART model =================\n",
    "# ============================================\n",
    "\n",
    "# prefix-tuning/p-tuning v2 version\n",
    "class BartPrefixForConditionalGeneration(PreTrainedModel):\n",
    "    # mandatory config_class attribute\n",
    "    config_class = PromptBartConfig\n",
    "    def __init__(self, config, **kwargs):\n",
    "        super().__init__(config)\n",
    "        self.model = BartForConditionalGeneration(config)\n",
    "        # self.model = get_peft_model(bart_model, **kwargs['peft_config'])\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(kwargs['tokenizer_path'])\n",
    "        \n",
    "        self.config = config\n",
    "        self.segment_alignment = config.segment_alignment\n",
    "        self.extract_special_tokens(self.tokenizer)\n",
    "        self.pre_seq_len = config.pre_seq_len\n",
    "        self.n_layer = config.num_hidden_layers\n",
    "        self.n_head = config.num_attention_heads\n",
    "        self.n_embd = config.hidden_size // config.num_attention_heads\n",
    "        # self.extend_word_embeddings(config.pre_seq_len, tokenizer)\n",
    "        \n",
    "        # tokenizer.num_special_tokens_to_add()cal the number of special tokens needed to add except [SEP]\n",
    "        self.segment_size = config.input_size - self.pre_seq_len - self.tokenizer.num_special_tokens_to_add()\n",
    "        if 'sep_token' in self.tokenizer.special_tokens_map:\n",
    "            self.segment_size -= 1\n",
    "        \n",
    "        # TODO: forget some part of long range memory and add new memory\n",
    "        self.test = nn.Linear(self.n_embd, self.n_embd)\n",
    "        \n",
    "    def pad_and_segment(self, input_ids, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        segment input_ids into segments\n",
    "        \n",
    "        input sample:\n",
    "        segmented_batch = [\n",
    "            [sample1_seg1, sample1_seg2, sample1_seg3],\n",
    "            [sample2_seg1, sample2_seg2],\n",
    "            [sample3_seg1, sample3_seg2, sample3_seg3, sample3_seg4]\n",
    "        ]\n",
    "                   \n",
    "        output sample:\n",
    "        segmented_batch = [\n",
    "            [sample1_seg1, sample2_seg1, sample3_seg1],\n",
    "            [sample1_seg2, sample2_seg2, sample3_seg2],\n",
    "            [sample1_seg3, full_padding, sample3_seg3],\n",
    "            [full_padding, full_padding, sample3_seg4]\n",
    "        ]\n",
    "        \"\"\"\n",
    "        segmented_batch = []\n",
    "        segmented_batch_attention_masks = []\n",
    "        segmented_batch_labels = []\n",
    "        \n",
    "        if attention_mask is None:\n",
    "            attention_mask = [None] * input_ids.shape[0]\n",
    "        batch_attention_mask = attention_mask\n",
    "            \n",
    "        # inference mode\n",
    "        if labels is None:\n",
    "            labels = [None] * input_ids.shape[0]\n",
    "        batch_labels = labels\n",
    "        \n",
    "        # input_ids: [batch_size, seq_len]\n",
    "        for seq, attn_mask, label in zip(input_ids, batch_attention_mask, batch_labels):\n",
    "            \n",
    "            # pytorch syntax: element-wise operation\n",
    "            drop_mask = sum([seq == t for t in self.special_token_ids])\n",
    "            # Convert non-zero elements to 1\n",
    "            drop_mask = torch.tensor([1 if t != 0 else 0 for t in drop_mask])\n",
    "            \n",
    "            # bool type slice for tensor type\n",
    "            # remove special tokens\n",
    "            seq = seq[(1 - drop_mask).bool()]\n",
    "            \n",
    "            # truncate the sequence to the maximum length\n",
    "            seq = seq[:self.segment_size * self.config.max_n_segments]\n",
    "            \n",
    "            if attn_mask is not None:\n",
    "                attn_mask_drop_mask = sum([attn_mask == self.pad_token_id])\n",
    "                attn_mask = attn_mask[attn_mask_drop_mask.bool()]\n",
    "                attn_mask = attn_mask[:self.segment_size * self.config.max_n_segments]\n",
    "            if label is not None:\n",
    "                label_drop_mask = sum([label == t for t in self.special_token_ids + [-100]])\n",
    "                label_drop_mask = torch.tensor([1 if t != 0 else 0 for t in label_drop_mask])\n",
    "                label = label[(1-label_drop_mask).bool()]\n",
    "                # TODO：label = label[:self.config.sum_max_size * self.config.max_n_segments]\n",
    "                label = label[:self.segment_size * self.config.max_n_segments]\n",
    "            \n",
    "            align = self.segment_alignment\n",
    "            if align in {'right', None}:\n",
    "                split_inds = (list(range(len(seq), 0, -self.segment_size)) + [0])[::-1]\n",
    "            elif align == 'left':\n",
    "                split_inds = list(range(0, len(seq), self.segment_size)) + [len(seq)]\n",
    "            elif align == 'center':\n",
    "                n_seg = math.ceil(len(seq) / self.segment_size)\n",
    "                split_inds = list(range(0, len(seq), math.ceil(len(seq) / n_seg))) + [len(seq)]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            input_segments = [seq[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "            input_segments = [self.pad_add_special_tokens(t, self.config.input_size) for t in input_segments]\n",
    "            \n",
    "            # add empty segment markers if needed\n",
    "            n_empty_segments = self.config.max_n_segments - len(input_segments)\n",
    "            # input_segments:\n",
    "            input_segments = input_segments + [self.get_full_padding_segment()] * n_empty_segments\n",
    "            \n",
    "            # segmented_batch: \n",
    "            segmented_batch.append(input_segments)\n",
    "            \n",
    "            if attn_mask is not None:\n",
    "                attn_mask_segments = [attn_mask[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "                attn_mask_segments = [self.pad_add_special_tokens(t, self.config.input_size, add_to='attention_mask') for t in attn_mask_segments]\n",
    "                attn_mask_segments = attn_mask_segments + [self.get_full_padding_segment()] * n_empty_segments\n",
    "                segmented_batch_attention_masks.append(attn_mask_segments)\n",
    "            \n",
    "            # TODO: labels need to be segmented by other rules\n",
    "            if label is not None:\n",
    "                # full_segment_size = len(input_segments)\n",
    "                end_index = math.ceil(len(label) // (full_segment_size := len(input_segments)))\n",
    "                labels_segments = [label[:(end_index*(i+1))] for i in range(full_segment_size)]\n",
    "                labels_segments = [self.pad_add_special_tokens(t, self.config.input_size, add_to='labels') for t in labels_segments]\n",
    "                labels_segments = labels_segments + [self.get_full_padding_segment()] * n_empty_segments\n",
    "                segmented_batch_labels.append(labels_segments)\n",
    "                \n",
    "        segmented_batch = [[sample[seg_num] for sample in segmented_batch] \n",
    "                            for seg_num in range(self.config.max_n_segments)]\n",
    "        segmented_batch_attention_masks = [[sample[seg_num] for sample in segmented_batch_attention_masks]\n",
    "                                           for seg_num in range(self.config.max_n_segments)]\n",
    "        segmented_batch_labels = [[sample[seg_num] for sample in segmented_batch_labels]\n",
    "                                  for seg_num in range(self.config.max_n_segments)]\n",
    "        return segmented_batch, segmented_batch_attention_masks, segmented_batch_labels\n",
    "        \n",
    "    def get_full_padding_segment(self,):\n",
    "        padding_segment = torch.tensor([self.pad_token_id for _ in range(self.config.input_size)])\n",
    "        return padding_segment\n",
    "    \n",
    "    def extract_special_tokens(self, tokenizer):\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.special_token_ids = [tokenizer.pad_token_id]\n",
    "        for token in ['cls_token', 'sep_token', 'eos_token', 'bos_token']:\n",
    "            token_id = getattr(tokenizer, f'{token}_id')\n",
    "            if token_id is not None:\n",
    "                self.register_buffer(token, torch.tensor([token_id]))\n",
    "                self.special_token_ids.append(token_id)\n",
    "            else:\n",
    "                setattr(self, token, None)\n",
    "                \n",
    "    # def extend_word_embeddings(self, tokenizer):\n",
    "    #     vocab_size = self.model.config.vocab_size\n",
    "    #     # NOTE: Really necessary???\n",
    "    #     extended_vocab_size = vocab_size + self.config.pre_seq_len\n",
    "    #     self.pre_seq_len = self.config.pre_seq_len\n",
    "\n",
    "    # Memory mechanism like RNN\n",
    "    def forget_and_memory(self,):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    #  prefix-tuning don't need to concat prefix and input sequence\n",
    "    def pad_add_special_tokens(self, tensor, segment_size, \n",
    "                               prompts=None, prompt_attention_mask=None, # maybe better to use pre_seq_len and generate prompts attention mask?\n",
    "                               add_to='input_ids'):\n",
    "        \"\"\"\n",
    "        bart tokenizer:\n",
    "        {'bos_token': '<s>', 0\n",
    "         'eos_token': '</s>', 2\n",
    "         'unk_token': '<unk>', 3\n",
    "         'sep_token': '</s>', 0\n",
    "         'pad_token': '<pad>', 1\n",
    "         'cls_token': '<s>', 0\n",
    "         'mask_token': '<mask>' 50264\n",
    "        }\n",
    "        \"\"\"\n",
    "        input_elements = []\n",
    "        # Add special tokens: <s> and </s> to the input sequence\n",
    "        # For prefix-prop\n",
    "        if prompts is not None:\n",
    "            if add_to == 'inputs':\n",
    "                input_elements += [self.cls_token, prompts, self.sep_token, tensor, self.sep_token]\n",
    "            # For Bart, only the pad token is 0 in attention_mask\n",
    "            elif add_to == 'attention_mask':\n",
    "                mask_value = torch.ones((1), device=tensor.device)\n",
    "                input_elements += [mask_value, prompt_attention_mask, mask_value, tensor, mask_value]\n",
    "            # As a encoder-decoder model：is not needed to add prompt to labels\n",
    "            elif add_to == 'labels':\n",
    "                # NOTE: for Seq2Seq Models labels are used for decoder_input_ids in training\n",
    "                # and decoder_input_ids must start with the eos_token\n",
    "                input_elements += [self.eos_token, tensor, self.sep_token]\n",
    "        # For prefix-tuning/p-tuning v2\n",
    "        else:\n",
    "            if add_to == 'input_ids':\n",
    "                input_elements += [self.cls_token, tensor, self.sep_token]\n",
    "            elif add_to == 'attention_mask':\n",
    "                mask_value = torch.ones((1), device=tensor.device)\n",
    "                input_elements += [mask_value, tensor, mask_value]\n",
    "            elif add_to == 'labels':\n",
    "                input_elements += [self.eos_token, tensor, self.sep_token]\n",
    "        tensor = torch.cat(input_elements)\n",
    "        \n",
    "        # Add padding tokens\n",
    "        # TODO: implement summary module\n",
    "        #       now config.sum_token_size default = 0\n",
    "        pad_size = segment_size - tensor.shape[0] - self.config.sum_token_size\n",
    "        if pad_size > 0:\n",
    "            if add_to == 'input_ids':\n",
    "                tensor = F.pad(tensor, (0, pad_size), value=self.pad_token_id)\n",
    "            elif add_to == 'attention_mask':\n",
    "                tensor = F.pad(tensor, (0, pad_size), value=0)\n",
    "            elif add_to == 'labels':\n",
    "                # for Seq2Seq labels need to be pad by -100\n",
    "                tensor = F.pad(tensor, (0, pad_size), value=-100)\n",
    "        return tensor\n",
    "\n",
    "        # TODO: this implementation just add <s> and </s> to the input sequence\n",
    "        #       maybe need to add other special tokens\n",
    "    \n",
    "    def prepare_kwargs(self, segment, kwargs):\n",
    "        segment_input_ids, segment_attention_mask, segment_label = segment\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "        \n",
    "        # [sample1_seg1, sample2_seg1, sample3_seg1,....] up to batch_size\n",
    "        # Some of the segments are None like: [sample1_seg3, None, sample3_seg3]\n",
    "        non_empty_mask = [s is not None for s in segment_input_ids]\n",
    "        # all the segments are None, due to the max_n_segments >> the number of segments        \n",
    "        if sum(non_empty_mask) == 0:\n",
    "            return None, non_empty_mask\n",
    "        \n",
    "        # convert list to tensor\n",
    "        \n",
    "        segment_input_ids = [tensor.to(self.model.device) if tensor is not None else None for tensor in segment_input_ids]\n",
    "        input_ids = torch.stack([s for s in segment_input_ids if s is not None])\n",
    "        seg_kwargs['input_ids'] = input_ids\n",
    "        \n",
    "        if segment_attention_mask is not None:\n",
    "            seg_kwargs['attention_mask'] = self.get_attention_mask(input_ids)\n",
    "        \n",
    "        if seg_kwargs['labels'] is not None:\n",
    "            seg_kwargs['labels'] = torch.stack([el for el, m in zip(segment_label, non_empty_mask) if m])\n",
    "        \n",
    "        # # generate prompts \n",
    "        # batch_size = input_ids.shape[0]\n",
    "        # past_key_values = self.get_prompt(batch_size)\n",
    "        # prefix_attention_mask = torch.ones(batch_size, self.pre_seq_len)\n",
    "        # if segment_attention_mask is not None:\n",
    "            # attention_mask = torch.stack([s for s in segment_attention_mask if s is not None])\n",
    "            # attn_mask = torch.cat([prefix_attention_mask, attention_mask], dim=1)\n",
    "        # seg_kwargs['attention_mask'] = attn_mask\n",
    "        # seg_kwargs['past_key_values'] = past_key_values\n",
    "        \n",
    "        return seg_kwargs, non_empty_mask\n",
    "        \n",
    "    def get_attention_mask(self, tensor):\n",
    "        mask = torch.ones_like(tensor)\n",
    "        mask[tensor == self.pad_token_id] = 0\n",
    "        return mask\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, Seq2SeqLMOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "        \"\"\" \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        kwargs = {\n",
    "            'attention_mask': attention_mask, \n",
    "            'inputs_embeds': inputs_embeds,\n",
    "            'labels': labels, \n",
    "            'output_attentions': output_attentions,\n",
    "            'output_hidden_states': output_hidden_states, \n",
    "            'return_dict': return_dict,\n",
    "        }\n",
    "\n",
    "        # segmented: [max_n_segments, batch_size, segment_size]\n",
    "        # !!! Note: the batch_size is not the same as the input batch_size\n",
    "        segmented = self.pad_and_segment(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "        \n",
    "        # NOTE: why???\n",
    "        # if self.pre_seq_len == 0:\n",
    "        #     segmented = segmented[-1:]\n",
    "        \n",
    "        model_outputs = []\n",
    "        for seg_num, segment in enumerate(zip(*segmented)):\n",
    "            in_ids, attn_mask, l = segment\n",
    "            # TODO: can't control the number of gradient accumulation steps now\n",
    "            if self.config.bptt_depth != -1:\n",
    "                raise NotImplementedError\n",
    "            \n",
    "            seg_kwargs, non_empty_mask = self.prepare_kwargs(segment, kwargs)\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "\n",
    "            out = self.model(**seg_kwargs)\n",
    "            model_outputs.append(out)\n",
    "            # memory_tokens = out.past_key_values\n",
    "            # print(\"memory_tokens: \", memory_tokens.shape)\n",
    "        \n",
    "        out = self.process_outputs(input_ids, model_outputs, output_attentions, output_hidden_states)\n",
    "        return out\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        min_length: Optional[int] = None,\n",
    "        do_sample: Optional[bool] = None,\n",
    "        early_stopping: Optional[bool] = None,\n",
    "        num_beams: Optional[int] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "        top_k: Optional[int] = None,\n",
    "        top_p: Optional[float] = None,\n",
    "        repetition_penalty: Optional[float] = None,\n",
    "        bad_words_ids: Optional[Iterable[int]] = None,\n",
    "        bos_token_id: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "        length_penalty: Optional[float] = None,\n",
    "        no_repeat_ngram_size: Optional[int] = None,\n",
    "        num_return_sequences: Optional[int] = None,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        decoder_start_token_id: Optional[int] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        **model_specific_kwargs\n",
    "    ) -> torch.LongTensor:\n",
    "\n",
    "        kwargs = {\n",
    "            'input_ids': input_ids,\n",
    "            'num_beams': num_beams,\n",
    "            'min_length': min_length,\n",
    "            'max_length': max_length,\n",
    "            'labels': None,\n",
    "            'attention_mask': None\n",
    "        }\n",
    "        \n",
    "        segmented = self.pad_and_segment(\n",
    "            input_ids=input_ids,\n",
    "        )\n",
    "        \n",
    "        model_outputs = []\n",
    "        for seg_num, segment in enumerate(zip(*segmented)):\n",
    "            in_ids, attn_mask, l = segment\n",
    "            \n",
    "            if self.config.bptt_depth != -1:\n",
    "                raise NotImplementedError\n",
    "            \n",
    "            seg_kwargs, non_empty_mask = self.prepare_kwargs(segment, kwargs)\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            \n",
    "            out = self.model.generate(**seg_kwargs)\n",
    "            model_outputs.append(out)\n",
    "        # print('model_outputs: ', model_outputs)\n",
    "        # the last segment of each sample and test the first sample's last segment\n",
    "        print(\"decoded_model_outputs: \", self.tokenizer.decode(model_outputs[-1][0], skip_special_tokens=True))\n",
    "        \n",
    "        # TODO: need to process fully padding segment\n",
    "        #       process_output for generate\n",
    "        return model_outputs\n",
    "        \n",
    "    def process_outputs(self, input_ids, model_outputs, output_attentions, output_hidden_states):\n",
    "        out = model_outputs[-1] # get the last segment output\n",
    "        \n",
    "        bs, seq_len = input_ids.shape\n",
    "        \n",
    "        losses = []\n",
    "        logits = []\n",
    "        labels_segm = []\n",
    "        \n",
    "        for out in model_outputs:\n",
    "            losses.append(out['loss'])\n",
    "            logits.append(out['logits'].detach())\n",
    "            # if out['seg_kwargs'] is not None:\n",
    "            #     labels_segm += [out['seg_kwargs']['labels']]\n",
    "        \n",
    "        if not output_hidden_states:\n",
    "            for key in out.keys():\n",
    "                if 'hidden_state' in key:\n",
    "                    out[key] = None\n",
    "                    \n",
    "        for i, l in enumerate(losses):\n",
    "            out[f'loss_{i}'] = l.mean()\n",
    "            \n",
    "        out['loss'] = torch.stack(losses).mean()\n",
    "        \n",
    "        # TODO: need to be fixed | out of index error\n",
    "        # for i in range(len(logits)):\n",
    "        #     logits[i] = F.pad(logits[i], (0, 0, 0, 0, 0, bs - logits[i].shape[0]))\n",
    "        #     labels_segm[i] = F.pad(labels_segm[i], (0, 0, 0, bs - labels_segm[i].shape[0]), value=-100)\n",
    "        \n",
    "        out['logits'] = torch.cat(logits, dim=1)\n",
    "        # Warning: rmt logits, labels, masks are not in the same order as in input data:\n",
    "        # the first dimension is number of segments!\n",
    "        # so, torch.cat will result in segm0, segm0,.. and only after all segm0 will come segm1, ... .\n",
    "        # not segm0, segm1, segm0, segm1 as in input data\n",
    "        out['logits_segm'] = [logits]\n",
    "        out['labels_segm'] = [labels_segm]\n",
    "        \n",
    "        return out\n",
    "    \n",
    "# prefix-propagation version\n",
    "class BartPrefixPropForConditionalGeneration(BartPretrainedModel):\n",
    "    def __init__(self, config, checkpoint):\n",
    "        super().__init__(config)\n",
    "        self.model = BartForConditionalGeneration.from_pretrained(checkpoint)\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(checkpoint)\n",
    "        \n",
    "        self.config = config\n",
    "        self.segment_alignment = config.segment_alignment\n",
    "        self.extract_special_tokens(self.tokenizer)\n",
    "        self.pre_seq_len = config.pre_seq_len\n",
    "        self.n_layer = config.num_hidden_layers\n",
    "        self.n_head = config.num_attention_heads\n",
    "        self.n_embd = config.hidden_size // config.num_attention_heads\n",
    "        # self.extend_word_embeddings(config.pre_seq_len, tokenizer)\n",
    "        \n",
    "        # tokenizer.num_special_tokens_to_add()cal the number of special tokens needed to add except [SEP]\n",
    "        self.segment_size = config.input_size - self.pre_seq_len - self.tokenizer.num_special_tokens_to_add()\n",
    "        if 'sep_token' in self.tokenizer.special_tokens_map:\n",
    "            self.segment_size -= 1\n",
    "        \n",
    "        # TODO: forget some part of long range memory and add new memory\n",
    "\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.prefix_tokens = torch.arange(self.pre_seq_len).long()\n",
    "        self.prefix_encoder = PrefixEncoder(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "        model_param = 0\n",
    "        all_param = 0\n",
    "        \n",
    "        # count the number of trainable parameters in bart\n",
    "        for name, param in self.model.named_parameters():\n",
    "            model_param += param.numel() # numel() returns the total number of elements in the input tensor\n",
    "            \n",
    "        for name, param in self.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            \n",
    "        trainable_param = all_param - model_param\n",
    "        \n",
    "        print(\"Total parameters: {:,}\".format(all_param))\n",
    "        print(\"Trainable parameters: {:,} {:,%}\".format((trainable_param), trainable_param/all_param))\n",
    "    \n",
    "    def get_prompt(self, batch_size, memory=None):\n",
    "        if memory is not None:\n",
    "            prompts = self.prefix_encoder(memory)\n",
    "        else: \n",
    "            prefix_tokens = (\n",
    "                self.prefix_tokens.unsqueeze(0).expand(batch_size, -1).to(self.model.device)\n",
    "            )\n",
    "            prompts = self.prefix_encoder(prefix_tokens)\n",
    "        return prompts\n",
    "    \n",
    "    def extract_special_tokens(self, tokenizer):\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.special_token_ids = [tokenizer.pad_token_id]\n",
    "        for token in ['cls_token', 'sep_token', 'eos_token', 'bos_token']:\n",
    "            token_id = getattr(tokenizer, f'{token}_id')\n",
    "            if token_id is not None:\n",
    "                self.register_buffer(token, torch.tensor([token_id]))\n",
    "                self.special_token_ids.append(token_id)\n",
    "            else:\n",
    "                setattr(self, token, None)\n",
    "# ============================================\n",
    "# ================ T5 model ==================\n",
    "# ============================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import PromptBartConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptBartConfig {\n",
       "  \"_name_or_path\": \"facebook/bart-base\",\n",
       "  \"activation_dropout\": 0.1,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartModel\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"bptt_depth\": -1,\n",
       "  \"classif_dropout\": 0.1,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 12,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"dropout\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 12,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"forced_bos_token_id\": 0,\n",
       "  \"forced_eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"input_size\": 512,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"label_max_size\": 256,\n",
       "  \"max_n_segments\": 4,\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_beams\": 4,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"pre_seq_len\": 20,\n",
       "  \"prefix_projection\": false,\n",
       "  \"scale_embedding\": false,\n",
       "  \"segment_alignment\": \"left\",\n",
       "  \"sum_loss\": true,\n",
       "  \"sum_token_size\": 0,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"length_penalty\": 1.0,\n",
       "      \"max_length\": 128,\n",
       "      \"min_length\": 12,\n",
       "      \"num_beams\": 4\n",
       "    },\n",
       "    \"summarization_cnn\": {\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 142,\n",
       "      \"min_length\": 56,\n",
       "      \"num_beams\": 4\n",
       "    },\n",
       "    \"summarization_xsum\": {\n",
       "      \"length_penalty\": 1.0,\n",
       "      \"max_length\": 62,\n",
       "      \"min_length\": 11,\n",
       "      \"num_beams\": 6\n",
       "    }\n",
       "  },\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.32.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "bart_config = AutoConfig.from_pretrained('facebook/bart-base')\n",
    "custom_config = PromptBartConfig(**bart_config.to_dict())\n",
    "custom_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSeq2SeqLM(\n",
       "  (base_model): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): Embedding(50265, 768, padding_idx=1)\n",
       "      (encoder): BartEncoder(\n",
       "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       "  )\n",
       "  (prompt_encoder): ModuleDict(\n",
       "    (default): PrefixEncoder(\n",
       "      (embedding): Embedding(20, 9216)\n",
       "    )\n",
       "  )\n",
       "  (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_model = BartPrefixForConditionalGeneration(custom_config, tokenizer_path='facebook/bart-base')\n",
    "pretrained_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "bart_model.model.load_state_dict(pretrained_model.state_dict())\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    num_virtual_tokens=20,\n",
    ")\n",
    "bart_model.model = get_peft_model(bart_model.model, peft_config)\n",
    "bart_model.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_model.model.save_pretrained('./test_peft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartPrefixForConditionalGeneration(\n",
       "  (model): BartForConditionalGeneration(\n",
       "    (model): BartModel(\n",
       "      (shared): Embedding(50265, 768, padding_idx=1)\n",
       "      (encoder): BartEncoder(\n",
       "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartEncoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (activation_fn): GELUActivation()\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): BartDecoder(\n",
       "        (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "        (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "        (layers): ModuleList(\n",
       "          (0-5): 6 x BartDecoderLayer(\n",
       "            (self_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (activation_fn): GELUActivation()\n",
       "            (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (encoder_attn): BartAttention(\n",
       "              (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            )\n",
       "            (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       "  )\n",
       "  (test): Linear(in_features=64, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model = BartPrefixForConditionalGeneration(custom_config, tokenizer_path='facebook/bart-base')\n",
    "pretrained_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n",
    "test_model.model.load_state_dict(pretrained_model.state_dict())\n",
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BartPrefixForConditionalGeneration(\n",
       "  (model): PeftModelForSeq2SeqLM(\n",
       "    (base_model): PeftModelForSeq2SeqLM(\n",
       "      (base_model): BartForConditionalGeneration(\n",
       "        (model): BartModel(\n",
       "          (shared): Embedding(50265, 768, padding_idx=1)\n",
       "          (encoder): BartEncoder(\n",
       "            (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "            (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "            (layers): ModuleList(\n",
       "              (0-5): 6 x BartEncoderLayer(\n",
       "                (self_attn): BartAttention(\n",
       "                  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (activation_fn): GELUActivation()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (decoder): BartDecoder(\n",
       "            (embed_tokens): Embedding(50265, 768, padding_idx=1)\n",
       "            (embed_positions): BartLearnedPositionalEmbedding(1026, 768)\n",
       "            (layers): ModuleList(\n",
       "              (0-5): 6 x BartDecoderLayer(\n",
       "                (self_attn): BartAttention(\n",
       "                  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (activation_fn): GELUActivation()\n",
       "                (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (encoder_attn): BartAttention(\n",
       "                  (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                  (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                )\n",
       "                (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (layernorm_embedding): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (lm_head): Linear(in_features=768, out_features=50265, bias=False)\n",
       "      )\n",
       "      (prompt_encoder): ModuleDict(\n",
       "        (default): PrefixEncoder(\n",
       "          (embedding): Embedding(20, 9216)\n",
       "        )\n",
       "      )\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "    )\n",
       "    (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "  )\n",
       "  (test): Linear(in_features=64, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "test_model.model = PeftModel.from_pretrained(test_model.model, './test_peft')\n",
    "test_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_seg = torch.tensor([1 for _ in range(5)])\n",
    "non_empyty_mask = torch.tensor([True, False, True, True, True])\n",
    "padding_seg[non_empyty_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([1, 2, 3])]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs = []\n",
    "app = torch.tensor([1,2,3])\n",
    "model_outputs.append(app)\n",
    "model_outputs[0] = torch.tensor([1,2,3])\n",
    "model_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_outputs = [1]\n",
    "not len(model_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
