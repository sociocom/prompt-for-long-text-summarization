{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "from datasets import load_dataset, load_metric\n",
    "import torch\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "!wandb login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vw970or3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e259c5475ee34e1fa7a9bdecc7283256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.013 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.227164…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▄▅▁▂▅▄▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.99713</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rare-wind-1</strong> at: <a href='https://wandb.ai/kaifan-li/my_project/runs/vw970or3' target=\"_blank\">https://wandb.ai/kaifan-li/my_project/runs/vw970or3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230804_225122-vw970or3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vw970or3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07bc6da2c3d4e70a35c6eab7030ed93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668189813693366, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/is/kaifan-l/private_room/proj-repos/prompt-for-long-text-summarization/wandb/run-20230804_225935-7shln82z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaifan-li/my_project/runs/7shln82z' target=\"_blank\">swept-dawn-2</a></strong> to <a href='https://wandb.ai/kaifan-li/my_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaifan-li/my_project' target=\"_blank\">https://wandb.ai/kaifan-li/my_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaifan-li/my_project/runs/7shln82z' target=\"_blank\">https://wandb.ai/kaifan-li/my_project/runs/7shln82z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ['WANDB_DIR'] = os.getcwd() + '/wandb/'\n",
    "os.environ['WANDB_CACHE_DIR'] = os.getcwd() + '/wandb/.cache/'\n",
    "os.environ['WANDB_CONFIG_DIR'] = os.getcwd() + '/wandb/.config/'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 初始化wandb\n",
    "wandb.init(\n",
    "    entity='kaifan-li',\n",
    "    project=\"my_project\"\n",
    ")\n",
    "\n",
    "# 构建模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i in range(100):\n",
    "        inputs = torch.randn(32, 10)  # 随机生成输入数据\n",
    "        labels = torch.randn(32, 1)   # 随机生成标签\n",
    "        \n",
    "        # 正向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # 记录训练过程和指标\n",
    "    avg_loss = running_loss / 100\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": avg_loss})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers import BartConfig, T5Config\n",
    "\n",
    "class PromptBartConfig(BartConfig):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.pre_seq_len = config.pre_seq_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一步是将处理后的数据集 `segmented_batch` 进行转置，使得每个子句在第一维度，即 `batch_size = 1`。原始的 `segmented_batch` 是一个列表的列表，其中每个列表代表一个样本，而每个样本可能被分成多个子句。而在这个步骤中，我们将所有样本的第 `seg_num` 个子句提取出来，形成一个新的列表，并将这些列表组成一个新的 `segmented_batch` 列表。\n",
    "\n",
    "举个例子，假设原始的 `segmented_batch` 如下：\n",
    "\n",
    "```python\n",
    "segmented_batch = [[sample1_seg1, sample1_seg2, sample1_seg3],\n",
    "                   [sample2_seg1, sample2_seg2],\n",
    "                   [sample3_seg1, sample3_seg2, sample3_seg3, sample3_seg4]]\n",
    "```\n",
    "\n",
    "其中 `sample1_seg1` 表示第一个样本的第一个子句，`sample1_seg2` 表示第一个样本的第二个子句，以此类推。\n",
    "\n",
    "经过转置后的 `segmented_batch` 如下：\n",
    "\n",
    "```python\n",
    "segmented_batch = [[sample1_seg1, sample2_seg1, sample3_seg1],\n",
    "                   [sample1_seg2, sample2_seg2, sample3_seg2],\n",
    "                   [sample1_seg3, None, sample3_seg3],\n",
    "                   [None, None, sample3_seg4]]\n",
    "```\n",
    "\n",
    "注意，这里可能会出现 `None`，因为不同样本可能具有不同数量的子句。这种转置操作通常用于将一个批次的数据转换为序列模型的输入，使得每个子句在第一维度，方便进行后续的处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seq: tensor([101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 102, 111, 112, 113,\n",
      "        114, 115, 102, 116, 117, 102])\n",
      "drop_mask: tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 1])\n",
      "seq: tensor([103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "        117])\n",
      "input_segments flag 1: [tensor([103, 104, 105, 106, 107]), tensor([108, 109, 110, 111, 112]), tensor([113, 114, 115, 116, 117])]\n",
      "input_segments flag 2: [None, None, None, None, None, None, None, tensor([103, 104, 105, 106, 107]), tensor([108, 109, 110, 111, 112]), tensor([113, 114, 115, 116, 117])]\n",
      "segmented_batch: [[None, None, None, None, None, None, None, tensor([103, 104, 105, 106, 107]), tensor([108, 109, 110, 111, 112]), tensor([113, 114, 115, 116, 117])]]\n",
      "seq: tensor([101, 102, 103, 104, 105, 106, 102, 107, 108, 109, 110, 111, 102, 112,\n",
      "        113, 114, 115, 102, 116, 117])\n",
      "drop_mask: tensor([1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0])\n",
      "seq: tensor([103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116,\n",
      "        117])\n",
      "input_segments flag 1: [tensor([103, 104, 105, 106, 107]), tensor([108, 109, 110, 111, 112]), tensor([113, 114, 115, 116, 117])]\n",
      "input_segments flag 2: [None, None, None, None, None, None, None, tensor([103, 104, 105, 106, 107]), tensor([108, 109, 110, 111, 112]), tensor([113, 114, 115, 116, 117])]\n",
      "segmented_batch: [[None, None, None, None, None, None, None, tensor([103, 104, 105, 106, 107]), tensor([108, 109, 110, 111, 112]), tensor([113, 114, 115, 116, 117])], [None, None, None, None, None, None, None, tensor([103, 104, 105, 106, 107]), tensor([108, 109, 110, 111, 112]), tensor([113, 114, 115, 116, 117])]]\n",
      "=============\n",
      "segmented_batch: [[None, None], [None, None], [None, None], [None, None], [None, None], [None, None], [None, None], [tensor([103, 104, 105, 106, 107]), tensor([103, 104, 105, 106, 107])], [tensor([108, 109, 110, 111, 112]), tensor([108, 109, 110, 111, 112])], [tensor([113, 114, 115, 116, 117]), tensor([113, 114, 115, 116, 117])]]\n",
      "Original input_ids:\n",
      "tensor([[101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 102, 111, 112, 113,\n",
      "         114, 115, 102, 116, 117, 102],\n",
      "        [101, 102, 103, 104, 105, 106, 102, 107, 108, 109, 110, 111, 102, 112,\n",
      "         113, 114, 115, 102, 116, 117]])\n",
      "Processed input_ids:\n",
      "[[None, None], [None, None], [None, None], [None, None], [None, None], [None, None], [None, None], [tensor([103, 104, 105, 106, 107]), tensor([103, 104, 105, 106, 107])], [tensor([108, 109, 110, 111, 112]), tensor([108, 109, 110, 111, 112])], [tensor([113, 114, 115, 116, 117]), tensor([113, 114, 115, 116, 117])]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "input_ids = torch.tensor([\n",
    "    [101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 102, 111, 112, 113, 114, 115, 102, 116, 117, 102],\n",
    "    [101, 102, 103, 104, 105, 106, 102, 107, 108, 109, 110, 111, 102, 112, 113, 114, 115, 102, 116, 117]\n",
    "])\n",
    "\n",
    "special_token_ids = [101, 102]\n",
    "\n",
    "\n",
    "\n",
    "class Processor:\n",
    "    def __init__(self, segment_size, rmt_config, special_token_ids):\n",
    "        self.segment_size = segment_size\n",
    "        self.rmt_config = rmt_config\n",
    "        self.special_token_ids = special_token_ids\n",
    "\n",
    "    def pad_and_segment(self, input_ids):\n",
    "        segmented_batch = []\n",
    "        for seq in input_ids:\n",
    "            print('seq:', seq)\n",
    "            drop_mask = sum([seq == t for t in self.special_token_ids])\n",
    "            print('drop_mask:', drop_mask)\n",
    "            seq = seq[(1 - drop_mask).bool()]\n",
    "            print('seq:', seq)\n",
    "            seq = seq[:self.segment_size * self.rmt_config['max_n_segments']]\n",
    "            \n",
    "            align = self.rmt_config['segment_alignment']\n",
    "            if align in {'right', None}:\n",
    "                split_inds = (list(range(len(seq), 0, -self.rmt_config['segment_size'])) + [0])[::-1]\n",
    "            elif align == 'left':\n",
    "                split_inds = list(range(0, len(seq), self.rmt_config['segment_size'])) + [len(seq)]\n",
    "            elif align == 'center':\n",
    "                n_seg = math.ceil(len(seq) / self.rmt_config['segment_size'])\n",
    "                split_inds = list(range(0, len(seq), math.ceil(len(seq) / n_seg))) + [len(seq)]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            input_segments = [seq[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "            print('input_segments flag 1:', input_segments)\n",
    "            # TODO: do the implementation\n",
    "            # input_segments = [self.pad_add_special_tokens(t, self.config.input_size) for t in input_segments]\n",
    "            \n",
    "            # add empty segment markers if needed\n",
    "            n_empty_segments = self.rmt_config['max_n_segments'] - len(input_segments)\n",
    "            # input_segments:\n",
    "            input_segments = [None] * n_empty_segments + input_segments\n",
    "            print('input_segments flag 2:', input_segments)\n",
    "            # segmented_batch: \n",
    "            segmented_batch.append(input_segments)\n",
    "            print('segmented_batch:', segmented_batch)\n",
    "            \n",
    "        print('=============')\n",
    "        segmented_batch = [[sample[seg_num] for sample in segmented_batch] \\\n",
    "                    for seg_num in range(self.rmt_config['max_n_segments'])]\n",
    "        print('segmented_batch:', segmented_batch)\n",
    "        return segmented_batch\n",
    "\n",
    "# 假设我们有一个名为 rmt_config 的配置字典和 segment_size 变量\n",
    "rmt_config = {'max_n_segments': 10,\n",
    "              'segment_size': 5,\n",
    "              'segment_alignment': 'right',}\n",
    "segment_size = 5\n",
    "\n",
    "processor = Processor(segment_size, rmt_config, special_token_ids)\n",
    "output = processor.pad_and_segment(input_ids)\n",
    "\n",
    "print(\"Original input_ids:\")\n",
    "print(input_ids)\n",
    "print(\"Processed input_ids:\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = \"Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13014"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(long_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/is/kaifan-l/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "\n",
    "import numpy as np\n",
    "import psutil\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "\n",
    "from peft import PrefixTuningConfig, TaskType, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator() # device_placement=\"cuda:0\"\n",
    "model_name_or_path = \"facebook/bart-large\"\n",
    "dataset_name = \"cnn_dailymail\"\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    num_virtual_tokens=20,\n",
    ")\n",
    "text_column = 'article'\n",
    "label_column = 'highlights'\n",
    "lr = 3e-3\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "do_test = True\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset cnn_dailymail (/home/is/kaifan-l/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1d9366c0e4492695f192c66a459242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnn_dataset = load_dataset(dataset_name, \"3.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "target_max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples[text_column]\n",
    "    targets = examples[label_column]\n",
    "    model_inputs = tokenizer(inputs, truncation=True) # 这里暂时不padding\n",
    "    targets = tokenizer(\n",
    "        targets,\n",
    "        max_length=target_max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    targets = targets['input_ids']\n",
    "    targets[targets == tokenizer.pad_token_id] = -100\n",
    "    model_inputs['labels'] = targets\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/is/kaifan-l/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-032269279110a08c.arrow\n",
      "Loading cached processed dataset at /home/is/kaifan-l/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-39ca471479d521d4.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1d05f5b90074d9a98b8c4beef10ee6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with accelerator.main_process_first():\n",
    "    cnn_dataset = cnn_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        remove_columns=cnn_dataset[\"train\"].column_names,\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/is/kaifan-l/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-99312ead8dbc8f26.arrow\n",
      "Loading cached shuffled indices for dataset at /home/is/kaifan-l/.cache/huggingface/datasets/cnn_dailymail/3.0.0/3.0.0/1b3c71476f6d152c31c1730e83ccb08bcf23e348233f4fcc11e182248e6bf7de/cache-0044f3d14da28a04.arrow\n"
     ]
    }
   ],
   "source": [
    "train_size = int(len(cnn_dataset[\"train\"]) * 0.01)\n",
    "eval_size = int(len(cnn_dataset[\"validation\"]) * 0.01)\n",
    "test_size = int(len(cnn_dataset[\"test\"]) * 0.01)\n",
    "\n",
    "# 从打乱后的数据集中随机抽取指定数量的数据\n",
    "train_dataset = cnn_dataset[\"train\"].shuffle(seed=42).select(range(train_size))\n",
    "eval_dataset = cnn_dataset[\"validation\"].shuffle(seed=42).select(range(eval_size))\n",
    "test_dataset = cnn_dataset[\"test\"].shuffle(seed=42).select(range(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    return tokenizer.pad(examples, padding='longest', return_tensors='pt')\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True, # 将数据加载到固定的内存中，可以加速数据加载\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0,  9518,  3579,  ...,  -100,  -100,  -100],\n",
      "        [    0, 29178,   260,  ...,  -100,  -100,  -100],\n",
      "        [    0, 10993,  4645,  ...,  -100,  -100,  -100],\n",
      "        ...,\n",
      "        [    0,  3762,     9,  ...,  -100,  -100,  -100],\n",
      "        [    0,  4993,    10,  ...,  -100,  -100,  -100],\n",
      "        [    0,   133,  1275,  ...,  -100,  -100,  -100]])\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    print(batch['labels'])\n",
    "    break\n",
    "\n",
    "# \"input_ids\" | \"attention_mask\" | \"labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "for step, batch in enumerate(train_dataloader):\n",
    "    print(batch.keys())\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import logging\n",
    "import copy\n",
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "from transformers import (\n",
    "    BartForConditionalGeneration, \n",
    "    T5ForConditionalGeneration,\n",
    "    # GPT2ForConditionalGeneration,\n",
    ")\n",
    "from transformers import BartConfig, T5Config, GPT2Config\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "from model.prefix_encoder import PrefixEncoder\n",
    "\n",
    "# copied from transformers.modeling_bart.py\n",
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    if pad_token_id is None:\n",
    "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
    "    # replace possible -100 values in labels by `pad_token_id`\n",
    "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "\n",
    "    return shifted_input_ids\n",
    "\n",
    "# prefix-tuning/p-tuning v2 version\n",
    "class BartPrefixForConditionalGeneration(BartForConditionalGeneration):\n",
    "    def __init__(self, config: BartConfig, tokenizer):\n",
    "        super().__init__(config)\n",
    "        # self.model = BartModel(config)\n",
    "        # self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
    "        # self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n",
    "        \n",
    "        # MODIFIED\n",
    "        # Start\n",
    "        self.set_params(\n",
    "            tokenizer=tokenizer,\n",
    "            config=config\n",
    "        )\n",
    "        \n",
    "        # TODO: forget some part of long range memory and add new memory\n",
    "        # \n",
    "        # End\n",
    "        \n",
    "        # https://github.com/huggingface/transformers/issues/4701\n",
    "        # if we use BartPrefixForConditionalGeneration.from_pretrained() to load the model, \n",
    "        # it will not overwrite the pretrained weights of the model\n",
    "        # Initialize weights and apply final processing\n",
    "        # self.post_init()\n",
    "        \n",
    "        # MODIFIED\n",
    "        # Start\n",
    "        for param in self.model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        for param in self.lm_head.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        self.pre_seq_len = config.pre_seq_len\n",
    "        self.n_layer = config.num_hidden_layers\n",
    "        self.n_head = config.num_attention_heads\n",
    "        self.n_embd = config.hidden_size // config.num_attention_heads\n",
    "        \n",
    "        self.prefix_tokens = torch.arange(self.pre_seq_len).long()\n",
    "        self.prefix_encoder = PrefixEncoder(config)\n",
    "        self.dropout = nn.Dropout()\n",
    "        \n",
    "        bart_param = 0\n",
    "        all_param = 0\n",
    "        \n",
    "        # count the number of trainable parameters in bart\n",
    "        for name, param in self.model.named_parameters():\n",
    "            bart_param += param.numel() # numel() returns the total number of elements in the input tensor\n",
    "        \n",
    "        for name, param in self.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            \n",
    "        trainable_param = all_param - bart_param\n",
    "        \n",
    "        print(\"Total parameters: {:,}\".format(all_param))\n",
    "        print(\"Trainable parameters: {:,} {:,%}\".format((trainable_param), trainable_param/all_param))\n",
    "        # End\n",
    "\n",
    "    # MODIFIED\n",
    "    # Start\n",
    "    def get_prompt(self, batch_size):\n",
    "        prefix_tokens = self.prefix_tokens.unsqueeze(0).expand(batch_size, -1).to(self.model.device)\n",
    "        past_key_values = self.prefix_encoder(prefix_tokens)\n",
    "        bsz, seqlen, _ = past_key_values.shape\n",
    "        past_key_values = past_key_values.view(\n",
    "            bsz,\n",
    "            seqlen,\n",
    "            self.n_layer * 2,\n",
    "            self.n_head,\n",
    "            self.n_embd\n",
    "        )        \n",
    "        past_key_values = self.dropout(past_key_values)\n",
    "        past_key_values = past_key_values.permute([2, 0, 3, 1, 4])\n",
    "        return past_key_values\n",
    "    # End\n",
    "    \n",
    "    # MODIFIED\n",
    "    # Start\n",
    "    def pad_and_segment(self, input_ids, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        segment input_ids into segments\n",
    "        \n",
    "        input sample:\n",
    "        segmented_batch = [\n",
    "            [sample1_seg1, sample1_seg2, sample1_seg3],\n",
    "            [sample2_seg1, sample2_seg2],\n",
    "            [sample3_seg1, sample3_seg2, sample3_seg3, sample3_seg4]\n",
    "        ]\n",
    "                   \n",
    "        output sample:\n",
    "        segmented_batch = [\n",
    "            [sample1_seg1, sample2_seg1, sample3_seg1],\n",
    "            [sample1_seg2, sample2_seg2, sample3_seg2],\n",
    "            [sample1_seg3, None, sample3_seg3],\n",
    "            [None, None, sample3_seg4]\n",
    "        ]\n",
    "        \"\"\"\n",
    "        segmented_batch = []\n",
    "        segmented_batch_attention_masks = []\n",
    "        segmented_batch_labels = []\n",
    "        \n",
    "        if attention_mask is None:\n",
    "            attention_mask = [None] * input_ids.shape[0]\n",
    "        batch_attention_mask = attention_mask\n",
    "            \n",
    "        # inference mode\n",
    "        if labels is None:\n",
    "            labels = [None] * input_ids.shape[0]\n",
    "        batch_labels = labels\n",
    "        \n",
    "        # input_ids: [batch_size, seq_len]\n",
    "        for seq, attn_mask, label in zip(input_ids, batch_attention_mask, batch_labels):\n",
    "            \n",
    "            # pytorch syntax: element-wise operation\n",
    "            drop_mask = sum([seq == t for t in self.special_token_ids])\n",
    "            \n",
    "            # bool type slice for tensor type\n",
    "            # remove special tokens\n",
    "            seq = seq[(1 - drop_mask).bool()]\n",
    "            \n",
    "            # truncate the sequence to the maximum length\n",
    "            # TODO: config is NotImplemented  dict or dataclass?\n",
    "            seq = seq[:self.config.segment_size * self.config.max_n_segments]\n",
    "            \n",
    "            if att_mask is not None:\n",
    "                att_mask = att_mask[(1-drop_mask).bool()]\n",
    "                att_mask = att_mask[:self.config.segment_size * self.config.max_n_segments]\n",
    "            if label is not None:\n",
    "                label = label[(1-drop_mask).bool()]\n",
    "                label = label[:self.config.segment_size * self.config.max_n_segments]\n",
    "            \n",
    "            \n",
    "            align = self.config.segment_alignment\n",
    "            if align in {'right', None}:\n",
    "                split_inds = (list(range(len(seq), 0, -self.config.segment_size)) + [0])[::-1]\n",
    "            elif align == 'left':\n",
    "                split_inds = list(range(0, len(seq), self.config.segment_size)) + [len(seq)]\n",
    "            elif align == 'center':\n",
    "                n_seg = math.ceil(len(seq) / self.config.segment_size)\n",
    "                split_inds = list(range(0, len(seq), math.ceil(len(seq) / n_seg))) + [len(seq)]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            input_segments = [seq[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "            input_segments = [self.pad_add_special_tokens(t, self.config.input_size) for t in input_segments]\n",
    "            \n",
    "            # add empty segment markers if needed\n",
    "            n_empty_segments = self.config.max_n_segments - len(input_segments)\n",
    "            # input_segments:\n",
    "            input_segments = [None] * n_empty_segments + input_segments\n",
    "            \n",
    "            # segmented_batch: \n",
    "            segmented_batch.append(input_segments)\n",
    "            \n",
    "            if attn_mask is not None:\n",
    "                attn_mask_segments = [att_mask[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "                attn_mask_segments = [self.pad_add_special_tokens(t, self.config.input_size, add_to='attention_mask') for t in attn_mask_segments]\n",
    "                attn_mask_segments = [None] * n_empty_segments + attn_mask_segments\n",
    "                segmented_batch_attention_masks.append(attn_mask_segments)\n",
    "            \n",
    "            if labels is not None:\n",
    "                labels_segments = [labels[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "                labels_segments = [self.pad_add_special_tokens(t, self.config.input_size, add_to='labels') for t in labels_segments]\n",
    "                labels_segments = [None] * n_empty_segments + labels_segments\n",
    "                segmented_batch_labels.append(labels_segments)\n",
    "                \n",
    "        segmented_batch = [[sample[seg_num] for sample in segmented_batch] \n",
    "                            for seg_num in range(self.config.max_n_segments)]\n",
    "        segmented_batch_attention_masks = [[sample[seg_num] for sample in segmented_batch_attention_masks]\n",
    "                                           for seg_num in range(self.config.max_n_segments)]\n",
    "        segmented_batch_labels = [[sample[seg_num] for sample in segmented_batch_labels]\n",
    "                                  for seg_num in range(self.config.max_n_segments)]\n",
    "        return segmented_batch, segmented_batch_attention_masks, segmented_batch_labels\n",
    "    # End\n",
    "    \n",
    "    def set_params(self, tokenizer, config):\n",
    "        self.config = config \n",
    "        self.extract_special_tokens(tokenizer)\n",
    "        # self.extend_word_embeddings(config['pre_seq_len'], tokenizer)\n",
    "        \n",
    "        # tokenizer.num_special_tokens_to_add()cal the number of special tokens needed to add except [SEP]\n",
    "        self.segment_size = config.input_size - self.config.pre_seq_len - tokenizer.num_special_tokens_to_add()\n",
    "        if 'sep_token' in tokenizer.special_tokens_map:\n",
    "            self.segment_size -= 1\n",
    "        \n",
    "    def extract_special_tokens(self, tokenizer):\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.special_token_ids = [tokenizer.pad_token_id]\n",
    "        for token in ['cls_token', 'sep_token', 'eos_token', 'bos_token']:\n",
    "            token_id = getattr(tokenizer, f'{token}_id')\n",
    "            if token_id is not None:\n",
    "                self.register_buffer(token, torch.tensor([token_id]))\n",
    "                self.special_token_ids.append(token_id)\n",
    "            else:\n",
    "                setattr(self, token, None)\n",
    "                \n",
    "    # def extend_word_embeddings(self, tokenizer):\n",
    "    #     vocab_size = self.model.config.vocab_size\n",
    "    #     # NOTE: Really necessary???\n",
    "    #     extended_vocab_size = vocab_size + self.config.pre_seq_len\n",
    "    #     self.pre_seq_len = self.config.pre_seq_len\n",
    "\n",
    "    # Memory mechanism like RNN\n",
    "    def forget_and_memory(self,):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    #  prefix-tuning don't need to concat prefix and input sequence\n",
    "    def pad_add_special_tokens(self, tensor, segment_size, \n",
    "                               prompts=None, prompt_attention_mask=None, # maybe better to use pre_seq_len and generate prompts attention mask?\n",
    "                               add_to='input_ids'):\n",
    "        \"\"\"\n",
    "        bart tokenizer:\n",
    "        {'bos_token': '<s>', 0\n",
    "         'eos_token': '</s>', 2\n",
    "         'unk_token': '<unk>', 3\n",
    "         'sep_token': '</s>', 0\n",
    "         'pad_token': '<pad>', 1\n",
    "         'cls_token': '<s>', 0\n",
    "         'mask_token': '<mask>' 50264\n",
    "        }\n",
    "        \"\"\"\n",
    "        input_elements = []\n",
    "        # Add special tokens: <s> and </s> to the input sequence\n",
    "        # For prefix-prop\n",
    "        if prompts is not None:\n",
    "            if add_to == 'inputs':\n",
    "                input_elements += [self.cls_token, prompts, self.sep_token, tensor, self.sep_token]\n",
    "            # For Bart, only the pad token is 0 in attention_mask\n",
    "            elif add_to == 'attention_mask':\n",
    "                mask_value = torch.ones((1), device=tensor.device)\n",
    "                input_elements += [mask_value, prompt_attention_mask, mask_value, tensor, mask_value]\n",
    "            # As a encoder-decoder model：is not needed to add prompt to labels\n",
    "            elif add_to == 'labels':\n",
    "                input_elements += [self.cls_token, tensor, self.sep_token]\n",
    "        # For prefix-tuning/p-tuning v2\n",
    "        else:\n",
    "            if add_to == 'input_ids':\n",
    "                input_elements += [self.cls_token, tensor, self.sep_token]\n",
    "            elif add_to == 'attention_mask':\n",
    "                mask_value = torch.ones((1), device=tensor.device)\n",
    "                input_elements += [mask_value, tensor, mask_value]\n",
    "            elif add_to == 'labels':\n",
    "                input_elements += [self.cls_token, tensor, self.sep_token]\n",
    "        tensor = torch.cat(input_elements)\n",
    "        \n",
    "        # Add padding tokens\n",
    "        # TODO: implement summary module\n",
    "        #       now self.config.sum_size default = 0\n",
    "        pad_size = segment_size - tensor.shape[0] - self.config.sum_size\n",
    "        if pad_size > 0:\n",
    "            if add_to == 'input_ids':\n",
    "                tensor = F.pad(tensor, (0, pad_size), value=self.pad_token_id)\n",
    "            elif add_to == 'attention_mask':\n",
    "                tensor = F.pad(tensor, (0, pad_size), value=0)\n",
    "            elif add_to == 'labels':\n",
    "                # for Seq2Seq labels need to be pad by -100\n",
    "                tensor = F.pad(tensor, (0, pad_size), value=-100)\n",
    "                pass\n",
    "        return tensor\n",
    "\n",
    "        # TODO: this implementation just add <s> and </s> to the input sequence\n",
    "        #       maybe need to add other special tokens\n",
    "    \n",
    "    def prepare_kwargs(self, segment_input_ids, kwargs):\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "        # [sample1_seg1, sample2_seg1, sample3_seg1,....] up to batch_size\n",
    "        # Some of the segments are None like: [sample1_seg3, None, sample3_seg3]\n",
    "        non_empty_mask = [s is not None for s in segment_input_ids]\n",
    "        \n",
    "        # all the segments are None, due to the max_n_segments >> the number of segments\n",
    "        if sum(non_empty_mask) == 0:\n",
    "            return None, non_empty_mask\n",
    "        \n",
    "        input_ids = torch.stack([s for s in segment_input_ids if s is not None])\n",
    "        # embedding layer\n",
    "        input_embeds = self.model.shared(input_ids)\n",
    "        \n",
    "        seg_kwargs['input_ids'] = None\n",
    "        seg_kwargs['inputs_embeds'] = input_embeds\n",
    "        if seg_kwargs.get('labels') is not None:\n",
    "            seg_kwargs['labels'] = seg_kwargs['labels'][non_empty_mask]\n",
    "        seg_kwargs['attention_mask'] = self.get_attention_mask(input_ids)\n",
    "    \n",
    "    def get_attention_mask(self, tensor):\n",
    "        mask = torch.ones_like(tensor)\n",
    "        mask[tensor == self.pad_token_id] = 0\n",
    "        return mask\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, Seq2SeqLMOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "        \"\"\" \n",
    "        \n",
    "        kwargs = {\n",
    "            'attention_mask': attention_mask, \n",
    "            # 'token_type_ids': token_type_ids,\n",
    "            # 'position_ids': position_ids, \n",
    "            'inputs_embeds': inputs_embeds,\n",
    "            'labels': labels, 'output_attentions': output_attentions,\n",
    "            'output_hidden_states': output_hidden_states, 'return_dict': return_dict,\n",
    "        }\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        if labels is not None:\n",
    "            if use_cache:\n",
    "                logger.warning(\"The `use_cache` argument is changed to `False` since `labels` is provided.\")\n",
    "            use_cache = False\n",
    "            if decoder_input_ids is None and decoder_inputs_embeds is None:\n",
    "                decoder_input_ids = shift_tokens_right(\n",
    "                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
    "                )\n",
    "                \n",
    "        # MODIFIED: add prefix encoder\n",
    "        batch_size = input_ids.shape[0]\n",
    "        past_key_values = self.get_prompt(batch_size)\n",
    "        prefix_attention_mask = torch.ones(batch_size, self.pre_seq_len)\n",
    "        attention_mask = torch.cat([prefix_attention_mask, attention_mask], dim=1)\n",
    "        \n",
    "        # segmented: [max_n_segments, batch_size, segment_size]\n",
    "        segmented = self.pad_and_segment(input_ids)\n",
    "        \n",
    "        # NOTE: why???\n",
    "        # if self.pre_seq_len == 0:\n",
    "        #     segmented = segmented[-1:]\n",
    "        \n",
    "        model_outputs = []\n",
    "        for seg_num, segment_input_ids in enumerate(segmented):\n",
    "            if self.config.bptt_depth != -1:\n",
    "                raise NotImplementedError\n",
    "\n",
    "    def generate(self):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    \n",
    "class BartPrefixPropForConditionalGeneration(BartForConditionalGeneration):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "\n",
    "# ============================================\n",
    "# MODIFIED from transformers.modeling_t5.py\n",
    "# ============================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.summarization import BartPrefixForConditionalGeneration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"pre_seq_len\": 20,\n",
    "    \"input_size\": 512,\n",
    "    \"segment_size\": 512,\n",
    "    \"max_n_segments\": 3,\n",
    "    \"bptt_depth\": 2,\n",
    "    \"prefix_projection\" : False,\n",
    "    \"hidden_dropout_prob\": 0.1,\n",
    "    \"hidden_size\": 1024\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_config(config, kwargs):\n",
    "    if config is not None:\n",
    "        config.pre_seq_len = kwargs['pre_seq_len']\n",
    "        config.segment_size = kwargs['segment_size']\n",
    "        config.input_size = kwargs['input_size']\n",
    "        config.max_n_segments = kwargs['max_n_segments']\n",
    "        config.bptt_depth = kwargs['bptt_depth']\n",
    "        config.prefix_projection = kwargs['prefix_projection']\n",
    "        config.hidden_dropout_prob = kwargs['hidden_dropout_prob']\n",
    "        config.hidden_size = kwargs['hidden_size']\n",
    "    else:\n",
    "        raise Exception('config is None!')\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'BartConfig' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m\"\u001b[39m\u001b[39mfacebook/bart-large-cnn\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m bart_config \u001b[39m=\u001b[39m set_config(bart_config, kwargs)\n\u001b[0;32m----> 5\u001b[0m bart_generator \u001b[39m=\u001b[39m BartPrefixForConditionalGeneration\u001b[39m.\u001b[39;49mfrom_pretrained(\n\u001b[1;32m      6\u001b[0m     \u001b[39m\"\u001b[39;49m\u001b[39mfacebook/bart-large-cnn\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      7\u001b[0m     config\u001b[39m=\u001b[39;49mbart_config,\n\u001b[1;32m      8\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m bart_generator, bart_config\n",
      "File \u001b[0;32m~/miniconda3/envs/summarization/lib/python3.10/site-packages/transformers/modeling_utils.py:2629\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2626\u001b[0m     init_contexts\u001b[39m.\u001b[39mappend(init_empty_weights())\n\u001b[1;32m   2628\u001b[0m \u001b[39mwith\u001b[39;00m ContextManagers(init_contexts):\n\u001b[0;32m-> 2629\u001b[0m     model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m(config, \u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m   2631\u001b[0m \u001b[39m# Check first if we are `from_pt`\u001b[39;00m\n\u001b[1;32m   2632\u001b[0m \u001b[39mif\u001b[39;00m use_keep_in_fp32_modules:\n",
      "File \u001b[0;32m~/private_room/proj-repos/prompt-for-long-text-summarization/model/summarization.py:47\u001b[0m, in \u001b[0;36mBartPrefixForConditionalGeneration.__init__\u001b[0;34m(self, config, tokenizer)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(config)\n\u001b[1;32m     41\u001b[0m \u001b[39m# self.model = BartModel(config)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39m# self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[39m# self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \n\u001b[1;32m     45\u001b[0m \u001b[39m# MODIFIED\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m# Start\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mset_params(\n\u001b[1;32m     48\u001b[0m     tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m     49\u001b[0m     config\u001b[39m=\u001b[39;49mconfig\n\u001b[1;32m     50\u001b[0m )\n\u001b[1;32m     51\u001b[0m \u001b[39m# self.config = config\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m# self.dropout = nn.Dropout(config.hidden_dropout_prob)\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39m# MODIFIED\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39m# Start\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[39mfor\u001b[39;00m param \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/private_room/proj-repos/prompt-for-long-text-summarization/model/summarization.py:216\u001b[0m, in \u001b[0;36mBartPrefixForConditionalGeneration.set_params\u001b[0;34m(self, tokenizer, config)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mextract_special_tokens(tokenizer)\n\u001b[1;32m    213\u001b[0m \u001b[39m# self.extend_word_embeddings(config['pre_seq_len'], tokenizer)\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \n\u001b[1;32m    215\u001b[0m \u001b[39m# tokenizer.num_special_tokens_to_add()cal the number of special tokens needed to add except [SEP]\u001b[39;00m\n\u001b[0;32m--> 216\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msegment_size \u001b[39m=\u001b[39m config[\u001b[39m'\u001b[39;49m\u001b[39minput_size\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_seq_len \u001b[39m-\u001b[39m tokenizer\u001b[39m.\u001b[39mnum_special_tokens_to_add()\n\u001b[1;32m    217\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39msep_token\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m tokenizer\u001b[39m.\u001b[39mspecial_tokens_map:\n\u001b[1;32m    218\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msegment_size \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'BartConfig' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "bart_config = AutoConfig.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "bart_config = set_config(bart_config, kwargs)\n",
    "bart_generator = BartPrefixForConditionalGeneration.from_pretrained(\n",
    "    \"facebook/bart-large-cnn\",\n",
    "    config=bart_config,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "bart_generator, bart_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total trainable parameters: 1\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "trainable_parameters = [param for param in bart_generator.parameters() if param.requires_grad]\n",
    "\n",
    "# 打印可训练参数的数量\n",
    "print(\"Total trainable parameters:\", len(trainable_parameters))\n",
    "\n",
    "# 打印每个可训练参数的名称\n",
    "for param in trainable_parameters:\n",
    "    print(param.data.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PrefixEncoder(\n",
       "  (embedding): Embedding(20, 24576)\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bart_generator.prefix_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, batch in enumerate(train_dataloader):\n",
    "    bart_generator(**batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
