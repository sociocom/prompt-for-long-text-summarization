{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n",
      "Aborted!\n"
     ]
    }
   ],
   "source": [
    "!wandb login\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:vw970or3) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: WARNING Source type is set to 'repo' but some required information is missing from the environment. A job will not be created from this run. See https://docs.wandb.ai/guides/launch/create-job\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e259c5475ee34e1fa7a9bdecc7283256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.003 MB of 0.013 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.227164…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>loss</td><td>█▄▅▁▂▅▄▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>loss</td><td>0.99713</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">rare-wind-1</strong> at: <a href='https://wandb.ai/kaifan-li/my_project/runs/vw970or3' target=\"_blank\">https://wandb.ai/kaifan-li/my_project/runs/vw970or3</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20230804_225122-vw970or3/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:vw970or3). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d07bc6da2c3d4e70a35c6eab7030ed93",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016668189813693366, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.8"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/is/kaifan-l/private_room/proj-repos/prompt-for-long-text-summarization/wandb/run-20230804_225935-7shln82z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/kaifan-li/my_project/runs/7shln82z' target=\"_blank\">swept-dawn-2</a></strong> to <a href='https://wandb.ai/kaifan-li/my_project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/kaifan-li/my_project' target=\"_blank\">https://wandb.ai/kaifan-li/my_project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/kaifan-li/my_project/runs/7shln82z' target=\"_blank\">https://wandb.ai/kaifan-li/my_project/runs/7shln82z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import wandb\n",
    "import os\n",
    "os.environ['WANDB_DIR'] = os.getcwd() + '/wandb/'\n",
    "os.environ['WANDB_CACHE_DIR'] = os.getcwd() + '/wandb/.cache/'\n",
    "os.environ['WANDB_CONFIG_DIR'] = os.getcwd() + '/wandb/.config/'\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 初始化wandb\n",
    "wandb.init(\n",
    "    entity='kaifan-li',\n",
    "    project=\"my_project\"\n",
    ")\n",
    "\n",
    "# 构建模型\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(10, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleModel()\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# 训练循环\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i in range(100):\n",
    "        inputs = torch.randn(32, 10)  # 随机生成输入数据\n",
    "        labels = torch.randn(32, 1)   # 随机生成标签\n",
    "        \n",
    "        # 正向传播\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # 记录训练过程和指标\n",
    "    avg_loss = running_loss / 100\n",
    "    wandb.log({\"epoch\": epoch, \"loss\": avg_loss})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_text = \"Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.Long documents, like research papers, scientific articles, or books, often contain a wealth of information and insights. These documents can cover complex topics, present detailed arguments, and provide extensive evidence to support their claims. However, handling such long texts can be challenging, especially when working with models that have input length limitations.n natural language processing tasks, like text classification or language generation, it is common to use pretrained transformer models like BERT, GPT-3, or T5. These models typically have a maximum input length of 512 tokens, which means they cannot directly handle documents that exceed this limit.To process long documents with pretrained models, a common approach is to split the text into smaller segments or chunks, process each segment independently with the model, and then combine the results. By dividing the document into smaller parts, each segment can fit within the model's input length constraints.However, this approach requires careful handling to ensure that the divisions do not disrupt the coherence and context of the document. Some strategies involve using sliding windows, adding special tokens to mark the beginning and end of segments, or using an overlap between segments to preserve context.Researchers and developers often implement custom solutions to tackle long document processing, depending on their specific use case and model requirements.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13014"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(long_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/is/kaifan-l/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import sys\n",
    "import threading\n",
    "\n",
    "import numpy as np\n",
    "import psutil\n",
    "import torch\n",
    "import random\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from datasets import load_dataset\n",
    "import evaluate\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, get_linear_schedule_with_warmup, set_seed\n",
    "\n",
    "from peft import PrefixTuningConfig, TaskType, get_peft_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accelerator = Accelerator() # device_placement=\"cuda:0\"\n",
    "model_name_or_path = \"facebook/bart-base\"\n",
    "dataset_name = \"cnn_dailymail\"\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    num_virtual_tokens=20,\n",
    ")\n",
    "text_column = 'article'\n",
    "label_column = 'highlights'\n",
    "lr = 3e-3\n",
    "num_epochs = 10\n",
    "batch_size = 32\n",
    "seed = 42\n",
    "do_test = True\n",
    "set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_dataset = load_dataset(dataset_name, \"3.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "target_max_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    inputs = examples[text_column]\n",
    "    targets = examples[label_column]\n",
    "    model_inputs = tokenizer(inputs, truncation=True) # 这里暂时不padding\n",
    "    targets = tokenizer(\n",
    "        targets,\n",
    "        max_length=target_max_length,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt',\n",
    "    )\n",
    "    targets = targets['input_ids']\n",
    "    targets[targets == tokenizer.pad_token_id] = -100\n",
    "    model_inputs['labels'] = targets\n",
    "    \n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset: 100%|██████████| 11490/11490 [00:11<00:00, 992.19 examples/s]\n"
     ]
    }
   ],
   "source": [
    "with accelerator.main_process_first():\n",
    "    cnn_dataset = cnn_dataset.map(\n",
    "        preprocess_function,\n",
    "        batched=True,\n",
    "        num_proc=1,\n",
    "        remove_columns=cnn_dataset[\"train\"].column_names,\n",
    "        load_from_cache_file=True,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "    )\n",
    "accelerator.wait_for_everyone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(cnn_dataset[\"train\"]) * 0.1)\n",
    "eval_size = int(len(cnn_dataset[\"validation\"]) * 0.01)\n",
    "test_size = int(len(cnn_dataset[\"test\"]) * 0.01)\n",
    "\n",
    "# 从打乱后的数据集中随机抽取指定数量的数据\n",
    "train_dataset = cnn_dataset[\"train\"].shuffle(seed=42).select(range(train_size))\n",
    "eval_dataset = cnn_dataset[\"validation\"].shuffle(seed=42).select(range(eval_size))\n",
    "test_dataset = cnn_dataset[\"test\"].shuffle(seed=42).select(range(test_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    return tokenizer.pad(examples, padding='longest', return_tensors='pt')\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    batch_size=batch_size,\n",
    "    pin_memory=True, # 将数据加载到固定的内存中，可以加速数据加载\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BartTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 1024])\n"
     ]
    }
   ],
   "source": [
    "for num, batch in enumerate(train_dataloader):\n",
    "    print(batch[\"input_ids\"].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import logging\n",
    "import copy\n",
    "import math\n",
    "from typing import List, Optional, Tuple, Union\n",
    "\n",
    "from transformers import (\n",
    "    BartForConditionalGeneration, \n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "from transformers import BartConfig\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "from model.prefix_encoder import PrefixEncoder\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "class CustomBartConfig(BartConfig):\n",
    "    def __init__(self,\n",
    "                 pre_seq_len=20,\n",
    "                 input_size=512,\n",
    "                 max_n_segments=2,\n",
    "                 bptt_depth=-1,\n",
    "                 prefix_projection=False, \n",
    "                 hidden_dropout_prob=0.1,\n",
    "                 segment_alignment='left',\n",
    "                 sum_token_size=0,\n",
    "                 label_max_size=142,\n",
    "                 sum_loss=True,\n",
    "                 **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.pre_seq_len = pre_seq_len\n",
    "        self.input_size = input_size\n",
    "        self.max_n_segments = max_n_segments\n",
    "        self.bptt_depth = bptt_depth\n",
    "        self.prefix_projection = prefix_projection # whether to use reparametrization trick\n",
    "        self.hidden_dropout_prob = hidden_dropout_prob # dropout for prefix encoder\n",
    "        self.segment_alignment = segment_alignment\n",
    "        self.sum_token_size = sum_token_size\n",
    "        self.label_max_size = label_max_size # the max size of labels\n",
    "        self.sum_loss = sum_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_config = BartConfig.from_pretrained('facebook/bart-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomBartConfig {\n",
       "  \"_name_or_path\": \"bart-base\",\n",
       "  \"activation_dropout\": 0.1,\n",
       "  \"activation_function\": \"gelu\",\n",
       "  \"add_bias_logits\": false,\n",
       "  \"add_final_layer_norm\": false,\n",
       "  \"architectures\": [\n",
       "    \"BartModel\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0.1,\n",
       "  \"bos_token_id\": 0,\n",
       "  \"bptt_depth\": -1,\n",
       "  \"classif_dropout\": 0.1,\n",
       "  \"classifier_dropout\": 0.0,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_attention_heads\": 12,\n",
       "  \"decoder_ffn_dim\": 3072,\n",
       "  \"decoder_layerdrop\": 0.0,\n",
       "  \"decoder_layers\": 6,\n",
       "  \"decoder_start_token_id\": 2,\n",
       "  \"dropout\": 0.1,\n",
       "  \"early_stopping\": true,\n",
       "  \"encoder_attention_heads\": 12,\n",
       "  \"encoder_ffn_dim\": 3072,\n",
       "  \"encoder_layerdrop\": 0.0,\n",
       "  \"encoder_layers\": 6,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"forced_bos_token_id\": 0,\n",
       "  \"forced_eos_token_id\": 2,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_dropout_prob\": 0.1,\n",
       "  \"id2label\": {\n",
       "    \"0\": \"LABEL_0\",\n",
       "    \"1\": \"LABEL_1\",\n",
       "    \"2\": \"LABEL_2\"\n",
       "  },\n",
       "  \"init_std\": 0.02,\n",
       "  \"input_size\": 512,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"label2id\": {\n",
       "    \"LABEL_0\": 0,\n",
       "    \"LABEL_1\": 1,\n",
       "    \"LABEL_2\": 2\n",
       "  },\n",
       "  \"label_max_size\": 142,\n",
       "  \"max_n_segments\": 2,\n",
       "  \"max_position_embeddings\": 1024,\n",
       "  \"model_type\": \"bart\",\n",
       "  \"no_repeat_ngram_size\": 3,\n",
       "  \"normalize_before\": false,\n",
       "  \"normalize_embedding\": true,\n",
       "  \"num_beams\": 4,\n",
       "  \"num_hidden_layers\": 6,\n",
       "  \"pad_token_id\": 1,\n",
       "  \"pre_seq_len\": 20,\n",
       "  \"prefix_projection\": false,\n",
       "  \"scale_embedding\": false,\n",
       "  \"segment_alignment\": \"left\",\n",
       "  \"sum_loss\": true,\n",
       "  \"sum_token_size\": 0,\n",
       "  \"task_specific_params\": {\n",
       "    \"summarization\": {\n",
       "      \"length_penalty\": 1.0,\n",
       "      \"max_length\": 128,\n",
       "      \"min_length\": 12,\n",
       "      \"num_beams\": 4\n",
       "    },\n",
       "    \"summarization_cnn\": {\n",
       "      \"length_penalty\": 2.0,\n",
       "      \"max_length\": 142,\n",
       "      \"min_length\": 56,\n",
       "      \"num_beams\": 4\n",
       "    },\n",
       "    \"summarization_xsum\": {\n",
       "      \"length_penalty\": 1.0,\n",
       "      \"max_length\": 62,\n",
       "      \"min_length\": 11,\n",
       "      \"num_beams\": 6\n",
       "    }\n",
       "  },\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.32.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50265\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "custom_config = CustomBartConfig(**bart_config.to_dict())\n",
    "custom_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartTokenizer\n",
    "tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "import logging\n",
    "import copy\n",
    "import math\n",
    "from typing import List, Optional, Tuple, Union, Iterable\n",
    "\n",
    "from transformers import (\n",
    "    BartPretrainedModel,\n",
    "    BartForConditionalGeneration, \n",
    "    T5ForConditionalGeneration,\n",
    ")\n",
    "from transformers import BartConfig, T5Config\n",
    "from transformers.modeling_outputs import Seq2SeqLMOutput\n",
    "\n",
    "from model.prefix_encoder import PrefixEncoder\n",
    "from peft import PrefixTuningConfig, TaskType, get_peft_model\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copied from transformers.modeling_bart.py\n",
    "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
    "    \"\"\"\n",
    "    Shift input ids one token to the right.\n",
    "    \"\"\"\n",
    "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
    "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
    "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
    "\n",
    "    if pad_token_id is None:\n",
    "        raise ValueError(\"self.model.config.pad_token_id has to be defined.\")\n",
    "    # replace possible -100 values in labels by `pad_token_id`\n",
    "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
    "\n",
    "    return shifted_input_ids\n",
    "# prefix-tuning/p-tuning v2 version\n",
    "class BartPrefixForConditionalGeneration(BartPretrainedModel):\n",
    "    def __init__(self, config, checkpoint, peft_config):\n",
    "        super().__init__(config)\n",
    "        # copied from BartForConditionalGeneration.__init__()\n",
    "        # self.model = BartModel(config)\n",
    "        # self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
    "        # self.lm_head = nn.Linear(config.d_model, self.model.shared.num_embeddings, bias=False)\n",
    "        # self.post_init() will not overwrite the pretrained parameters when using from_pretrained()\n",
    "        \n",
    "        bart_model = BartForConditionalGeneration.from_pretrained(checkpoint)\n",
    "        self.model = get_peft_model(bart_model, peft_config)\n",
    "        self.tokenizer = BartTokenizer.from_pretrained(checkpoint)\n",
    "        self.config = config\n",
    "\n",
    "        self.segment_alignment = config.segment_alignment\n",
    "        self.extract_special_tokens(tokenizer)\n",
    "        self.pre_seq_len = config.pre_seq_len\n",
    "        self.n_layer = config.num_hidden_layers\n",
    "        self.n_head = config.num_attention_heads\n",
    "        self.n_embd = config.hidden_size // config.num_attention_heads\n",
    "        # self.extend_word_embeddings(config.pre_seq_len, tokenizer)\n",
    "        \n",
    "        # tokenizer.num_special_tokens_to_add()cal the number of special tokens needed to add except [SEP]\n",
    "        # bart-base: 489\n",
    "        self.segment_size = config.input_size - self.pre_seq_len - tokenizer.num_special_tokens_to_add()\n",
    "        if 'sep_token' in tokenizer.special_tokens_map:\n",
    "            self.segment_size -= 1\n",
    "        \n",
    "        # TODO: forget some part of long range memory and add new memory\n",
    "\n",
    "        # for param in self.model.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        self.prefix_tokens = torch.arange(self.pre_seq_len).long()\n",
    "        self.prefix_encoder = PrefixEncoder(config)\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        \n",
    "        bart_param = 0\n",
    "        all_param = 0\n",
    "        \n",
    "        # count the number of trainable parameters in bart\n",
    "        for name, param in self.model.named_parameters():\n",
    "            bart_param += param.numel() # numel() returns the total number of elements in the input tensor\n",
    "            \n",
    "        for name, param in self.named_parameters():\n",
    "            all_param += param.numel()\n",
    "            \n",
    "        trainable_param = all_param - bart_param\n",
    "        \n",
    "        print(\"Total parameters: {:,}\".format(all_param))\n",
    "        print(\"Trainable parameters: {:,} {:,%}\".format((trainable_param), trainable_param/all_param))\n",
    "\n",
    "    def get_prompt(self, batch_size):\n",
    "        prefix_tokens = self.prefix_tokens.unsqueeze(0).expand(batch_size, -1).to(self.model.device)\n",
    "        past_key_values = self.prefix_encoder(prefix_tokens)\n",
    "        bsz, seqlen, _ = past_key_values.shape\n",
    "        past_key_values = past_key_values.view(\n",
    "            bsz,\n",
    "            seqlen,\n",
    "            self.n_layer * 2,\n",
    "            self.n_head,\n",
    "            self.n_embd\n",
    "        )        \n",
    "        past_key_values = self.dropout(past_key_values)\n",
    "        # (2,batch_size,n_head,seq_len,head_dim)\n",
    "        past_key_values = past_key_values.permute([2, 0, 3, 1, 4]).split(2)\n",
    "        return past_key_values\n",
    "    \n",
    "    # TODO：split labels other warys\n",
    "    # TODO: 25% -> 50% -> 75% -> 100% -> 100% -> 100% -> 100% -> 100% -> 100% -> 100%\n",
    "    def pad_and_segment(self, input_ids, attention_mask=None, labels=None):\n",
    "        \"\"\"\n",
    "        segment input_ids into segments\n",
    "        \n",
    "        input sample:\n",
    "        segmented_batch = [\n",
    "            [sample1_seg1, sample1_seg2, sample1_seg3],\n",
    "            [sample2_seg1, sample2_seg2],\n",
    "            [sample3_seg1, sample3_seg2, sample3_seg3, sample3_seg4]\n",
    "        ]\n",
    "                   \n",
    "        output sample:\n",
    "        segmented_batch = [\n",
    "            [sample1_seg1, sample2_seg1, sample3_seg1],\n",
    "            [sample1_seg2, sample2_seg2, sample3_seg2],\n",
    "            [sample1_seg3, None, sample3_seg3],\n",
    "            [None, None, sample3_seg4]\n",
    "        ]\n",
    "        \"\"\"\n",
    "        segmented_batch = []\n",
    "        segmented_batch_attention_masks = []\n",
    "        segmented_batch_labels = []\n",
    "        \n",
    "        if attention_mask is None:\n",
    "            attention_mask = [None] * input_ids.shape[0]\n",
    "        batch_attention_mask = attention_mask\n",
    "            \n",
    "        # inference mode\n",
    "        if labels is None:\n",
    "            labels = [None] * input_ids.shape[0]\n",
    "        batch_labels = labels\n",
    "        \n",
    "        # input_ids: [batch_size, seq_len]\n",
    "        for seq, attn_mask, label in zip(input_ids, batch_attention_mask, batch_labels):\n",
    "\n",
    "            # pytorch syntax: element-wise operation\n",
    "            drop_mask = sum([seq == t for t in self.special_token_ids])\n",
    "            drop_mask = torch.tensor([1 if t != 0 else 0 for t in drop_mask])\n",
    "\n",
    "            # bool type slice for tensor type\n",
    "            # remove special tokens\n",
    "            seq = seq[(1 - drop_mask).bool()]\n",
    "            seq = seq[:self.segment_size * self.config.max_n_segments]\n",
    "            \n",
    "            if attn_mask is not None:\n",
    "                attn_mask_drop_mask = sum([attn_mask == self.pad_token_id])\n",
    "                attn_mask = attn_mask[attn_mask_drop_mask.bool()]\n",
    "                attn_mask = attn_mask[:self.segment_size * self.config.max_n_segments]\n",
    "            if label is not None:\n",
    "                label_drop_mask = sum([label == t for t in self.special_token_ids + [-100]])\n",
    "                label_drop_mask = torch.tensor([1 if t != 0 else 0 for t in label_drop_mask])\n",
    "                label = label[(1-label_drop_mask).bool()]\n",
    "                # TODO：label = label[:self.config.sum_max_size * self.config.max_n_segments]\n",
    "                label = label[:self.segment_size * self.config.max_n_segments]\n",
    "            \n",
    "            align = self.segment_alignment\n",
    "            if align in {'right', None}:\n",
    "                split_inds = (list(range(len(seq), 0, -self.segment_size)) + [0])[::-1]\n",
    "            elif align == 'left':\n",
    "                split_inds = list(range(0, len(seq), self.segment_size)) + [len(seq)]\n",
    "            elif align == 'center':\n",
    "                n_seg = math.ceil(len(seq) / self.segment_size)\n",
    "                split_inds = list(range(0, len(seq), math.ceil(len(seq) / n_seg))) + [len(seq)]\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "            input_segments = [seq[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "            input_segments = [self.pad_add_special_tokens(t, self.config.input_size) for t in input_segments]\n",
    "            \n",
    "            # add empty segment markers if needed\n",
    "            n_empty_segments = self.config.max_n_segments - len(input_segments)\n",
    "            # input_segments:\n",
    "            # print(\"input_segments:\", len(input_segments))\n",
    "            input_segments = input_segments + [self.get_full_padding_segment()] * n_empty_segments\n",
    "            \n",
    "            # segmented_batch: \n",
    "            segmented_batch.append(input_segments)\n",
    "\n",
    "            if attn_mask is not None:\n",
    "                attn_mask_segments = [attn_mask[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "                attn_mask_segments = [self.pad_add_special_tokens(t, self.config.input_size, add_to='attention_mask') for t in attn_mask_segments]\n",
    "                attn_mask_segments = attn_mask_segments + [self.get_full_padding_segment()] * n_empty_segments\n",
    "                segmented_batch_attention_masks.append(attn_mask_segments)\n",
    "            \n",
    "            # TODO: labels need to be segmented by other rules\n",
    "            if label is not None:\n",
    "                print(\"label:\", len(label))\n",
    "                end_index = math.ceil(len(label) // (full_segment_size := len(input_segments)))\n",
    "                # # for i in range(full_segment_size):\n",
    "                # #     end = (i+1) * end_index\n",
    "                # #     labels_segments.append(label[:end])\n",
    "                labels_segments = [label[:(end_index*(i+1))] for i in range(full_segment_size)]\n",
    "                for i in range(len(labels_segments)):\n",
    "                    print(\"labels_segments:\", i, len(labels_segments[i]))\n",
    "            # TODO: labels need to be segmented by other rules\n",
    "\n",
    "                # full_segment_size = len(input_segments)\n",
    "                # labels_segments = [label[start:end] for (start, end) in zip(split_inds, split_inds[1:])]\n",
    "                labels_segments = [self.pad_add_special_tokens(t, self.config.input_size, add_to='labels') for t in labels_segments]\n",
    "                labels_segments = labels_segments + [self.get_full_padding_segment()] * n_empty_segments\n",
    "                segmented_batch_labels.append(labels_segments)\n",
    "                for i in range(len(labels_segments)):\n",
    "                    print(\"labels_segments:\", i, len(labels_segments[i]))\n",
    "                \n",
    "        segmented_batch = [[sample[seg_num] for sample in segmented_batch] \n",
    "                            for seg_num in range(self.config.max_n_segments)]\n",
    "        segmented_batch_attention_masks = [[sample[seg_num] for sample in segmented_batch_attention_masks]\n",
    "                                           for seg_num in range(self.config.max_n_segments)]\n",
    "        segmented_batch_labels = [[sample[seg_num] for sample in segmented_batch_labels]\n",
    "                                  for seg_num in range(self.config.max_n_segments)]\n",
    "\n",
    "        return segmented_batch, segmented_batch_attention_masks, segmented_batch_labels\n",
    "        \n",
    "    def extract_special_tokens(self, tokenizer):\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.special_token_ids = [tokenizer.pad_token_id]\n",
    "        for token in ['cls_token', 'sep_token', 'eos_token', 'bos_token']:\n",
    "            token_id = getattr(tokenizer, f'{token}_id')\n",
    "            if token_id is not None:\n",
    "                self.register_buffer(token, torch.tensor([token_id]))\n",
    "                self.special_token_ids.append(token_id)\n",
    "            else:\n",
    "                setattr(self, token, None)\n",
    "                \n",
    "    # def extend_word_embeddings(self, tokenizer):\n",
    "    #     vocab_size = self.model.config.vocab_size\n",
    "    #     # NOTE: Really necessary???\n",
    "    #     extended_vocab_size = vocab_size + self.config.pre_seq_len\n",
    "    #     self.pre_seq_len = self.config.pre_seq_len\n",
    "    \n",
    "    def get_full_padding_segment(self,):\n",
    "        padding_segment = torch.tensor([self.pad_token_id for _ in range(self.config.input_size)])\n",
    "        return padding_segment\n",
    "    \n",
    "    # Memory mechanism like RNN\n",
    "    def forget_and_memory(self,):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    #  prefix-tuning don't need to concat prefix and input sequence\n",
    "    def pad_add_special_tokens(self, tensor, segment_size, \n",
    "                               prompts=None, prompt_attention_mask=None, # maybe better to use pre_seq_len and generate prompts attention mask?\n",
    "                               add_to='input_ids'):\n",
    "        \"\"\"\n",
    "        bart tokenizer:\n",
    "        {'bos_token': '<s>', 0\n",
    "         'eos_token': '</s>', 2\n",
    "         'unk_token': '<unk>', 3\n",
    "         'sep_token': '</s>', 0\n",
    "         'pad_token': '<pad>', 1\n",
    "         'cls_token': '<s>', 0\n",
    "         'mask_token': '<mask>' 50264\n",
    "        }\n",
    "        \"\"\"\n",
    "        input_elements = []\n",
    "        # Add special tokens: <s> and </s> to the input sequence\n",
    "        # For prefix-prop\n",
    "        if prompts is not None:\n",
    "            if add_to == 'inputs':\n",
    "                input_elements += [self.cls_token, prompts, self.sep_token, tensor, self.sep_token]\n",
    "            # For Bart, only the pad token is 0 in attention_mask\n",
    "            elif add_to == 'attention_mask':\n",
    "                mask_value = torch.ones((1), device=tensor.device)\n",
    "                input_elements += [mask_value, prompt_attention_mask, mask_value, tensor, mask_value]\n",
    "            # As a encoder-decoder model：is not needed to add prompt to labels\n",
    "            elif add_to == 'labels':\n",
    "                input_elements += [self.eos_token, tensor, self.sep_token]\n",
    "        # For prefix-tuning/p-tuning v2\n",
    "        else:\n",
    "            if add_to == 'input_ids':\n",
    "                input_elements += [self.sep_token, tensor, self.sep_token]\n",
    "            elif add_to == 'attention_mask':\n",
    "                mask_value = torch.ones((1), device=tensor.device)\n",
    "                input_elements += [mask_value, tensor, mask_value]\n",
    "            elif add_to == 'labels':\n",
    "                input_elements += [self.eos_token, tensor, self.sep_token]\n",
    "        tensor = torch.cat(input_elements)\n",
    "\n",
    "        # Add padding tokens\n",
    "        # TODO: implement summary module\n",
    "        #       now self.config.sum_size default = 0\n",
    "        pad_size = segment_size - tensor.shape[0] - self.config.sum_token_size\n",
    "        if pad_size > 0:\n",
    "            if add_to == 'input_ids':\n",
    "                tensor = F.pad(tensor, (0, pad_size), value=self.pad_token_id)\n",
    "            elif add_to == 'attention_mask':\n",
    "                tensor = F.pad(tensor, (0, pad_size), value=0)\n",
    "            elif add_to == 'labels':\n",
    "                # for Seq2Seq labels need to be pad by -100\n",
    "                tensor = F.pad(tensor, (0, pad_size), value=-100)\n",
    "        return tensor\n",
    "\n",
    "        # TODO: this implementation just add <s> and </s> to the input sequence\n",
    "        #       maybe need to add other special tokens\n",
    "    \n",
    "    def prepare_kwargs(self, segment, kwargs):\n",
    "        segment_input_ids, segment_attention_mask, segment_label = segment\n",
    "        seg_kwargs = dict(**kwargs)\n",
    "        \n",
    "        # [sample1_seg1, sample2_seg1, sample3_seg1,....] up to batch_size\n",
    "        # Some of the segments are None like: [sample1_seg3, None, sample3_seg3]\n",
    "        non_empty_mask = [s is not None for s in segment_input_ids]\n",
    "        print(\"non_empty_mask:\", non_empty_mask)\n",
    "        # all the segments are None, due to the max_n_segments >> the number of segments        \n",
    "        if sum(non_empty_mask) == 0:\n",
    "            return None, non_empty_mask\n",
    "        \n",
    "        # convert list to tensor\n",
    "        # print(\"segment_input_ids:\", segment_input_ids)\n",
    "\n",
    "        input_ids = torch.stack([s for s in segment_input_ids if s is not None])\n",
    "        # print(\"input_ids:\", input_ids.shape)\n",
    "        # input_embeds = self.model.embeddings(input_ids)\n",
    "\n",
    "        seg_kwargs['input_ids'] = input_ids\n",
    "    \n",
    "        # seg_kwargs['inputs_embeds'] = input_embeds\n",
    "        \n",
    "        # if seg_kwargs.get('token_type_ids') is not None:\n",
    "        #     seg_kwargs['token_type_ids'] = self.get_token_type_ids(input_ids)\n",
    "\n",
    "        # seg_kwargs['decoder_input_ids'] = torch.stack([el for el, m in zip(segment_label, non_empty_mask) if m])\n",
    "        # seg_kwargs['decoder_input_ids'] = seg_kwargs['decoder_input_ids'][non_empty_mask]\n",
    "        # print(\"decoder_input_ids:\", seg_kwargs['decoder_input_ids'].shape)\n",
    "        # if seg_kwargs['labels_mask'] is not None:\n",
    "        # seg_kwargs['labels_mask'] = torch.stack([el for el, m in zip(segment_labels_mask, non_empty_mask) if m])\n",
    "        # if seg_kwargs.get('token_type_ids') is not None:\n",
    "        #     seg_kwargs['token_type_ids'] = self.get_token_type_ids(input_ids)\n",
    "        # seg_kwargs['output_hidden_states'] = True\n",
    "        \n",
    "        # generate prompts\n",
    "        batch_size = seg_kwargs['input_ids'].shape[0]\n",
    "        # print('batch_size:', batch_size)\n",
    "        past_key_values = self.get_prompt(batch_size)\n",
    "        prefix_attention_mask = torch.ones(batch_size, self.pre_seq_len)\n",
    "        \n",
    "        if seg_kwargs['labels'] is not None:\n",
    "            seg_kwargs['labels'] = torch.stack([el for el, m in zip(segment_label, non_empty_mask) if m])\n",
    "        \n",
    "        # print(\"labels:\", seg_kwargs['labels'].shape, seg_kwargs['labels'])\n",
    "        # attn_mask = torch.cat([prefix_attention_mask, attention_mask], dim=1)\n",
    "        # seg_kwargs['past_key_values'] = past_key_values\n",
    "        if seg_kwargs['attention_mask'] is not None:\n",
    "            seg_kwargs['attention_mask'] = self.get_attention_mask(input_ids)\n",
    "        # seg_kwargs['attention_mask'] = attn_mask\n",
    "        return seg_kwargs, non_empty_mask\n",
    "        \n",
    "    def get_attention_mask(self, tensor):\n",
    "        mask = torch.ones_like(tensor)\n",
    "        mask[tensor == self.pad_token_id] = 0\n",
    "        return mask\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_input_ids: Optional[torch.LongTensor] = None,\n",
    "        decoder_attention_mask: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        decoder_head_mask: Optional[torch.Tensor] = None,\n",
    "        cross_attn_head_mask: Optional[torch.Tensor] = None,\n",
    "        encoder_outputs: Optional[List[torch.FloatTensor]] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = True,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, Seq2SeqLMOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n",
    "            Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n",
    "            config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n",
    "            (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n",
    "\n",
    "        Returns:\n",
    "        \"\"\" \n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        kwargs = {\n",
    "            'attention_mask': attention_mask, \n",
    "            # 'token_type_ids': token_type_ids,\n",
    "            # 'position_ids': position_ids, \n",
    "            'inputs_embeds': inputs_embeds,\n",
    "            'labels': labels,\n",
    "            'output_attentions': output_attentions,\n",
    "            'output_hidden_states': output_hidden_states, \n",
    "            'return_dict': return_dict,\n",
    "        }\n",
    "        # segmented: [max_n_segments, batch_size, segment_size]\n",
    "        # !!! Note: the batch_size is not the same as the input batch_size\n",
    "        segmented = self.pad_and_segment(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels,\n",
    "        )\n",
    "\n",
    "        # NOTE: why???\n",
    "        # if self.pre_seq_len == 0:\n",
    "        #     segmented = segmented[-1:]\n",
    "        \n",
    "        model_outputs = []\n",
    "        for seg_num, segment in enumerate(zip(*segmented)):\n",
    "            print(\"seg_num:\", seg_num)\n",
    "            \n",
    "            in_ids, attn_mask, l = segment\n",
    "            print(\"batch_size:\", len(in_ids))\n",
    "            # TODO: can't control the number of gradient accumulation steps now\n",
    "            if self.config.bptt_depth != -1:\n",
    "                raise NotImplementedError\n",
    "            \n",
    "            seg_kwargs, non_empty_mask = self.prepare_kwargs(segment, kwargs)\n",
    "            # print(\"in_ids|attn_mask|l:\", seg_kwargs['input_ids'].shape, seg_kwargs['attention_mask'].shape, seg_kwargs['decoder_input_ids'].shape)\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "\n",
    "            out = self.model(**seg_kwargs)\n",
    "            print('decoder_input_ids:', decoder_input_ids)\n",
    "            # out = self.model(\n",
    "            #     input_ids=seg_kwargs['input_ids'],\n",
    "            #     attention_mask=seg_kwargs['attention_mask'],\n",
    "            #     # TODO: 只能concat attention到decoder attention mask\n",
    "            #     # past_key_values=seg_kwargs['past_key_values'],\n",
    "            #     # TODO: decoder_attention_mask: https://github.com/huggingface/transformers/issues/25271\n",
    "            #     decoder_input_ids=decoder_input_ids\n",
    "            # )\n",
    "            # self.prefix_tokens = out.encoder_last_hidden_state[-1][:, 1:self.pre_seq_len+1]\n",
    "            # self.prefix_tokens = out.last_hidden_state[:, :self.pre_seq_len]\n",
    "            out['seg_kwargs'] = seg_kwargs\n",
    "            model_outputs.append(out)\n",
    "            print('past_key_values:', out.past_key_values)\n",
    "            print('out:', out)\n",
    "        out = self.process_outputs(model_outputs, output_attentions, output_hidden_states)\n",
    "        print('model is finished')\n",
    "        return out\n",
    "            \n",
    "    @torch.no_grad()\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        max_length: Optional[int] = None,\n",
    "        min_length: Optional[int] = None,\n",
    "        do_sample: Optional[bool] = None,\n",
    "        early_stopping: Optional[bool] = None,\n",
    "        num_beams: Optional[int] = None,\n",
    "        temperature: Optional[float] = None,\n",
    "        top_k: Optional[int] = None,\n",
    "        top_p: Optional[float] = None,\n",
    "        repetition_penalty: Optional[float] = None,\n",
    "        bad_words_ids: Optional[Iterable[int]] = None,\n",
    "        bos_token_id: Optional[int] = None,\n",
    "        pad_token_id: Optional[int] = None,\n",
    "        eos_token_id: Optional[int] = None,\n",
    "        length_penalty: Optional[float] = None,\n",
    "        no_repeat_ngram_size: Optional[int] = None,\n",
    "        num_return_sequences: Optional[int] = None,\n",
    "        attention_mask: Optional[torch.LongTensor] = None,\n",
    "        decoder_start_token_id: Optional[int] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        **model_specific_kwargs\n",
    "    ) -> torch.LongTensor:\n",
    "\n",
    "\n",
    "        kwargs = {\n",
    "            'input_ids': input_ids,\n",
    "            'num_beams': num_beams,\n",
    "            'min_length': min_length,\n",
    "            'max_length': max_length,\n",
    "            'labels': None,\n",
    "            'attention_mask': None\n",
    "        }\n",
    "        \n",
    "        segmented = self.pad_and_segment(\n",
    "            input_ids=input_ids,\n",
    "        )\n",
    "        \n",
    "        model_outputs = []\n",
    "        for seg_num, segment in enumerate(zip(*segmented)):\n",
    "            in_ids, attn_mask, l = segment\n",
    "            \n",
    "            if self.config.bptt_depth != -1:\n",
    "                raise NotImplementedError\n",
    "            \n",
    "            seg_kwargs, non_empty_mask = self.prepare_kwargs(segment, kwargs)\n",
    "            if sum(non_empty_mask) == 0:\n",
    "                continue\n",
    "            \n",
    "            out = self.model.generate(**seg_kwargs)\n",
    "            \n",
    "            model_outputs.append(out)\n",
    "            print('out:', out)\n",
    "        print(\"model_outputs: \", self.tokenizer.decode(model_outputs[-1][-1], skip_special_tokens=True))\n",
    "        \n",
    "    def process_outputs(self, input_ids, model_outputs, output_attentions, output_hidden_states):\n",
    "        out = model_outputs[-1] # get the last segment output\n",
    "        \n",
    "        bs, seq_len = input_ids.shape\n",
    "        \n",
    "        losses = []\n",
    "        logits = []\n",
    "        labels_segm = []\n",
    "        \n",
    "        for out in model_outputs:\n",
    "            losses.append(out['loss'])\n",
    "            logits.append(out['logits'].detach())\n",
    "            labels_segm += [out['seg_kwargs']['labels']]\n",
    "        \n",
    "        if not output_hidden_states:\n",
    "            for key in out.keys():\n",
    "                if 'hidden_state' in key:\n",
    "                    out[key] = None\n",
    "                    \n",
    "        for i, l in enumerate(losses):\n",
    "            out[f'loss_{i}'] = l.mean()\n",
    "            \n",
    "        out['loss'] = torch.stack(losses).mean()\n",
    "        \n",
    "        for i in range(len(logits)):\n",
    "            logits[i] = F.pad(logits[i], (0, 0, 0, 0, 0, bs - logits[i].shape[0]))\n",
    "            labels_segm[i] = F.pad(labels_segm[i], (0, 0, 0, bs - labels_segm[i].shape[0]), value=-100)\n",
    "        \n",
    "        out['logits'] = torch.cat(logits, dim=1)\n",
    "        # Warning: rmt logits, labels, masks are not in the same order as in input data:\n",
    "        # the first dimension is number of segments!\n",
    "        # so, torch.cat will result in segm0, segm0,.. and only after all segm0 will come segm1, ... .\n",
    "        # not segm0, segm1, segm0, segm1 as in input data\n",
    "        out['logits_segm'] = [logits]\n",
    "        out['labels_segm'] = [labels_segm]\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seg = math.ceil(10 / 4)\n",
    "seg \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 139,789,056\n",
      "Trainable parameters: 184,320 0.131856%\n",
      "trainable params: 184,320 || all params: 139,604,736 || trainable%: 0.13202990477343118\n"
     ]
    }
   ],
   "source": [
    "# from model.summarization import BartPrefixForConditionalGeneration\n",
    "checkpoint = 'facebook/bart-base'\n",
    "peft_config = PrefixTuningConfig(\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    inference_mode=False,\n",
    "    num_virtual_tokens=20,\n",
    ")\n",
    "model = BartPrefixForConditionalGeneration(    \n",
    "    checkpoint=checkpoint,\n",
    "    config=custom_config,\n",
    "    peft_config=peft_config\n",
    ")\n",
    "model.model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids.shape: torch.Size([32, 1024])\n",
      "attention_mask.shape: torch.Size([32, 1024])\n",
      "labels.shape: torch.Size([32, 128])\n",
      "label: 56\n",
      "labels_segments: 0 28\n",
      "labels_segments: 1 56\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 36\n",
      "labels_segments: 0 18\n",
      "labels_segments: 1 36\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 72\n",
      "labels_segments: 0 36\n",
      "labels_segments: 1 72\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 14\n",
      "labels_segments: 0 7\n",
      "labels_segments: 1 14\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 66\n",
      "labels_segments: 0 33\n",
      "labels_segments: 1 66\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 28\n",
      "labels_segments: 0 14\n",
      "labels_segments: 1 28\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 36\n",
      "labels_segments: 0 18\n",
      "labels_segments: 1 36\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 93\n",
      "labels_segments: 0 46\n",
      "labels_segments: 1 92\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 70\n",
      "labels_segments: 0 35\n",
      "labels_segments: 1 70\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 78\n",
      "labels_segments: 0 39\n",
      "labels_segments: 1 78\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 56\n",
      "labels_segments: 0 28\n",
      "labels_segments: 1 56\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 106\n",
      "labels_segments: 0 53\n",
      "labels_segments: 1 106\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 71\n",
      "labels_segments: 0 35\n",
      "labels_segments: 1 70\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 63\n",
      "labels_segments: 0 31\n",
      "labels_segments: 1 62\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "labels_segments: 2 512\n",
      "label: 93\n",
      "labels_segments: 0 46\n",
      "labels_segments: 1 92\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 43\n",
      "labels_segments: 0 21\n",
      "labels_segments: 1 42\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 90\n",
      "labels_segments: 0 45\n",
      "labels_segments: 1 90\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "labels_segments: 2 512\n",
      "label: 77\n",
      "labels_segments: 0 38\n",
      "labels_segments: 1 76\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 78\n",
      "labels_segments: 0 39\n",
      "labels_segments: 1 78\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 47\n",
      "labels_segments: 0 23\n",
      "labels_segments: 1 46\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "labels_segments: 2 512\n",
      "label: 106\n",
      "labels_segments: 0 53\n",
      "labels_segments: 1 106\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 83\n",
      "labels_segments: 0 41\n",
      "labels_segments: 1 82\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 33\n",
      "labels_segments: 0 16\n",
      "labels_segments: 1 32\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 44\n",
      "labels_segments: 0 22\n",
      "labels_segments: 1 44\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 66\n",
      "labels_segments: 0 33\n",
      "labels_segments: 1 66\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 77\n",
      "labels_segments: 0 38\n",
      "labels_segments: 1 76\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 51\n",
      "labels_segments: 0 25\n",
      "labels_segments: 1 50\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 126\n",
      "labels_segments: 0 63\n",
      "labels_segments: 1 126\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 48\n",
      "labels_segments: 0 24\n",
      "labels_segments: 1 48\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 126\n",
      "labels_segments: 0 63\n",
      "labels_segments: 1 126\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 83\n",
      "labels_segments: 0 41\n",
      "labels_segments: 1 82\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "label: 126\n",
      "labels_segments: 0 63\n",
      "labels_segments: 1 126\n",
      "labels_segments: 0 512\n",
      "labels_segments: 1 512\n",
      "seg_num: 0\n",
      "batch_size: 32\n",
      "non_empty_mask: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "decoder_input_ids: None\n",
      "past_key_values: None\n",
      "out: Seq2SeqLMOutput(loss=tensor(9.1501, grad_fn=<NllLossBackward0>), logits=tensor([[[-4.5796, -4.3795,  3.4955,  ..., -3.5561, -4.2382, -0.5397],\n",
      "         [-4.1515, -4.1827,  3.7551,  ..., -3.5827, -4.0878, -0.1824],\n",
      "         [-3.7802, -4.2243,  1.7686,  ..., -3.8953, -4.3906, -0.7383],\n",
      "         ...,\n",
      "         [-5.3507, -3.5794,  2.2576,  ..., -3.9726, -3.8343, -1.3937],\n",
      "         [-5.5923, -3.5542,  2.1144,  ..., -3.9905, -3.8378, -0.9187],\n",
      "         [-5.5928, -3.6473,  1.7244,  ..., -3.9700, -3.9115, -0.9518]],\n",
      "\n",
      "        [[-3.4713, -4.6960,  3.3924,  ..., -4.0632, -4.6293,  0.1065],\n",
      "         [-3.7282, -4.4514,  3.3747,  ..., -3.9116, -4.3722,  0.1142],\n",
      "         [-3.3651, -2.9545,  3.9748,  ..., -2.0851, -2.5665,  0.1401],\n",
      "         ...,\n",
      "         [-7.1142, -4.1007,  1.0114,  ..., -4.7105, -4.4117, -1.2219],\n",
      "         [-7.5081, -4.1152,  0.7717,  ..., -4.8028, -4.5010, -0.8964],\n",
      "         [-7.0036, -4.2378,  0.6018,  ..., -4.8349, -4.6534, -0.8909]],\n",
      "\n",
      "        [[-2.0597, -4.4456,  4.1441,  ..., -4.2677, -4.8033,  0.1256],\n",
      "         [-2.1891, -3.9667,  4.2284,  ..., -3.6770, -4.2052,  1.1698],\n",
      "         [-3.4055, -3.8559,  1.0341,  ..., -3.0321, -3.2082,  1.7280],\n",
      "         ...,\n",
      "         [-5.9897, -3.8279,  2.4643,  ..., -4.2127, -4.1563, -1.3956],\n",
      "         [-6.3539, -3.8406,  2.2817,  ..., -4.2400, -4.2168, -1.2565],\n",
      "         [-6.2332, -3.9660,  2.0085,  ..., -4.2961, -4.3419, -1.3286]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.6849, -4.6541,  3.7121,  ..., -3.7883, -4.4064,  0.5643],\n",
      "         [-5.7036, -4.3998,  3.2752,  ..., -3.6841, -4.0440,  1.2672],\n",
      "         [-3.3316, -2.8908,  1.9476,  ..., -0.9695, -1.8155,  0.0492],\n",
      "         ...,\n",
      "         [-6.4392, -3.8964,  2.6030,  ..., -4.2515, -4.2030, -0.8088],\n",
      "         [-6.8670, -3.8460,  2.3414,  ..., -4.2609, -4.2610, -0.6142],\n",
      "         [-6.7216, -4.0053,  2.1898,  ..., -4.3910, -4.4413, -0.8460]],\n",
      "\n",
      "        [[-2.5675, -4.6756,  3.8477,  ..., -4.8572, -5.1279, -0.5956],\n",
      "         [-3.2372, -4.5085,  3.3906,  ..., -4.5705, -4.7842, -0.7985],\n",
      "         [-6.5419, -4.6892,  0.6762,  ..., -4.4295, -4.6912, -2.2204],\n",
      "         ...,\n",
      "         [-5.9449, -3.9351,  2.0122,  ..., -4.5319, -4.3425, -1.7520],\n",
      "         [-6.2458, -3.9204,  1.7982,  ..., -4.5760, -4.3147, -1.4327],\n",
      "         [-6.2199, -4.1220,  1.6168,  ..., -4.7300, -4.5285, -1.4964]],\n",
      "\n",
      "        [[-1.6395, -4.5090,  4.2929,  ..., -4.4529, -5.0436, -0.6155],\n",
      "         [-3.2896, -4.4695,  3.5915,  ..., -4.2615, -4.8241, -0.6717],\n",
      "         [-1.9293, -4.1349,  2.3399,  ..., -4.1027, -4.5640, -1.7417],\n",
      "         ...,\n",
      "         [-7.8161, -4.3405,  1.2057,  ..., -4.8667, -4.8925, -2.0301],\n",
      "         [-8.2631, -4.3293,  1.2349,  ..., -4.9107, -4.9897, -1.8620],\n",
      "         [-8.5166, -4.5569,  1.0437,  ..., -5.0781, -5.2347, -1.9074]]],\n",
      "       grad_fn=<AddBackward0>), past_key_values=None, decoder_hidden_states=(tensor([[[ 5.4487e-01, -2.0410e-01,  1.1865e-01,  ...,  2.6924e-01,\n",
      "          -1.1948e-01,  1.9697e-01],\n",
      "         [ 5.9953e-01, -1.8683e-01,  1.0440e-01,  ...,  1.6923e-01,\n",
      "          -2.8865e-03,  2.1166e-01],\n",
      "         [-2.7799e-01,  2.8828e-01, -7.3028e-01,  ..., -1.1586e+00,\n",
      "           1.8877e-01, -2.9742e-02],\n",
      "         ...,\n",
      "         [ 3.5735e-01,  1.4292e-01,  4.5212e-02,  ..., -9.3843e-02,\n",
      "          -2.7352e-02, -2.3044e-01],\n",
      "         [ 1.4780e-01, -4.1068e-02,  6.6296e-02,  ...,  1.4331e-01,\n",
      "          -3.3200e-01, -3.0151e-01],\n",
      "         [ 2.8179e-01, -2.3902e-01,  3.1419e-03,  ...,  2.1478e-01,\n",
      "          -6.5562e-01, -2.9937e-01]],\n",
      "\n",
      "        [[ 5.4487e-01, -2.0410e-01,  1.1865e-01,  ...,  2.6924e-01,\n",
      "          -1.1948e-01,  1.9697e-01],\n",
      "         [ 5.9953e-01, -1.8683e-01,  1.0440e-01,  ...,  1.6923e-01,\n",
      "          -2.8865e-03,  2.1166e-01],\n",
      "         [-4.1811e-02, -2.1991e-01,  3.6687e-01,  ...,  4.9226e-01,\n",
      "           8.5032e-01, -1.1033e-01],\n",
      "         ...,\n",
      "         [ 3.5735e-01,  1.4292e-01,  4.5212e-02,  ..., -9.3843e-02,\n",
      "          -2.7352e-02, -2.3044e-01],\n",
      "         [ 1.4780e-01, -4.1068e-02,  6.6296e-02,  ...,  1.4331e-01,\n",
      "          -3.3200e-01, -3.0151e-01],\n",
      "         [ 2.8179e-01, -2.3902e-01,  3.1419e-03,  ...,  2.1478e-01,\n",
      "          -6.5562e-01, -2.9937e-01]],\n",
      "\n",
      "        [[ 5.4487e-01, -2.0410e-01,  1.1865e-01,  ...,  2.6924e-01,\n",
      "          -1.1948e-01,  1.9697e-01],\n",
      "         [ 5.9953e-01, -1.8683e-01,  1.0440e-01,  ...,  1.6923e-01,\n",
      "          -2.8865e-03,  2.1166e-01],\n",
      "         [ 7.9545e-01, -1.5292e-01,  4.2505e-02,  ...,  1.6360e-01,\n",
      "          -4.8583e-01,  7.2543e-02],\n",
      "         ...,\n",
      "         [ 3.5735e-01,  1.4292e-01,  4.5212e-02,  ..., -9.3843e-02,\n",
      "          -2.7352e-02, -2.3044e-01],\n",
      "         [ 1.4780e-01, -4.1068e-02,  6.6296e-02,  ...,  1.4331e-01,\n",
      "          -3.3200e-01, -3.0151e-01],\n",
      "         [ 2.8179e-01, -2.3902e-01,  3.1419e-03,  ...,  2.1478e-01,\n",
      "          -6.5562e-01, -2.9937e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 5.4487e-01, -2.0410e-01,  1.1865e-01,  ...,  2.6924e-01,\n",
      "          -1.1948e-01,  1.9697e-01],\n",
      "         [ 5.9953e-01, -1.8683e-01,  1.0440e-01,  ...,  1.6923e-01,\n",
      "          -2.8865e-03,  2.1166e-01],\n",
      "         [ 3.0398e-01,  2.7637e-01, -3.5502e-01,  ...,  1.8763e-01,\n",
      "          -7.9256e-04, -2.0805e-01],\n",
      "         ...,\n",
      "         [ 3.5735e-01,  1.4292e-01,  4.5212e-02,  ..., -9.3843e-02,\n",
      "          -2.7352e-02, -2.3044e-01],\n",
      "         [ 1.4780e-01, -4.1068e-02,  6.6296e-02,  ...,  1.4331e-01,\n",
      "          -3.3200e-01, -3.0151e-01],\n",
      "         [ 2.8179e-01, -2.3902e-01,  3.1419e-03,  ...,  2.1478e-01,\n",
      "          -6.5562e-01, -2.9937e-01]],\n",
      "\n",
      "        [[ 5.4487e-01, -2.0410e-01,  1.1865e-01,  ...,  2.6924e-01,\n",
      "          -1.1948e-01,  1.9697e-01],\n",
      "         [ 5.9953e-01, -1.8683e-01,  1.0440e-01,  ...,  1.6923e-01,\n",
      "          -2.8865e-03,  2.1166e-01],\n",
      "         [-1.7130e-01,  3.7637e-01,  9.2002e-02,  ..., -5.6459e-01,\n",
      "          -5.7175e-01, -4.7302e-01],\n",
      "         ...,\n",
      "         [ 3.5735e-01,  1.4292e-01,  4.5212e-02,  ..., -9.3843e-02,\n",
      "          -2.7352e-02, -2.3044e-01],\n",
      "         [ 1.4780e-01, -4.1068e-02,  6.6296e-02,  ...,  1.4331e-01,\n",
      "          -3.3200e-01, -3.0151e-01],\n",
      "         [ 2.8179e-01, -2.3902e-01,  3.1419e-03,  ...,  2.1478e-01,\n",
      "          -6.5562e-01, -2.9937e-01]],\n",
      "\n",
      "        [[ 5.4487e-01, -2.0410e-01,  1.1865e-01,  ...,  2.6924e-01,\n",
      "          -1.1948e-01,  1.9697e-01],\n",
      "         [ 5.9953e-01, -1.8683e-01,  1.0440e-01,  ...,  1.6923e-01,\n",
      "          -2.8865e-03,  2.1166e-01],\n",
      "         [-7.2732e-01,  2.1149e-02, -4.7241e-01,  ..., -2.9487e-01,\n",
      "          -2.7592e-01, -5.3279e-01],\n",
      "         ...,\n",
      "         [ 3.5735e-01,  1.4292e-01,  4.5212e-02,  ..., -9.3843e-02,\n",
      "          -2.7352e-02, -2.3044e-01],\n",
      "         [ 1.4780e-01, -4.1068e-02,  6.6296e-02,  ...,  1.4331e-01,\n",
      "          -3.3200e-01, -3.0151e-01],\n",
      "         [ 2.8179e-01, -2.3902e-01,  3.1419e-03,  ...,  2.1478e-01,\n",
      "          -6.5562e-01, -2.9937e-01]]]), tensor([[[ 0.8638, -0.4277,  0.0068,  ..., -0.5105, -0.5385,  0.3243],\n",
      "         [ 0.8958, -0.3147, -0.1507,  ..., -0.5836, -0.6247,  0.3226],\n",
      "         [-0.1063,  0.0702, -0.2325,  ..., -0.5445, -0.2659, -0.2043],\n",
      "         ...,\n",
      "         [ 0.0660, -0.0237,  0.2086,  ...,  0.3096, -0.3196, -0.2554],\n",
      "         [ 0.0092, -0.0512,  0.2288,  ...,  0.5003, -0.4805, -0.3451],\n",
      "         [ 0.0900, -0.1774,  0.1733,  ...,  0.6271, -0.7865, -0.2281]],\n",
      "\n",
      "        [[ 0.7822, -0.6871,  0.4080,  ..., -0.3738, -0.5875,  0.1214],\n",
      "         [ 0.7359, -0.5148,  0.1610,  ..., -0.4595, -0.5631,  0.2079],\n",
      "         [ 0.0923, -0.4423,  0.3704,  ..., -0.1074,  0.0284, -0.0812],\n",
      "         ...,\n",
      "         [ 0.0922, -0.1343,  0.2472,  ...,  0.2924, -0.3392, -0.2990],\n",
      "         [ 0.0271, -0.1511,  0.2612,  ...,  0.5028, -0.5036, -0.4004],\n",
      "         [ 0.1371, -0.2744,  0.2100,  ...,  0.5923, -0.7831, -0.2759]],\n",
      "\n",
      "        [[ 0.6580, -0.5102, -0.0901,  ..., -0.6518, -0.7070,  0.2358],\n",
      "         [ 0.6868, -0.4710, -0.2143,  ..., -0.6325, -0.6939,  0.1954],\n",
      "         [ 0.3578,  0.4760,  0.1347,  ...,  0.1973, -0.1215,  0.0481],\n",
      "         ...,\n",
      "         [ 0.0136, -0.1500,  0.2639,  ...,  0.2910, -0.3322, -0.3861],\n",
      "         [-0.0327, -0.1785,  0.2693,  ...,  0.4717, -0.4578, -0.4782],\n",
      "         [ 0.0456, -0.3018,  0.1975,  ...,  0.5883, -0.7799, -0.3459]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.8843, -0.5652,  0.2858,  ..., -0.5209, -0.5967,  0.1849],\n",
      "         [ 0.8826, -0.4752,  0.0826,  ..., -0.5956, -0.6062,  0.1932],\n",
      "         [ 0.1876,  0.1576, -0.0786,  ..., -0.0154, -0.4078, -0.5248],\n",
      "         ...,\n",
      "         [ 0.1464, -0.0580,  0.2350,  ...,  0.3006, -0.2917, -0.2833],\n",
      "         [ 0.0610, -0.1019,  0.2239,  ...,  0.4461, -0.4417, -0.3899],\n",
      "         [ 0.1736, -0.1929,  0.1497,  ...,  0.5964, -0.7552, -0.2849]],\n",
      "\n",
      "        [[ 0.9436, -0.7870,  0.2020,  ..., -0.4119, -0.4944,  0.1521],\n",
      "         [ 0.9175, -0.6642,  0.0148,  ..., -0.4790, -0.5374,  0.1296],\n",
      "         [-0.2211, -0.1418,  0.5225,  ..., -0.5307, -0.5657, -0.1420],\n",
      "         ...,\n",
      "         [ 0.1592, -0.1033,  0.1742,  ...,  0.4022, -0.3691, -0.2582],\n",
      "         [ 0.1077, -0.1402,  0.1750,  ...,  0.5955, -0.5284, -0.3186],\n",
      "         [ 0.1963, -0.2774,  0.1321,  ...,  0.6940, -0.8576, -0.2409]],\n",
      "\n",
      "        [[ 0.9233, -0.6387,  0.2346,  ..., -0.4196, -0.3694,  0.2145],\n",
      "         [ 0.9200, -0.4734,  0.0476,  ..., -0.5196, -0.4729,  0.1437],\n",
      "         [-0.5926, -0.3527, -0.0395,  ..., -0.2029, -0.3918, -0.1538],\n",
      "         ...,\n",
      "         [ 0.0565, -0.1111,  0.1692,  ...,  0.2701, -0.3357, -0.2500],\n",
      "         [ 0.0022, -0.1249,  0.1892,  ...,  0.4438, -0.4983, -0.3433],\n",
      "         [ 0.0990, -0.2517,  0.1299,  ...,  0.5616, -0.7920, -0.2306]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2588, -0.3747, -0.1807,  ..., -0.7627, -0.1899,  0.6324],\n",
      "         [ 0.1462, -0.2191, -0.5399,  ..., -0.7851, -0.3704,  0.4358],\n",
      "         [-0.1596,  0.3063, -0.2479,  ..., -0.4747,  0.7450,  0.1945],\n",
      "         ...,\n",
      "         [ 0.0349,  0.3258,  0.2355,  ..., -0.4037, -0.4496,  0.2438],\n",
      "         [ 0.1569,  0.3493,  0.2287,  ..., -0.4326, -0.2837,  0.1972],\n",
      "         [ 0.0148,  0.3467,  0.2031,  ..., -0.3944, -0.1751,  0.1138]],\n",
      "\n",
      "        [[ 0.3002, -0.5385,  0.1235,  ..., -0.5917, -0.2019,  0.4299],\n",
      "         [ 0.0858, -0.3348, -0.2519,  ..., -0.4912, -0.2768,  0.2330],\n",
      "         [ 0.0902, -0.2250,  0.2502,  ..., -0.3178,  0.6824, -0.1481],\n",
      "         ...,\n",
      "         [ 0.0810,  0.3081,  0.2438,  ..., -0.4276, -0.3639,  0.1727],\n",
      "         [ 0.1719,  0.3607,  0.2352,  ..., -0.4438, -0.2578,  0.0915],\n",
      "         [ 0.0496,  0.3517,  0.2430,  ..., -0.3952, -0.1520,  0.0232]],\n",
      "\n",
      "        [[-0.1673, -0.7749, -0.0236,  ..., -0.3518, -0.5086,  0.5052],\n",
      "         [-0.0108, -0.5200, -0.2337,  ..., -0.2814, -0.4322,  0.0649],\n",
      "         [ 0.0280,  0.2426, -0.1194,  ...,  0.4742, -0.2116, -0.0337],\n",
      "         ...,\n",
      "         [ 0.0757,  0.3140,  0.2187,  ..., -0.4123, -0.3885,  0.1386],\n",
      "         [ 0.1751,  0.3410,  0.2276,  ..., -0.4734, -0.2511,  0.0782],\n",
      "         [ 0.0439,  0.3417,  0.2500,  ..., -0.4313, -0.1463, -0.0257]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.3786, -0.3262, -0.0452,  ..., -0.9057, -0.2657,  0.6373],\n",
      "         [ 0.3559, -0.1905, -0.4675,  ..., -0.9162, -0.2992,  0.5054],\n",
      "         [ 0.1065,  0.4473, -0.1268,  ..., -0.2152,  0.6961, -0.0865],\n",
      "         ...,\n",
      "         [ 0.0562,  0.2968,  0.2641,  ..., -0.3674, -0.4444,  0.2245],\n",
      "         [ 0.1757,  0.3301,  0.2495,  ..., -0.4344, -0.3213,  0.1270],\n",
      "         [ 0.0544,  0.3249,  0.2248,  ..., -0.3936, -0.1879,  0.0485]],\n",
      "\n",
      "        [[ 0.2292, -0.9085,  0.0063,  ..., -0.2870, -0.1996,  0.7797],\n",
      "         [ 0.1736, -0.7759, -0.2383,  ..., -0.2918, -0.3602,  0.4358],\n",
      "         [-0.3056,  0.5695,  0.4720,  ..., -0.3963,  0.2870,  0.1236],\n",
      "         ...,\n",
      "         [ 0.1239,  0.2701,  0.1292,  ..., -0.3752, -0.3797,  0.1825],\n",
      "         [ 0.1927,  0.2554,  0.1309,  ..., -0.4037, -0.3020,  0.1403],\n",
      "         [ 0.0568,  0.2992,  0.1874,  ..., -0.3460, -0.1989,  0.0329]],\n",
      "\n",
      "        [[ 0.1007, -0.8377,  0.0954,  ..., -0.5325, -0.1602,  0.5010],\n",
      "         [-0.0616, -0.5728, -0.2466,  ..., -0.6169, -0.3313,  0.1040],\n",
      "         [-0.1204,  0.1116,  0.1813,  ..., -0.2108,  0.1982,  0.1955],\n",
      "         ...,\n",
      "         [-0.0163,  0.3215,  0.2058,  ..., -0.3777, -0.4125,  0.3231],\n",
      "         [ 0.0970,  0.3821,  0.2308,  ..., -0.3966, -0.2832,  0.2369],\n",
      "         [-0.0535,  0.3676,  0.2346,  ..., -0.3343, -0.1573,  0.1191]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.7528, -0.2407,  0.2125,  ..., -0.4632,  0.4872,  0.4995],\n",
      "         [ 0.8198, -0.1676, -0.1890,  ..., -0.5353,  0.4002,  0.5702],\n",
      "         [-0.1296, -0.0023, -0.8304,  ..., -0.0394,  0.0965,  0.3977],\n",
      "         ...,\n",
      "         [-0.0699,  0.0793, -0.0909,  ..., -0.0957, -0.6598,  0.1838],\n",
      "         [ 0.1750,  0.1817, -0.1003,  ..., -0.1474, -0.6759,  0.1797],\n",
      "         [ 0.1598,  0.1740, -0.1517,  ..., -0.0216, -0.6145,  0.1910]],\n",
      "\n",
      "        [[ 0.8675, -0.3667,  0.4974,  ..., -0.1848,  0.4952,  0.5399],\n",
      "         [ 0.7935, -0.2421,  0.1392,  ..., -0.2535,  0.5667,  0.4528],\n",
      "         [ 0.4469, -0.3270, -0.1931,  ..., -0.0294,  0.0420,  0.2841],\n",
      "         ...,\n",
      "         [ 0.1107,  0.1922,  0.0438,  ..., -0.1541, -0.5944,  0.2731],\n",
      "         [ 0.2708,  0.2845, -0.0556,  ..., -0.2109, -0.6077,  0.1801],\n",
      "         [ 0.2015,  0.2949, -0.0486,  ..., -0.1419, -0.4849,  0.2042]],\n",
      "\n",
      "        [[ 0.5289, -0.4904,  0.3067,  ..., -0.0601,  0.1222,  0.3837],\n",
      "         [ 0.6680, -0.3466,  0.1625,  ...,  0.0831,  0.1462,  0.3784],\n",
      "         [-0.1159,  0.2699, -0.0533,  ...,  0.8550, -0.2808,  0.3153],\n",
      "         ...,\n",
      "         [-0.0502,  0.1642, -0.0253,  ..., -0.1525, -0.4984,  0.2385],\n",
      "         [ 0.1779,  0.2693, -0.0951,  ..., -0.2075, -0.5218,  0.1685],\n",
      "         [ 0.1716,  0.2581, -0.1450,  ..., -0.0810, -0.4461,  0.1944]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.7040, -0.3064,  0.0837,  ..., -0.4494,  0.3831,  0.6523],\n",
      "         [ 0.6799, -0.1480, -0.2495,  ..., -0.3872,  0.6185,  0.6072],\n",
      "         [ 0.1066,  0.1311, -0.5967,  ...,  0.2366,  0.2772,  0.3828],\n",
      "         ...,\n",
      "         [-0.2005,  0.0731, -0.1329,  ..., -0.0447, -0.6009,  0.2102],\n",
      "         [ 0.0039,  0.1563, -0.1927,  ..., -0.0799, -0.6188,  0.0319],\n",
      "         [ 0.0155,  0.1526, -0.2291,  ...,  0.0661, -0.5235,  0.1287]],\n",
      "\n",
      "        [[ 0.8225, -0.8598,  0.4714,  ...,  0.0657,  0.3324,  0.6119],\n",
      "         [ 0.9671, -0.6981,  0.3100,  ..., -0.0513,  0.4455,  0.5251],\n",
      "         [-0.1307,  0.2937,  0.4843,  ...,  0.1887, -0.1645,  0.3566],\n",
      "         ...,\n",
      "         [ 0.0290,  0.1475, -0.0241,  ..., -0.0588, -0.5832,  0.2915],\n",
      "         [ 0.2485,  0.2255, -0.0346,  ..., -0.1087, -0.5610,  0.2189],\n",
      "         [ 0.2328,  0.2331, -0.0566,  ...,  0.0836, -0.4362,  0.2524]],\n",
      "\n",
      "        [[ 0.7204, -0.6520,  0.5559,  ..., -0.1732,  0.4165,  0.4360],\n",
      "         [ 0.8039, -0.5398,  0.2189,  ..., -0.3367,  0.5679,  0.5039],\n",
      "         [-0.1755, -0.4753, -0.3000,  ...,  0.1294, -0.3559,  0.6233],\n",
      "         ...,\n",
      "         [-0.0073,  0.2061, -0.0788,  ..., -0.0645, -0.5123,  0.3714],\n",
      "         [ 0.1636,  0.3546, -0.1740,  ..., -0.0453, -0.4993,  0.2587],\n",
      "         [ 0.1375,  0.3377, -0.2215,  ...,  0.1475, -0.3773,  0.2948]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.2906, -0.1423,  0.9403,  ...,  0.1197, -0.3935, -0.2723],\n",
      "         [-0.1645, -0.0075,  0.6406,  ..., -0.0439, -0.3459, -0.1847],\n",
      "         [-0.5577, -0.1336, -0.2435,  ...,  0.0609, -0.5148,  0.2463],\n",
      "         ...,\n",
      "         [-0.1092,  0.2625,  0.1257,  ..., -0.4023, -0.4119, -0.2791],\n",
      "         [-0.0861,  0.2862,  0.0940,  ..., -0.4944, -0.4407, -0.2805],\n",
      "         [-0.1011,  0.3581,  0.1341,  ..., -0.3714, -0.4267, -0.3833]],\n",
      "\n",
      "        [[-0.2954, -0.3194,  1.0939,  ...,  0.3879, -0.3732, -0.4043],\n",
      "         [-0.3078, -0.1355,  0.7661,  ...,  0.3104, -0.3432, -0.3646],\n",
      "         [-0.3127, -0.5633,  0.4407,  ...,  0.1181, -0.4629,  0.4472],\n",
      "         ...,\n",
      "         [ 0.0632,  0.3032,  0.1503,  ..., -0.3229, -0.5138, -0.1205],\n",
      "         [ 0.0404,  0.3214,  0.0806,  ..., -0.3729, -0.5756, -0.2176],\n",
      "         [ 0.0769,  0.4067,  0.0985,  ..., -0.3010, -0.5472, -0.2828]],\n",
      "\n",
      "        [[-0.2666, -0.3232,  0.9701,  ...,  0.1561, -0.4042, -0.1733],\n",
      "         [-0.2131, -0.1193,  0.6773,  ...,  0.3057, -0.3761, -0.2350],\n",
      "         [-0.6528,  0.2160,  0.3922,  ...,  0.8130, -0.3775,  0.1518],\n",
      "         ...,\n",
      "         [-0.0596,  0.4102,  0.1621,  ..., -0.3493, -0.4577, -0.1564],\n",
      "         [-0.0566,  0.4630,  0.1211,  ..., -0.3924, -0.4925, -0.2199],\n",
      "         [-0.0229,  0.4992,  0.0961,  ..., -0.2934, -0.4718, -0.3151]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.4465, -0.3686,  0.6247,  ...,  0.1656, -0.3335, -0.0135],\n",
      "         [-0.4136, -0.2720,  0.3516,  ...,  0.1899, -0.2812,  0.0017],\n",
      "         [-0.3113, -0.0071,  0.3696,  ...,  0.1412, -0.2447,  0.5044],\n",
      "         ...,\n",
      "         [-0.4077,  0.3942,  0.0953,  ..., -0.2957, -0.5166, -0.2245],\n",
      "         [-0.4527,  0.3576, -0.0220,  ..., -0.3621, -0.5610, -0.2339],\n",
      "         [-0.3859,  0.4549,  0.0191,  ..., -0.1824, -0.5226, -0.3036]],\n",
      "\n",
      "        [[-0.3240, -0.7283,  0.8533,  ...,  0.2073, -0.4031, -0.1835],\n",
      "         [-0.1276, -0.3798,  0.7415,  ...,  0.2792, -0.3551, -0.2276],\n",
      "         [-0.2761, -0.0823,  0.4734,  ...,  0.2909, -0.3588,  0.2025],\n",
      "         ...,\n",
      "         [-0.0991,  0.2593,  0.0107,  ..., -0.4116, -0.4546, -0.1671],\n",
      "         [-0.0590,  0.2944,  0.0769,  ..., -0.4821, -0.5013, -0.1721],\n",
      "         [-0.0505,  0.3663,  0.0944,  ..., -0.3786, -0.4723, -0.2327]],\n",
      "\n",
      "        [[-0.3649, -0.4674,  0.9945,  ...,  0.1716, -0.3201, -0.2696],\n",
      "         [-0.3395, -0.2184,  0.8171,  ...,  0.1975, -0.2864, -0.2761],\n",
      "         [-0.3416, -0.4397,  0.1444,  ...,  0.2358, -0.4983,  0.4093],\n",
      "         ...,\n",
      "         [-0.1448,  0.3407,  0.1990,  ..., -0.3714, -0.4951, -0.1826],\n",
      "         [-0.1645,  0.4305,  0.1264,  ..., -0.4153, -0.5244, -0.2052],\n",
      "         [-0.1916,  0.4694,  0.1184,  ..., -0.2042, -0.4822, -0.2778]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-1.3057e-01,  1.7486e-01,  6.0330e-01,  ..., -2.2309e-01,\n",
      "          -1.5046e-01,  1.0743e-01],\n",
      "         [-1.4154e-01,  2.2511e-01,  3.9875e-01,  ..., -1.7005e-01,\n",
      "          -9.6182e-02, -1.5548e-01],\n",
      "         [-2.3956e-01, -4.8832e-02, -3.2085e-01,  ..., -2.6559e-01,\n",
      "          -3.2009e-01,  1.7091e-01],\n",
      "         ...,\n",
      "         [-2.5231e-01,  2.6580e-01, -1.7578e-01,  ..., -4.9866e-01,\n",
      "          -1.2944e-01, -4.5959e-01],\n",
      "         [-2.1338e-01,  2.7760e-01, -2.5997e-01,  ..., -5.4452e-01,\n",
      "          -1.5196e-01, -4.5136e-01],\n",
      "         [-2.4865e-01,  2.7731e-01, -2.6370e-01,  ..., -4.8868e-01,\n",
      "          -1.3254e-01, -5.9041e-01]],\n",
      "\n",
      "        [[-1.8583e-01, -1.5142e-01,  6.8315e-01,  ..., -4.7264e-02,\n",
      "          -1.3471e-01, -3.8041e-02],\n",
      "         [-1.4624e-01,  1.7437e-02,  5.6494e-01,  ...,  1.1144e-01,\n",
      "          -9.6523e-02, -1.1024e-01],\n",
      "         [-2.8964e-02, -4.4188e-01,  3.1267e-01,  ...,  3.3788e-02,\n",
      "          -1.4482e-01,  2.1953e-01],\n",
      "         ...,\n",
      "         [ 7.0086e-03,  4.6663e-01, -2.1180e-02,  ..., -6.5918e-01,\n",
      "          -2.1583e-01, -3.4325e-01],\n",
      "         [-3.4333e-02,  4.3871e-01, -1.5989e-01,  ..., -6.8068e-01,\n",
      "          -2.6362e-01, -2.5783e-01],\n",
      "         [ 5.3681e-03,  4.4904e-01, -1.6539e-01,  ..., -5.7968e-01,\n",
      "          -2.3596e-01, -3.6155e-01]],\n",
      "\n",
      "        [[-2.7915e-01, -2.1210e-01,  3.5931e-01,  ..., -1.7700e-01,\n",
      "          -1.9098e-01,  3.5466e-02],\n",
      "         [-1.8195e-01,  4.8530e-03,  2.3748e-01,  ...,  8.5126e-02,\n",
      "          -1.8368e-01, -2.0137e-01],\n",
      "         [-2.5920e-01,  3.5399e-01,  3.1597e-01,  ...,  4.2234e-01,\n",
      "          -1.3760e-01,  4.6998e-02],\n",
      "         ...,\n",
      "         [-3.1443e-01,  5.1582e-01, -8.7204e-02,  ..., -4.0533e-01,\n",
      "          -2.0165e-01, -2.8198e-01],\n",
      "         [-3.2572e-01,  5.1680e-01, -1.9216e-01,  ..., -4.0308e-01,\n",
      "          -2.2946e-01, -2.3045e-01],\n",
      "         [-3.0638e-01,  4.6942e-01, -2.1170e-01,  ..., -3.4957e-01,\n",
      "          -2.2397e-01, -3.1341e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.6443e-01,  2.0444e-01,  5.3198e-01,  ..., -1.9272e-01,\n",
      "          -6.9542e-02,  3.5538e-01],\n",
      "         [-3.6891e-01,  2.2172e-01,  3.7843e-01,  ...,  2.1330e-02,\n",
      "          -4.7796e-02, -5.9011e-02],\n",
      "         [-1.0084e-01,  1.0714e-02,  4.6979e-01,  ..., -5.5897e-02,\n",
      "           4.6370e-03,  3.8418e-01],\n",
      "         ...,\n",
      "         [-4.5691e-01,  4.3894e-01, -1.7831e-03,  ..., -3.8250e-01,\n",
      "          -2.7876e-01, -1.7833e-01],\n",
      "         [-5.0670e-01,  3.9143e-01, -1.6666e-01,  ..., -3.8497e-01,\n",
      "          -3.0632e-01, -1.1994e-01],\n",
      "         [-4.9727e-01,  4.0699e-01, -1.9206e-01,  ..., -2.6840e-01,\n",
      "          -2.7687e-01, -2.5941e-01]],\n",
      "\n",
      "        [[-1.3251e-01, -5.2764e-01,  3.5537e-01,  ..., -2.1678e-01,\n",
      "          -2.3009e-01,  6.2564e-02],\n",
      "         [-5.0052e-02, -2.4439e-01,  4.8133e-01,  ..., -6.0468e-02,\n",
      "          -1.4377e-01, -2.8538e-02],\n",
      "         [-3.0222e-01, -1.4294e-01,  9.6199e-02,  ..., -1.5960e-02,\n",
      "          -1.9193e-01,  2.6696e-01],\n",
      "         ...,\n",
      "         [-2.6620e-01,  3.9531e-01, -5.7073e-02,  ..., -3.7378e-01,\n",
      "          -1.9905e-01, -2.7568e-01],\n",
      "         [-2.2784e-01,  3.8700e-01, -1.2417e-01,  ..., -4.4476e-01,\n",
      "          -2.3351e-01, -2.4407e-01],\n",
      "         [-2.6405e-01,  3.9492e-01, -1.0872e-01,  ..., -3.5178e-01,\n",
      "          -2.3939e-01, -2.5938e-01]],\n",
      "\n",
      "        [[-2.2122e-01, -3.4802e-01,  5.5701e-01,  ..., -2.7710e-01,\n",
      "          -2.1347e-01,  1.7164e-01],\n",
      "         [-1.9769e-01, -6.9455e-02,  5.1339e-01,  ...,  2.4624e-04,\n",
      "          -8.4397e-02, -4.9203e-02],\n",
      "         [ 9.8197e-04, -3.5458e-01,  2.2988e-01,  ..., -3.2522e-01,\n",
      "          -2.4391e-01,  1.2244e-01],\n",
      "         ...,\n",
      "         [-3.1543e-01,  4.1707e-01, -2.4202e-02,  ..., -3.1754e-01,\n",
      "          -3.1420e-01, -1.4674e-01],\n",
      "         [-3.1580e-01,  4.4575e-01, -1.1142e-01,  ..., -3.3106e-01,\n",
      "          -3.2808e-01, -1.5168e-01],\n",
      "         [-3.5307e-01,  4.4999e-01, -1.6092e-01,  ..., -1.8826e-01,\n",
      "          -3.1613e-01, -2.7438e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.5142, -0.1510,  0.5241,  ..., -1.2837,  1.4633,  0.7886],\n",
      "         [ 1.0618, -0.2541,  0.3418,  ..., -0.5435,  1.9058,  0.3089],\n",
      "         [ 1.7390, -0.8262, -1.5209,  ..., -0.7839, -0.4509,  0.1627],\n",
      "         ...,\n",
      "         [ 0.2520,  0.8117, -0.2065,  ..., -0.0239,  0.3176, -1.8142],\n",
      "         [ 0.2685,  0.9123, -0.2535,  ..., -0.1480,  0.3130, -1.7731],\n",
      "         [ 0.1593,  0.9125, -0.1686,  ..., -0.1017,  0.3645, -1.8930]],\n",
      "\n",
      "        [[ 1.5396, -1.1482,  0.7441,  ..., -1.3411,  1.6641,  0.3903],\n",
      "         [ 1.3658, -1.1848,  0.3218,  ..., -0.4742,  1.9694,  0.1285],\n",
      "         [ 1.1253, -2.8562,  0.4510,  ..., -0.3987,  0.9994,  0.3390],\n",
      "         ...,\n",
      "         [ 0.3710,  1.4200,  0.0645,  ..., -0.3389, -0.5298, -1.2695],\n",
      "         [ 0.3451,  1.3950, -0.2457,  ..., -0.4887, -0.4610, -1.0810],\n",
      "         [ 0.4581,  1.4442, -0.2690,  ..., -0.4052, -0.3732, -1.3189]],\n",
      "\n",
      "        [[ 1.1493, -0.8320,  0.5706,  ..., -0.9843,  1.5386,  1.3253],\n",
      "         [ 0.7257, -0.6977,  0.2084,  ...,  0.0332,  1.7940,  0.9516],\n",
      "         [ 0.2447,  0.5205, -0.1444,  ...,  0.8947,  1.3706,  0.9306],\n",
      "         ...,\n",
      "         [-0.3290,  1.5543,  0.0346,  ...,  0.2474,  0.2061, -1.3750],\n",
      "         [-0.4691,  1.5877, -0.1561,  ...,  0.1395,  0.3433, -1.3038],\n",
      "         [-0.4271,  1.4679, -0.0596,  ...,  0.1048,  0.3973, -1.3892]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.6406,  0.2440,  1.1667,  ..., -1.6457,  1.3966,  1.0026],\n",
      "         [ 0.8208, -0.4423,  0.4821,  ..., -0.0883,  1.2641,  0.0760],\n",
      "         [ 0.9700, -2.1383,  2.4512,  ..., -0.7543,  2.3298, -0.0757],\n",
      "         ...,\n",
      "         [-0.5927,  1.1102,  0.3559,  ..., -0.0120, -0.3010, -1.1777],\n",
      "         [-0.7012,  1.1279,  0.1623,  ..., -0.0165, -0.2478, -1.1591],\n",
      "         [-0.6575,  1.1671,  0.2091,  ...,  0.0502, -0.2515, -1.2751]],\n",
      "\n",
      "        [[ 1.8769, -1.7464, -0.1412,  ..., -1.7070,  0.7091,  1.1427],\n",
      "         [ 1.6931, -1.6867, -0.0722,  ..., -1.0381,  1.0134,  0.5630],\n",
      "         [ 1.0057, -1.5236, -0.0988,  ..., -0.8935,  0.7584, -0.1002],\n",
      "         ...,\n",
      "         [ 0.4620,  1.3029, -0.1688,  ..., -0.3689, -0.6243, -1.9278],\n",
      "         [ 0.4674,  1.2788, -0.3612,  ..., -0.4372, -0.6791, -1.8011],\n",
      "         [ 0.4313,  1.2258, -0.2612,  ..., -0.4627, -0.6094, -1.8635]],\n",
      "\n",
      "        [[ 0.8495, -1.3208, -0.2307,  ..., -1.9371,  1.7563,  1.0297],\n",
      "         [ 0.6563, -1.2115, -0.2449,  ..., -1.0948,  2.1865,  0.1446],\n",
      "         [ 1.6053, -0.1595,  0.4021,  ..., -1.0474, -0.8533,  0.0786],\n",
      "         ...,\n",
      "         [-0.6652,  1.3236, -0.4783,  ..., -0.4556, -0.4609, -1.4307],\n",
      "         [-0.6752,  1.3673, -0.5791,  ..., -0.5580, -0.3192, -1.5066],\n",
      "         [-0.9025,  1.4019, -0.5560,  ..., -0.4221, -0.2276, -1.7466]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)), decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[-3.6734e-02,  8.6319e-03, -7.2903e-04,  ...,  1.2003e-02,\n",
      "          -1.3715e-03, -1.3877e-02],\n",
      "         [ 3.4117e-02,  3.0287e-01,  4.5873e-01,  ..., -4.8491e-01,\n",
      "          -1.2508e-01,  1.7999e-02],\n",
      "         [-3.1555e-01, -3.4673e-02,  8.4742e-02,  ..., -2.2923e-01,\n",
      "          -2.1967e-01,  2.7455e-02],\n",
      "         ...,\n",
      "         [ 1.0676e-01,  1.5592e-01,  4.6004e-02,  ...,  1.0182e-02,\n",
      "           3.0518e-01, -2.8509e-02],\n",
      "         [ 8.6725e-02,  1.6952e-01,  1.1605e-01,  ..., -7.0990e-03,\n",
      "           1.6469e-01,  3.0242e-02],\n",
      "         [ 8.6562e-02,  1.6181e-01,  3.1028e-04,  ..., -4.7521e-02,\n",
      "           3.3894e-01,  1.2672e-02]],\n",
      "\n",
      "        [[-3.5419e-02,  9.2873e-03, -1.6966e-03,  ...,  1.2503e-02,\n",
      "          -2.1651e-03, -1.2238e-02],\n",
      "         [ 2.5491e-02,  4.5046e-01,  4.4546e-01,  ..., -3.6255e-01,\n",
      "          -2.9727e-02,  9.2169e-02],\n",
      "         [-2.9702e-01,  9.0375e-02,  1.0779e-01,  ...,  8.3403e-02,\n",
      "          -2.0589e-01,  2.2857e-02],\n",
      "         ...,\n",
      "         [ 3.1931e-02,  9.5955e-02, -5.3182e-02,  ..., -6.9919e-02,\n",
      "           3.2908e-01,  2.9189e-02],\n",
      "         [-2.2695e-02,  1.2695e-01,  8.2310e-04,  ..., -2.1327e-02,\n",
      "           6.0888e-02, -4.3807e-02],\n",
      "         [ 4.1678e-02,  1.7811e-01, -2.6452e-02,  ..., -8.8489e-02,\n",
      "           2.0958e-01, -3.4919e-02]],\n",
      "\n",
      "        [[-3.6783e-02,  9.1253e-03, -1.4285e-03,  ...,  1.2207e-02,\n",
      "          -2.0960e-03, -1.2865e-02],\n",
      "         [-2.6580e-01, -5.8496e-02,  1.9636e-01,  ..., -5.6467e-01,\n",
      "          -1.1278e-01,  3.9604e-01],\n",
      "         [-2.1835e-01,  8.5198e-03,  2.0092e-01,  ..., -2.7704e-01,\n",
      "          -2.1240e-01,  1.2808e-01],\n",
      "         ...,\n",
      "         [ 1.9922e-01,  2.6604e-01,  1.4957e-01,  ...,  5.0302e-02,\n",
      "           1.8363e-01,  1.7662e-01],\n",
      "         [ 4.3705e-02,  1.9366e-01,  3.0340e-02,  ..., -3.1763e-03,\n",
      "           1.2702e-01, -4.0951e-02],\n",
      "         [ 5.7049e-02,  2.1417e-01,  2.0924e-02,  ..., -3.5107e-02,\n",
      "           1.2727e-01, -2.5509e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.7186e-02,  8.8164e-03, -1.4936e-03,  ...,  1.0904e-02,\n",
      "          -1.7325e-03, -1.3742e-02],\n",
      "         [ 5.6369e-02, -4.4141e-02,  3.7331e-01,  ..., -4.6092e-01,\n",
      "          -9.9286e-03, -6.6058e-02],\n",
      "         [ 5.0902e-01, -9.3291e-03,  3.8274e-01,  ..., -2.7135e-01,\n",
      "          -1.1596e-01, -8.2998e-02],\n",
      "         ...,\n",
      "         [ 1.1972e-01,  1.3091e-01,  1.3105e-01,  ...,  1.5859e-01,\n",
      "           3.9636e-01,  1.7175e-01],\n",
      "         [ 4.1628e-02,  9.7630e-02,  4.6365e-02,  ...,  6.3665e-02,\n",
      "           4.7453e-01, -3.8389e-02],\n",
      "         [ 8.0448e-02, -1.0848e-02,  1.2481e-01,  ...,  2.0559e-01,\n",
      "           5.4338e-01, -4.8583e-02]],\n",
      "\n",
      "        [[-3.6176e-02,  8.1172e-03, -2.2936e-03,  ...,  1.2429e-02,\n",
      "          -7.4224e-04, -1.3573e-02],\n",
      "         [-5.6736e-02,  5.6227e-02,  1.8314e-01,  ..., -5.5586e-01,\n",
      "          -8.0820e-01,  1.6854e-01],\n",
      "         [-6.6504e-02,  2.0570e-02,  5.3674e-02,  ..., -5.2859e-02,\n",
      "          -2.3335e-01, -4.1090e-02],\n",
      "         ...,\n",
      "         [ 2.6208e-01, -2.3544e-01,  3.4805e-01,  ...,  2.1148e-01,\n",
      "           3.5166e-01,  1.0027e-01],\n",
      "         [ 2.2969e-01, -2.3725e-01,  2.6886e-01,  ...,  2.4361e-01,\n",
      "           4.4454e-01, -3.5410e-02],\n",
      "         [ 2.4237e-01, -1.5929e-01,  2.0731e-01,  ...,  2.4843e-01,\n",
      "           4.3642e-01, -4.4908e-02]],\n",
      "\n",
      "        [[-3.5563e-02,  9.2754e-03, -1.7347e-03,  ...,  1.1789e-02,\n",
      "          -1.7801e-03, -1.3261e-02],\n",
      "         [-1.5304e-02,  3.8654e-01,  3.6954e-01,  ..., -4.5183e-01,\n",
      "          -6.8077e-02, -6.3915e-02],\n",
      "         [-3.5451e-01, -5.2322e-02,  8.2345e-02,  ...,  1.2869e-02,\n",
      "          -1.3913e-01, -1.3590e-01],\n",
      "         ...,\n",
      "         [ 1.9060e-01,  1.0166e-01,  1.2045e-01,  ...,  9.0907e-02,\n",
      "           3.0933e-01,  1.8290e-01],\n",
      "         [ 9.6616e-02,  2.0378e-01,  8.4501e-02,  ...,  1.0222e-01,\n",
      "           1.6628e-01,  3.5184e-03],\n",
      "         [ 1.1900e-01,  1.8047e-01,  4.2640e-02,  ...,  1.0734e-01,\n",
      "           3.7000e-01,  1.7160e-02]]]), encoder_hidden_states=(tensor([[[ 0.1810, -0.1337,  0.0424,  ...,  0.0790, -0.2555,  0.0459],\n",
      "         [ 0.6842,  0.0244,  0.5646,  ..., -0.2659, -0.1468, -0.3223],\n",
      "         [-0.3948, -0.1109,  0.3872,  ..., -0.2016, -0.0933,  0.0181],\n",
      "         ...,\n",
      "         [ 0.4157,  0.6533, -0.0622,  ...,  0.3168,  0.2436, -0.2378],\n",
      "         [ 0.4539,  0.4505, -0.0401,  ...,  0.4219,  0.1895, -0.1260],\n",
      "         [ 0.5269,  0.2336, -0.2028,  ...,  0.7653,  0.1041, -0.1408]],\n",
      "\n",
      "        [[ 0.1810, -0.1337,  0.0424,  ...,  0.0790, -0.2555,  0.0459],\n",
      "         [ 0.6842,  0.0244,  0.5646,  ..., -0.2659, -0.1468, -0.3223],\n",
      "         [-0.3948, -0.1109,  0.3872,  ..., -0.2016, -0.0933,  0.0181],\n",
      "         ...,\n",
      "         [ 0.4157,  0.6533, -0.0622,  ...,  0.3168,  0.2436, -0.2378],\n",
      "         [ 0.4539,  0.4505, -0.0401,  ...,  0.4219,  0.1895, -0.1260],\n",
      "         [ 0.5269,  0.2336, -0.2028,  ...,  0.7653,  0.1041, -0.1408]],\n",
      "\n",
      "        [[ 0.1810, -0.1337,  0.0424,  ...,  0.0790, -0.2555,  0.0459],\n",
      "         [-0.0893, -0.1705, -0.3339,  ..., -0.1253,  0.1086,  0.1398],\n",
      "         [ 0.2830, -0.0849,  0.3077,  ..., -0.5768,  0.2044,  0.2126],\n",
      "         ...,\n",
      "         [ 0.4157,  0.6533, -0.0622,  ...,  0.3168,  0.2436, -0.2378],\n",
      "         [ 0.4539,  0.4505, -0.0401,  ...,  0.4219,  0.1895, -0.1260],\n",
      "         [ 0.5269,  0.2336, -0.2028,  ...,  0.7653,  0.1041, -0.1408]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1810, -0.1337,  0.0424,  ...,  0.0790, -0.2555,  0.0459],\n",
      "         [ 0.3317, -0.3658,  0.3413,  ...,  0.4787,  0.2499, -0.3003],\n",
      "         [ 0.7257, -0.2176,  0.4374,  ..., -0.2051,  0.1029,  0.1593],\n",
      "         ...,\n",
      "         [ 0.4157,  0.6533, -0.0622,  ...,  0.3168,  0.2436, -0.2378],\n",
      "         [ 0.4539,  0.4505, -0.0401,  ...,  0.4219,  0.1895, -0.1260],\n",
      "         [ 0.5269,  0.2336, -0.2028,  ...,  0.7653,  0.1041, -0.1408]],\n",
      "\n",
      "        [[ 0.1810, -0.1337,  0.0424,  ...,  0.0790, -0.2555,  0.0459],\n",
      "         [ 0.3316, -0.1717,  0.2108,  ..., -0.8133, -1.2830,  0.3356],\n",
      "         [-0.7438, -0.3278,  0.1626,  ..., -0.2834, -0.1164, -0.2186],\n",
      "         ...,\n",
      "         [ 0.4157,  0.6533, -0.0622,  ...,  0.3168,  0.2436, -0.2378],\n",
      "         [ 0.4539,  0.4505, -0.0401,  ...,  0.4219,  0.1895, -0.1260],\n",
      "         [ 0.5269,  0.2336, -0.2028,  ...,  0.7653,  0.1041, -0.1408]],\n",
      "\n",
      "        [[ 0.1810, -0.1337,  0.0424,  ...,  0.0790, -0.2555,  0.0459],\n",
      "         [ 0.6842,  0.0244,  0.5646,  ..., -0.2659, -0.1468, -0.3223],\n",
      "         [-0.3948, -0.1109,  0.3872,  ..., -0.2016, -0.0933,  0.0181],\n",
      "         ...,\n",
      "         [ 0.4157,  0.6533, -0.0622,  ...,  0.3168,  0.2436, -0.2378],\n",
      "         [ 0.4539,  0.4505, -0.0401,  ...,  0.4219,  0.1895, -0.1260],\n",
      "         [ 0.5269,  0.2336, -0.2028,  ...,  0.7653,  0.1041, -0.1408]]]), tensor([[[-0.0380, -0.0252,  0.0567,  ...,  0.0280, -0.0251,  0.0174],\n",
      "         [ 0.5785,  0.0409,  0.6697,  ...,  0.2364,  0.1219, -0.0726],\n",
      "         [-0.2979,  0.2823,  0.0414,  ...,  0.2208, -0.2531,  0.1403],\n",
      "         ...,\n",
      "         [ 0.3077,  0.5781,  0.0792,  ...,  0.4728,  0.5677, -0.3358],\n",
      "         [ 0.4060,  0.5103,  0.1495,  ...,  0.8098,  0.3255, -0.0284],\n",
      "         [ 0.3984,  0.3378, -0.1424,  ...,  0.9221,  0.3168,  0.0716]],\n",
      "\n",
      "        [[-0.0416, -0.0375,  0.0527,  ...,  0.0209, -0.0303,  0.0212],\n",
      "         [ 0.5118,  0.0777,  0.6239,  ...,  0.1428,  0.1087, -0.0303],\n",
      "         [-0.3715,  0.2336,  0.0646,  ...,  0.1973, -0.2790,  0.0265],\n",
      "         ...,\n",
      "         [ 0.1821,  0.7147,  0.0530,  ...,  0.3677,  0.4739, -0.2157],\n",
      "         [ 0.0472,  0.4180,  0.0643,  ...,  0.5917,  0.4438, -0.1998],\n",
      "         [ 0.4191,  0.3388, -0.0274,  ...,  0.7609,  0.2196,  0.0157]],\n",
      "\n",
      "        [[-0.0281, -0.0273,  0.0551,  ...,  0.0250, -0.0305,  0.0223],\n",
      "         [-0.1352,  0.1457,  0.1824,  ..., -0.0978, -0.1993,  0.6333],\n",
      "         [ 0.2309, -0.0065,  0.4946,  ..., -0.3244, -0.1665,  0.3072],\n",
      "         ...,\n",
      "         [ 0.3522,  0.8129,  0.1496,  ...,  0.2600,  0.4735, -0.2405],\n",
      "         [ 0.2494,  0.5805,  0.0494,  ...,  0.6308,  0.5565, -0.2000],\n",
      "         [ 0.4413,  0.4676, -0.0904,  ...,  0.8188,  0.4287, -0.0151]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0495, -0.0299,  0.0538,  ...,  0.0272, -0.0304,  0.0179],\n",
      "         [ 0.4578, -0.3811,  0.5810,  ...,  0.6904,  0.1759, -0.3546],\n",
      "         [ 0.8055,  0.1988,  0.4976,  ...,  0.1304, -0.1296,  0.4332],\n",
      "         ...,\n",
      "         [ 0.3960,  0.5640,  0.1178,  ...,  0.3213,  0.4553, -0.2209],\n",
      "         [ 0.4314,  0.5106,  0.0609,  ...,  0.5975,  0.5167, -0.0537],\n",
      "         [ 0.4260,  0.3205,  0.0051,  ...,  0.8247,  0.4790,  0.1655]],\n",
      "\n",
      "        [[-0.0420, -0.0291,  0.0570,  ...,  0.0254, -0.0216,  0.0220],\n",
      "         [ 0.1482,  0.2837,  0.4431,  ..., -0.3945, -1.6952,  0.5638],\n",
      "         [-0.6642, -0.1402,  0.2141,  ..., -0.1715, -0.7138, -0.0033],\n",
      "         ...,\n",
      "         [ 0.3178,  0.5205,  0.1447,  ...,  0.3323,  0.4494, -0.2529],\n",
      "         [ 0.5320,  0.4257,  0.0699,  ...,  0.4586,  0.4068, -0.1808],\n",
      "         [ 0.3837,  0.3155, -0.0732,  ...,  0.9335,  0.3797, -0.1753]],\n",
      "\n",
      "        [[-0.0288, -0.0440,  0.0490,  ...,  0.0292, -0.0333,  0.0142],\n",
      "         [ 0.5127,  0.0564,  0.6216,  ...,  0.2134,  0.1252, -0.0194],\n",
      "         [-0.3474,  0.3227, -0.0112,  ...,  0.2202, -0.2167,  0.1147],\n",
      "         ...,\n",
      "         [ 0.4030,  0.5045,  0.0523,  ...,  0.2798,  0.4904, -0.3246],\n",
      "         [ 0.6025,  0.5374,  0.1301,  ...,  0.4946,  0.4705, -0.1254],\n",
      "         [ 0.4066,  0.2382, -0.1424,  ...,  0.9180,  0.5421, -0.0491]]]), tensor([[[-6.2080e-02,  5.2422e-03,  3.4844e-02,  ..., -3.3564e-02,\n",
      "          -4.7842e-02,  3.8186e-03],\n",
      "         [ 6.0791e-01,  3.3496e-01,  6.4234e-01,  ..., -2.2238e-01,\n",
      "           9.3869e-02, -3.7772e-02],\n",
      "         [-4.9711e-01,  1.6587e-01,  2.9287e-01,  ...,  2.1478e-01,\n",
      "          -3.9688e-01,  9.6822e-02],\n",
      "         ...,\n",
      "         [ 3.5421e-01,  3.7668e-01,  1.6214e-01,  ...,  2.7700e-01,\n",
      "           8.1765e-01, -3.6257e-01],\n",
      "         [ 3.6857e-01,  4.3959e-01,  1.8806e-01,  ...,  4.3426e-01,\n",
      "           5.4289e-01, -2.0985e-01],\n",
      "         [ 3.9261e-01,  4.2503e-01, -9.3932e-02,  ...,  5.6658e-01,\n",
      "           5.4945e-01, -2.2101e-01]],\n",
      "\n",
      "        [[-7.0659e-02, -7.9769e-04,  3.0630e-02,  ..., -3.8251e-02,\n",
      "          -5.0779e-02, -5.0380e-03],\n",
      "         [ 5.7554e-01,  2.3005e-01,  5.7942e-01,  ..., -5.5078e-02,\n",
      "           6.2465e-02,  3.6848e-02],\n",
      "         [-4.9937e-01,  4.6765e-01,  2.4630e-01,  ...,  2.7520e-01,\n",
      "          -2.5268e-01,  1.4528e-01],\n",
      "         ...,\n",
      "         [ 1.5626e-01,  5.0710e-01,  9.1691e-02,  ...,  1.1173e-01,\n",
      "           7.1049e-01, -2.4101e-01],\n",
      "         [ 2.8692e-02,  4.8681e-01,  4.1299e-03,  ...,  2.8228e-01,\n",
      "           7.0605e-01, -2.6407e-01],\n",
      "         [ 3.6843e-01,  4.2233e-01,  3.2834e-02,  ...,  3.6558e-01,\n",
      "           4.6974e-01, -2.0550e-01]],\n",
      "\n",
      "        [[-6.2439e-02,  5.5139e-03,  3.3182e-02,  ..., -4.6456e-02,\n",
      "          -5.8869e-02, -8.6530e-03],\n",
      "         [-4.3111e-02,  2.1671e-01,  1.1043e-01,  ..., -5.7620e-01,\n",
      "          -2.0107e-01,  3.0072e-01],\n",
      "         [ 2.1919e-01,  5.5508e-02,  4.1896e-01,  ..., -6.6453e-01,\n",
      "          -3.7984e-01, -5.9193e-02],\n",
      "         ...,\n",
      "         [ 3.8057e-01,  6.7677e-01,  2.3264e-01,  ...,  5.9092e-02,\n",
      "           6.0737e-01, -3.5423e-01],\n",
      "         [ 2.4181e-01,  6.5590e-01,  4.5732e-02,  ...,  2.7397e-01,\n",
      "           7.6153e-01, -3.4950e-01],\n",
      "         [ 4.0275e-01,  6.7561e-01, -8.1989e-02,  ...,  3.3933e-01,\n",
      "           6.2649e-01, -3.2636e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-6.7001e-02,  6.4824e-03,  3.6739e-02,  ..., -3.5737e-02,\n",
      "          -5.0259e-02, -3.6075e-03],\n",
      "         [ 4.6914e-01,  8.2782e-02,  5.5067e-01,  ...,  6.1265e-01,\n",
      "           2.7970e-01, -9.8352e-02],\n",
      "         [ 7.6335e-01,  3.4995e-01,  4.8522e-01,  ...,  6.4565e-02,\n",
      "           2.8651e-02,  3.1341e-01],\n",
      "         ...,\n",
      "         [ 3.5593e-01,  5.5602e-01,  2.0754e-01,  ...,  2.4078e-01,\n",
      "           6.7490e-01, -3.5870e-01],\n",
      "         [ 3.7169e-01,  4.4581e-01,  2.8631e-02,  ...,  3.6412e-01,\n",
      "           7.8261e-01, -2.5252e-01],\n",
      "         [ 3.9144e-01,  4.9217e-01,  5.0594e-02,  ...,  5.5332e-01,\n",
      "           7.1956e-01, -2.1672e-01]],\n",
      "\n",
      "        [[-6.0385e-02,  3.4136e-03,  3.2066e-02,  ..., -4.9499e-02,\n",
      "          -5.7040e-02, -5.7339e-03],\n",
      "         [ 3.2667e-01, -1.1396e-01,  3.3367e-01,  ..., -6.2779e-01,\n",
      "          -1.5970e+00,  4.3098e-01],\n",
      "         [-5.2714e-01, -1.7635e-01,  2.7365e-01,  ..., -1.0714e-01,\n",
      "          -5.6676e-01, -2.1593e-01],\n",
      "         ...,\n",
      "         [ 3.7523e-01,  2.2557e-01,  2.9435e-01,  ...,  2.2811e-01,\n",
      "           5.8466e-01, -4.1307e-01],\n",
      "         [ 5.5053e-01,  1.9905e-01,  1.7366e-01,  ...,  2.0408e-01,\n",
      "           6.2989e-01, -3.7163e-01],\n",
      "         [ 4.5455e-01,  2.7527e-01,  5.5935e-02,  ...,  6.0809e-01,\n",
      "           5.7608e-01, -4.5937e-01]],\n",
      "\n",
      "        [[-5.8663e-02,  4.4091e-03,  3.0879e-02,  ..., -3.1706e-02,\n",
      "          -6.2669e-02, -4.3221e-03],\n",
      "         [ 5.3918e-01,  4.1164e-01,  6.3839e-01,  ..., -1.5840e-01,\n",
      "           1.6177e-01, -8.0842e-02],\n",
      "         [-5.2840e-01,  1.9747e-01,  2.7332e-01,  ...,  3.6011e-01,\n",
      "          -3.2087e-01,  2.4482e-01],\n",
      "         ...,\n",
      "         [ 4.1292e-01,  3.6523e-01,  1.5442e-01,  ...,  2.5214e-01,\n",
      "           6.2704e-01, -3.0010e-01],\n",
      "         [ 5.3450e-01,  4.3160e-01,  1.7580e-01,  ...,  4.3910e-01,\n",
      "           6.2491e-01, -8.4102e-02],\n",
      "         [ 3.8049e-01,  2.9968e-01, -1.0034e-02,  ...,  6.1941e-01,\n",
      "           7.3983e-01, -1.9563e-01]]]), tensor([[[-6.4848e-02,  2.3370e-02,  8.1823e-03,  ..., -1.6875e-02,\n",
      "          -6.1049e-03, -1.4356e-03],\n",
      "         [ 2.7494e-01,  3.4187e-01,  6.9879e-01,  ..., -2.4917e-01,\n",
      "           3.8551e-02,  1.8602e-01],\n",
      "         [-6.6428e-01,  1.9228e-01,  9.1495e-02,  ...,  1.2626e-01,\n",
      "          -2.5972e-01,  3.9793e-01],\n",
      "         ...,\n",
      "         [-1.8315e-02, -2.3010e-01,  2.1958e-01,  ...,  3.3169e-01,\n",
      "           8.0055e-01,  1.8079e-01],\n",
      "         [ 1.0931e-01, -2.6565e-01,  2.9607e-01,  ...,  4.9660e-01,\n",
      "           5.0998e-01,  2.0820e-01],\n",
      "         [ 7.1265e-02, -2.2637e-01, -2.2273e-02,  ...,  5.3606e-01,\n",
      "           5.2269e-01,  2.8936e-01]],\n",
      "\n",
      "        [[-6.5374e-02,  2.2404e-02,  9.2285e-03,  ..., -1.9290e-02,\n",
      "          -3.2731e-03, -3.2658e-03],\n",
      "         [ 3.0591e-01,  1.3246e-01,  5.8023e-01,  ..., -1.0748e-01,\n",
      "           4.8603e-02,  2.9963e-01],\n",
      "         [-5.5747e-01,  3.5670e-01,  7.2405e-02,  ...,  4.0358e-01,\n",
      "          -1.8751e-01,  4.4845e-01],\n",
      "         ...,\n",
      "         [-1.6493e-01, -1.9076e-01,  9.4796e-02,  ..., -7.8122e-02,\n",
      "           6.2302e-01, -2.5612e-02],\n",
      "         [-1.8566e-01, -1.7228e-01,  8.4920e-02,  ...,  1.1575e-01,\n",
      "           5.7989e-01,  4.5392e-02],\n",
      "         [ 8.2076e-02, -1.8513e-01,  1.1505e-01,  ...,  1.7875e-01,\n",
      "           4.0625e-01,  6.8146e-02]],\n",
      "\n",
      "        [[-6.5048e-02,  2.2623e-02,  7.2410e-03,  ..., -1.5821e-02,\n",
      "          -1.0856e-02, -5.6833e-03],\n",
      "         [-2.8059e-01,  2.7664e-01,  1.6447e-01,  ..., -4.7123e-01,\n",
      "          -4.0805e-01,  4.6533e-01],\n",
      "         [ 5.4784e-02,  2.2833e-01,  4.2699e-01,  ..., -2.9179e-01,\n",
      "          -5.7199e-01,  2.1916e-01],\n",
      "         ...,\n",
      "         [-6.3107e-02,  5.7073e-02,  2.5142e-01,  ...,  1.2262e-02,\n",
      "           4.3562e-01, -3.1664e-02],\n",
      "         [-1.3901e-01,  4.0332e-02,  1.1283e-01,  ...,  2.3921e-01,\n",
      "           6.4675e-01, -5.4579e-02],\n",
      "         [ 2.8439e-03,  1.0772e-01,  6.3999e-04,  ...,  2.3571e-01,\n",
      "           4.7224e-01,  8.8303e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-6.5288e-02,  2.3846e-02,  6.6542e-03,  ..., -1.6513e-02,\n",
      "          -5.6191e-03, -5.7908e-03],\n",
      "         [ 3.6317e-01,  1.7374e-01,  5.7459e-01,  ...,  2.2458e-01,\n",
      "           2.3775e-01, -9.5782e-02],\n",
      "         [ 7.0273e-01,  5.6741e-01,  4.1541e-01,  ..., -1.4076e-01,\n",
      "          -8.6944e-02,  1.3448e-01],\n",
      "         ...,\n",
      "         [-3.0292e-02, -5.5813e-02,  2.8031e-01,  ...,  1.6619e-01,\n",
      "           6.3222e-01, -5.2959e-02],\n",
      "         [ 1.5656e-02, -2.2302e-01,  1.9682e-01,  ...,  3.6524e-01,\n",
      "           8.3023e-01,  1.0470e-01],\n",
      "         [ 2.7689e-02, -1.3795e-01,  2.1555e-01,  ...,  4.5692e-01,\n",
      "           7.2638e-01,  1.2799e-01]],\n",
      "\n",
      "        [[-6.6135e-02,  1.7195e-02,  9.1559e-03,  ..., -3.3006e-02,\n",
      "          -1.4021e-02, -1.6684e-03],\n",
      "         [ 2.5465e-01,  8.0265e-02,  4.6625e-01,  ..., -9.2481e-01,\n",
      "          -1.4951e+00, -1.7481e-01],\n",
      "         [-3.9433e-01,  5.1017e-02,  6.4881e-02,  ..., -2.4570e-01,\n",
      "          -6.4878e-01, -2.3515e-01],\n",
      "         ...,\n",
      "         [ 2.5452e-02, -2.6593e-01,  3.1343e-01,  ...,  3.2307e-02,\n",
      "           4.6431e-01, -2.2162e-01],\n",
      "         [ 1.8467e-01, -3.5171e-01,  2.8160e-01,  ...,  1.6061e-01,\n",
      "           5.3830e-01, -1.7515e-01],\n",
      "         [ 8.1270e-02, -2.2735e-01,  1.5876e-01,  ...,  4.2344e-01,\n",
      "           4.9798e-01, -1.2410e-01]],\n",
      "\n",
      "        [[-6.1053e-02,  1.7884e-02,  5.3081e-03,  ..., -1.7117e-02,\n",
      "          -1.2973e-02, -1.6393e-03],\n",
      "         [ 1.8258e-01,  1.0395e-01,  5.9191e-01,  ..., -6.6747e-02,\n",
      "           1.4052e-01,  1.2593e-01],\n",
      "         [-6.3555e-01,  1.5854e-01,  2.9516e-02,  ...,  4.4318e-01,\n",
      "          -9.4993e-02,  5.0763e-01],\n",
      "         ...,\n",
      "         [ 5.0376e-02, -2.3436e-01,  1.7610e-01,  ..., -1.5852e-02,\n",
      "           5.4181e-01, -6.4650e-02],\n",
      "         [ 2.4791e-01, -2.7626e-01,  2.5768e-01,  ...,  2.1176e-01,\n",
      "           5.3389e-01,  1.1477e-03],\n",
      "         [ 2.7805e-02, -2.3955e-01,  4.4797e-02,  ...,  3.5731e-01,\n",
      "           6.6862e-01,  1.2767e-01]]]), tensor([[[-8.8422e-02,  2.8670e-02, -9.1205e-03,  ...,  4.9515e-02,\n",
      "           5.5729e-03, -4.5183e-03],\n",
      "         [ 1.6107e-01,  6.0197e-01,  8.9130e-01,  ..., -5.6333e-01,\n",
      "          -2.2235e-02,  4.8218e-01],\n",
      "         [-5.8523e-01,  3.5262e-02, -8.5815e-02,  ..., -2.9441e-02,\n",
      "          -3.9554e-01,  5.8526e-01],\n",
      "         ...,\n",
      "         [ 1.5598e-01, -1.5623e-01,  3.1813e-01,  ...,  4.0727e-01,\n",
      "           1.0612e+00,  2.4151e-01],\n",
      "         [ 2.4182e-01, -1.8448e-01,  3.6864e-01,  ...,  6.0389e-01,\n",
      "           7.3074e-01,  2.9290e-01],\n",
      "         [ 2.5309e-01, -2.2100e-01,  4.3932e-02,  ...,  6.1255e-01,\n",
      "           8.0130e-01,  3.0471e-01]],\n",
      "\n",
      "        [[-8.9077e-02,  2.6950e-02, -1.3206e-02,  ...,  5.0460e-02,\n",
      "           6.0190e-03, -9.5236e-03],\n",
      "         [ 2.2699e-01,  4.8909e-01,  9.1280e-01,  ..., -2.3651e-01,\n",
      "          -2.9287e-02,  6.2851e-01],\n",
      "         [-4.2415e-01,  2.8888e-01,  1.0388e-01,  ...,  2.4311e-01,\n",
      "          -2.7156e-01,  7.2070e-01],\n",
      "         ...,\n",
      "         [ 2.4100e-02, -2.0060e-01,  9.3569e-02,  ...,  1.3907e-01,\n",
      "           8.6636e-01,  1.0903e-01],\n",
      "         [-8.7627e-02, -1.0227e-02,  8.9516e-02,  ...,  2.1947e-01,\n",
      "           6.0956e-01,  2.4797e-01],\n",
      "         [ 2.3762e-01, -1.2838e-01,  1.4411e-01,  ...,  3.3384e-01,\n",
      "           6.2802e-01,  2.5603e-01]],\n",
      "\n",
      "        [[-8.8063e-02,  2.7934e-02, -1.0892e-02,  ...,  4.8422e-02,\n",
      "           4.4609e-03, -8.5094e-03],\n",
      "         [-9.4871e-02,  1.1501e-01,  2.4311e-01,  ..., -3.6282e-01,\n",
      "          -5.3041e-01,  6.4398e-01],\n",
      "         [ 9.2159e-02,  5.5876e-02,  3.4606e-01,  ..., -4.5254e-02,\n",
      "          -4.8494e-01,  5.2174e-02],\n",
      "         ...,\n",
      "         [ 2.1649e-01,  2.8489e-02,  3.1617e-01,  ...,  2.3682e-01,\n",
      "           5.5507e-01,  2.1765e-01],\n",
      "         [-9.6768e-03,  9.8594e-02,  2.0925e-01,  ...,  3.2472e-01,\n",
      "           6.7927e-01,  2.9750e-01],\n",
      "         [ 1.1950e-01,  5.9278e-02,  8.7060e-02,  ...,  2.9636e-01,\n",
      "           5.3089e-01,  4.6454e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-8.7695e-02,  2.7256e-02, -9.5349e-03,  ...,  4.8376e-02,\n",
      "           6.6768e-03, -8.9215e-03],\n",
      "         [ 4.0182e-01, -1.0413e-01,  6.0619e-01,  ...,  1.4845e-02,\n",
      "           4.0466e-01, -7.5425e-03],\n",
      "         [ 9.3109e-01,  2.4927e-01,  3.8823e-01,  ..., -2.6646e-01,\n",
      "          -1.6382e-01,  1.2888e-01],\n",
      "         ...,\n",
      "         [ 1.4217e-01, -4.7033e-02,  3.4071e-01,  ...,  3.0222e-01,\n",
      "           8.8199e-01,  4.7254e-02],\n",
      "         [ 8.5651e-02, -2.2754e-01,  2.5417e-01,  ...,  4.9358e-01,\n",
      "           1.1298e+00,  1.4025e-01],\n",
      "         [ 1.4242e-01, -1.4300e-01,  2.7994e-01,  ...,  5.7720e-01,\n",
      "           1.0417e+00,  1.3588e-01]],\n",
      "\n",
      "        [[-8.7281e-02,  2.6853e-02, -1.2522e-02,  ...,  5.2754e-02,\n",
      "           4.0617e-03, -6.6071e-03],\n",
      "         [ 1.0455e-01,  9.8653e-02,  3.4055e-01,  ..., -6.6963e-01,\n",
      "          -1.5500e+00,  1.1746e-01],\n",
      "         [-9.8304e-02, -1.1629e-01,  1.3133e-01,  ...,  4.1371e-01,\n",
      "          -5.6484e-01, -2.7558e-01],\n",
      "         ...,\n",
      "         [ 3.1372e-01, -4.5893e-01,  4.8899e-01,  ...,  3.9571e-01,\n",
      "           6.8488e-01, -1.7076e-01],\n",
      "         [ 3.6315e-01, -4.7315e-01,  4.0396e-01,  ...,  5.4743e-01,\n",
      "           7.7755e-01, -1.0987e-01],\n",
      "         [ 3.3962e-01, -4.0842e-01,  3.0506e-01,  ...,  6.7701e-01,\n",
      "           8.1613e-01, -7.2638e-02]],\n",
      "\n",
      "        [[-8.8274e-02,  2.6955e-02, -9.5245e-03,  ...,  4.9653e-02,\n",
      "           5.0850e-03, -5.6104e-03],\n",
      "         [ 2.6525e-01,  3.2600e-01,  6.5835e-01,  ..., -2.9491e-01,\n",
      "          -2.8703e-02,  3.7253e-01],\n",
      "         [-4.2653e-01,  1.0075e-03, -1.0328e-01,  ...,  3.1825e-01,\n",
      "          -2.0692e-01,  6.6246e-01],\n",
      "         ...,\n",
      "         [ 2.2623e-01, -4.1540e-01,  2.1857e-01,  ...,  3.2334e-01,\n",
      "           7.4748e-01,  2.4327e-01],\n",
      "         [ 3.0765e-01, -2.5362e-01,  2.9857e-01,  ...,  5.5243e-01,\n",
      "           6.4157e-01,  2.8190e-01],\n",
      "         [ 1.6851e-01, -3.1907e-01,  1.0093e-01,  ...,  6.3166e-01,\n",
      "           9.2662e-01,  3.5374e-01]]]), tensor([[[-7.2489e-02,  1.9149e-02, -6.2451e-03,  ...,  1.9407e-02,\n",
      "           1.6205e-03, -2.0579e-02],\n",
      "         [ 1.3441e-01,  7.6795e-01,  9.7875e-01,  ..., -7.5050e-01,\n",
      "          -2.1646e-02,  2.9502e-01],\n",
      "         [-5.9893e-01, -1.0351e-01, -6.3974e-02,  ..., -2.8200e-01,\n",
      "          -3.7646e-01,  4.0520e-01],\n",
      "         ...,\n",
      "         [ 1.9768e-01,  2.7001e-01,  7.8726e-02,  ...,  1.9506e-01,\n",
      "           1.0174e+00,  1.4635e-01],\n",
      "         [ 2.1407e-01,  3.5517e-01,  2.1253e-01,  ...,  2.3887e-01,\n",
      "           6.5839e-01,  2.5303e-01],\n",
      "         [ 2.1254e-01,  2.2900e-01, -1.4247e-01,  ...,  2.7747e-01,\n",
      "           8.6615e-01,  1.9949e-01]],\n",
      "\n",
      "        [[-6.5594e-02,  1.9093e-02, -8.0888e-03,  ...,  2.0949e-02,\n",
      "           7.3208e-04, -1.9643e-02],\n",
      "         [ 1.9333e-01,  9.2098e-01,  9.4952e-01,  ..., -4.7207e-01,\n",
      "           6.3299e-02,  2.7541e-01],\n",
      "         [-5.5282e-01,  1.2790e-01,  3.5274e-02,  ..., -2.8550e-02,\n",
      "          -2.7979e-01,  3.7487e-01],\n",
      "         ...,\n",
      "         [ 1.0476e-01,  1.5379e-01, -1.1488e-01,  ..., -5.6776e-02,\n",
      "           9.5305e-01,  1.1738e-01],\n",
      "         [-8.3034e-03,  3.7851e-01, -1.4773e-02,  ..., -1.3940e-01,\n",
      "           3.9634e-01,  1.9929e-01],\n",
      "         [ 2.7022e-01,  3.1069e-01, -6.1049e-02,  ..., -7.4658e-02,\n",
      "           6.3897e-01,  1.4123e-01]],\n",
      "\n",
      "        [[-6.8306e-02,  2.0306e-02, -7.3639e-03,  ...,  2.2845e-02,\n",
      "           1.9452e-03, -1.6231e-02],\n",
      "         [-2.9569e-01,  1.4194e-01,  2.5211e-01,  ..., -5.1764e-01,\n",
      "          -2.8384e-01,  7.2549e-01],\n",
      "         [-1.5627e-01,  1.1375e-01,  3.4579e-01,  ..., -2.2649e-01,\n",
      "          -3.8729e-01,  1.6343e-01],\n",
      "         ...,\n",
      "         [ 4.1508e-01,  2.1279e-01,  3.0022e-01,  ...,  1.1936e-01,\n",
      "           5.7271e-01,  1.5208e-01],\n",
      "         [ 1.4835e-01,  4.0689e-01,  1.0344e-01,  ..., -4.5930e-02,\n",
      "           5.6959e-01,  8.6228e-02],\n",
      "         [ 2.3928e-01,  4.0162e-01,  5.1736e-03,  ..., -1.0972e-01,\n",
      "           4.7364e-01,  1.6167e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-6.4485e-02,  1.8032e-02, -7.3394e-03,  ...,  1.7868e-02,\n",
      "           9.7192e-04, -1.8844e-02],\n",
      "         [ 3.0527e-01,  4.3765e-02,  6.2646e-01,  ..., -4.4862e-01,\n",
      "           1.6733e-01, -3.0084e-01],\n",
      "         [ 1.0124e+00,  3.7872e-02,  5.6870e-01,  ..., -3.5137e-01,\n",
      "          -1.4826e-01, -1.0934e-01],\n",
      "         ...,\n",
      "         [ 2.4389e-01,  1.9740e-01,  2.5188e-01,  ...,  3.3142e-01,\n",
      "           9.7394e-01,  1.5062e-01],\n",
      "         [ 1.2152e-01,  2.1867e-01,  1.3349e-01,  ...,  3.0492e-01,\n",
      "           1.1847e+00,  1.0326e-01],\n",
      "         [ 1.8339e-01,  1.1924e-01,  1.9188e-01,  ...,  4.8694e-01,\n",
      "           1.1755e+00,  1.2624e-01]],\n",
      "\n",
      "        [[-6.6027e-02,  2.1351e-02, -1.0048e-02,  ...,  2.2962e-02,\n",
      "           4.0842e-03, -1.4148e-02],\n",
      "         [ 1.1910e-01,  3.0675e-01,  2.9606e-01,  ..., -7.5335e-01,\n",
      "          -1.4418e+00,  4.1185e-01],\n",
      "         [-1.8632e-01,  5.6439e-02,  1.4578e-01,  ...,  2.0672e-01,\n",
      "          -4.7558e-01, -1.6122e-01],\n",
      "         ...,\n",
      "         [ 5.2945e-01, -6.7382e-01,  5.4194e-01,  ...,  4.8371e-01,\n",
      "           8.4159e-01, -1.7456e-01],\n",
      "         [ 5.4261e-01, -5.7910e-01,  4.4834e-01,  ...,  4.8678e-01,\n",
      "           9.8116e-01, -1.9955e-01],\n",
      "         [ 5.3723e-01, -5.5993e-01,  3.1221e-01,  ...,  5.5313e-01,\n",
      "           9.9400e-01, -1.8515e-01]],\n",
      "\n",
      "        [[-6.4555e-02,  2.0989e-02, -6.8357e-03,  ...,  1.9244e-02,\n",
      "          -4.4423e-04, -1.5964e-02],\n",
      "         [ 1.0600e-01,  5.4379e-01,  6.6693e-01,  ..., -7.6965e-01,\n",
      "           6.1358e-02,  2.0243e-01],\n",
      "         [-5.6690e-01, -1.8469e-01, -1.5436e-01,  ..., -3.2118e-02,\n",
      "          -2.6684e-01,  3.5643e-01],\n",
      "         ...,\n",
      "         [ 4.2456e-01, -1.5216e-02,  1.7072e-01,  ...,  2.8597e-01,\n",
      "           8.2737e-01,  2.3898e-01],\n",
      "         [ 3.7994e-01,  3.6393e-01,  1.9665e-01,  ...,  2.6688e-01,\n",
      "           5.4406e-01,  2.1056e-01],\n",
      "         [ 3.0839e-01,  2.0919e-01,  3.0813e-02,  ...,  3.6772e-01,\n",
      "           9.8587e-01,  1.9235e-01]]]), tensor([[[-3.6734e-02,  8.6319e-03, -7.2903e-04,  ...,  1.2003e-02,\n",
      "          -1.3715e-03, -1.3877e-02],\n",
      "         [ 3.4117e-02,  3.0287e-01,  4.5873e-01,  ..., -4.8491e-01,\n",
      "          -1.2508e-01,  1.7999e-02],\n",
      "         [-3.1555e-01, -3.4673e-02,  8.4742e-02,  ..., -2.2923e-01,\n",
      "          -2.1967e-01,  2.7455e-02],\n",
      "         ...,\n",
      "         [ 1.0676e-01,  1.5592e-01,  4.6004e-02,  ...,  1.0182e-02,\n",
      "           3.0518e-01, -2.8509e-02],\n",
      "         [ 8.6725e-02,  1.6952e-01,  1.1605e-01,  ..., -7.0990e-03,\n",
      "           1.6469e-01,  3.0242e-02],\n",
      "         [ 8.6562e-02,  1.6181e-01,  3.1028e-04,  ..., -4.7521e-02,\n",
      "           3.3894e-01,  1.2672e-02]],\n",
      "\n",
      "        [[-3.5419e-02,  9.2873e-03, -1.6966e-03,  ...,  1.2503e-02,\n",
      "          -2.1651e-03, -1.2238e-02],\n",
      "         [ 2.5491e-02,  4.5046e-01,  4.4546e-01,  ..., -3.6255e-01,\n",
      "          -2.9727e-02,  9.2169e-02],\n",
      "         [-2.9702e-01,  9.0375e-02,  1.0779e-01,  ...,  8.3403e-02,\n",
      "          -2.0589e-01,  2.2857e-02],\n",
      "         ...,\n",
      "         [ 3.1931e-02,  9.5955e-02, -5.3182e-02,  ..., -6.9919e-02,\n",
      "           3.2908e-01,  2.9189e-02],\n",
      "         [-2.2695e-02,  1.2695e-01,  8.2310e-04,  ..., -2.1327e-02,\n",
      "           6.0888e-02, -4.3807e-02],\n",
      "         [ 4.1678e-02,  1.7811e-01, -2.6452e-02,  ..., -8.8489e-02,\n",
      "           2.0958e-01, -3.4919e-02]],\n",
      "\n",
      "        [[-3.6783e-02,  9.1253e-03, -1.4285e-03,  ...,  1.2207e-02,\n",
      "          -2.0960e-03, -1.2865e-02],\n",
      "         [-2.6580e-01, -5.8496e-02,  1.9636e-01,  ..., -5.6467e-01,\n",
      "          -1.1278e-01,  3.9604e-01],\n",
      "         [-2.1835e-01,  8.5198e-03,  2.0092e-01,  ..., -2.7704e-01,\n",
      "          -2.1240e-01,  1.2808e-01],\n",
      "         ...,\n",
      "         [ 1.9922e-01,  2.6604e-01,  1.4957e-01,  ...,  5.0302e-02,\n",
      "           1.8363e-01,  1.7662e-01],\n",
      "         [ 4.3705e-02,  1.9366e-01,  3.0340e-02,  ..., -3.1763e-03,\n",
      "           1.2702e-01, -4.0951e-02],\n",
      "         [ 5.7049e-02,  2.1417e-01,  2.0924e-02,  ..., -3.5107e-02,\n",
      "           1.2727e-01, -2.5509e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.7186e-02,  8.8164e-03, -1.4936e-03,  ...,  1.0904e-02,\n",
      "          -1.7325e-03, -1.3742e-02],\n",
      "         [ 5.6369e-02, -4.4141e-02,  3.7331e-01,  ..., -4.6092e-01,\n",
      "          -9.9286e-03, -6.6058e-02],\n",
      "         [ 5.0902e-01, -9.3291e-03,  3.8274e-01,  ..., -2.7135e-01,\n",
      "          -1.1596e-01, -8.2998e-02],\n",
      "         ...,\n",
      "         [ 1.1972e-01,  1.3091e-01,  1.3105e-01,  ...,  1.5859e-01,\n",
      "           3.9636e-01,  1.7175e-01],\n",
      "         [ 4.1628e-02,  9.7630e-02,  4.6365e-02,  ...,  6.3665e-02,\n",
      "           4.7453e-01, -3.8389e-02],\n",
      "         [ 8.0448e-02, -1.0848e-02,  1.2481e-01,  ...,  2.0559e-01,\n",
      "           5.4338e-01, -4.8583e-02]],\n",
      "\n",
      "        [[-3.6176e-02,  8.1172e-03, -2.2936e-03,  ...,  1.2429e-02,\n",
      "          -7.4224e-04, -1.3573e-02],\n",
      "         [-5.6736e-02,  5.6227e-02,  1.8314e-01,  ..., -5.5586e-01,\n",
      "          -8.0820e-01,  1.6854e-01],\n",
      "         [-6.6504e-02,  2.0570e-02,  5.3674e-02,  ..., -5.2859e-02,\n",
      "          -2.3335e-01, -4.1090e-02],\n",
      "         ...,\n",
      "         [ 2.6208e-01, -2.3544e-01,  3.4805e-01,  ...,  2.1148e-01,\n",
      "           3.5166e-01,  1.0027e-01],\n",
      "         [ 2.2969e-01, -2.3725e-01,  2.6886e-01,  ...,  2.4361e-01,\n",
      "           4.4454e-01, -3.5410e-02],\n",
      "         [ 2.4237e-01, -1.5929e-01,  2.0731e-01,  ...,  2.4843e-01,\n",
      "           4.3642e-01, -4.4908e-02]],\n",
      "\n",
      "        [[-3.5563e-02,  9.2754e-03, -1.7347e-03,  ...,  1.1789e-02,\n",
      "          -1.7801e-03, -1.3261e-02],\n",
      "         [-1.5304e-02,  3.8654e-01,  3.6954e-01,  ..., -4.5183e-01,\n",
      "          -6.8077e-02, -6.3915e-02],\n",
      "         [-3.5451e-01, -5.2322e-02,  8.2345e-02,  ...,  1.2869e-02,\n",
      "          -1.3913e-01, -1.3590e-01],\n",
      "         ...,\n",
      "         [ 1.9060e-01,  1.0166e-01,  1.2045e-01,  ...,  9.0907e-02,\n",
      "           3.0933e-01,  1.8290e-01],\n",
      "         [ 9.6616e-02,  2.0378e-01,  8.4501e-02,  ...,  1.0222e-01,\n",
      "           1.6628e-01,  3.5184e-03],\n",
      "         [ 1.1900e-01,  1.8047e-01,  4.2640e-02,  ...,  1.0734e-01,\n",
      "           3.7000e-01,  1.7160e-02]]])), encoder_attentions=None)\n",
      "seg_num: 1\n",
      "batch_size: 32\n",
      "non_empty_mask: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "decoder_input_ids: None\n",
      "past_key_values: None\n",
      "out: Seq2SeqLMOutput(loss=tensor(9.3627, grad_fn=<NllLossBackward0>), logits=tensor([[[-2.8339e+00, -4.1880e+00,  5.1398e+00,  ..., -3.9279e+00,\n",
      "          -4.4173e+00,  1.1241e-02],\n",
      "         [-3.5680e+00, -4.0084e+00,  4.8512e+00,  ..., -3.7425e+00,\n",
      "          -4.1579e+00,  9.7010e-02],\n",
      "         [-4.2248e+00, -3.7130e+00,  2.1291e+00,  ..., -2.4078e+00,\n",
      "          -2.9128e+00,  4.2661e-01],\n",
      "         ...,\n",
      "         [-8.8655e+00, -4.6346e+00,  2.6191e+00,  ..., -4.6124e+00,\n",
      "          -4.9202e+00, -3.5391e+00],\n",
      "         [-9.3904e+00, -4.5773e+00,  3.2248e+00,  ..., -4.8941e+00,\n",
      "          -5.0748e+00, -2.8014e+00],\n",
      "         [-1.0730e+01, -4.3379e+00,  3.2923e+00,  ..., -4.6992e+00,\n",
      "          -4.9636e+00, -2.2724e+00]],\n",
      "\n",
      "        [[-2.2198e+00, -4.6325e+00,  4.4943e+00,  ..., -4.4631e+00,\n",
      "          -4.9463e+00, -7.8256e-01],\n",
      "         [-3.0399e+00, -4.4600e+00,  4.1710e+00,  ..., -4.3813e+00,\n",
      "          -4.8030e+00, -8.9906e-01],\n",
      "         [-2.5886e+00, -3.7189e+00,  4.1224e+00,  ..., -3.0608e+00,\n",
      "          -3.4439e+00, -5.0627e-01],\n",
      "         ...,\n",
      "         [-8.8380e+00, -4.1138e+00,  3.7877e+00,  ..., -3.7960e+00,\n",
      "          -4.0309e+00, -1.6621e+00],\n",
      "         [-9.4767e+00, -4.2661e+00,  3.8598e+00,  ..., -4.0280e+00,\n",
      "          -4.2044e+00, -1.0630e+00],\n",
      "         [-1.0230e+01, -4.1876e+00,  3.7419e+00,  ..., -3.8832e+00,\n",
      "          -4.1809e+00, -1.3515e+00]],\n",
      "\n",
      "        [[-3.1875e+00, -4.5022e+00,  3.6319e+00,  ..., -4.1309e+00,\n",
      "          -4.6409e+00, -3.7341e-01],\n",
      "         [-3.6603e+00, -4.2593e+00,  3.6505e+00,  ..., -4.0443e+00,\n",
      "          -4.4800e+00, -3.0936e-01],\n",
      "         [-3.4044e+00, -3.5214e+00,  2.1613e+00,  ..., -2.7385e+00,\n",
      "          -2.9929e+00,  1.1472e+00],\n",
      "         ...,\n",
      "         [-7.8392e+00, -3.7906e+00,  1.5433e+00,  ..., -4.3897e+00,\n",
      "          -4.2904e+00, -2.9675e+00],\n",
      "         [-8.3815e+00, -3.8823e+00,  1.3908e+00,  ..., -4.4918e+00,\n",
      "          -4.4311e+00, -2.6545e+00],\n",
      "         [-8.3295e+00, -4.0638e+00,  9.4895e-01,  ..., -4.4515e+00,\n",
      "          -4.4513e+00, -2.4193e+00]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.1787e+00, -4.5348e+00,  4.8278e+00,  ..., -4.4150e+00,\n",
      "          -4.8864e+00,  2.2256e-01],\n",
      "         [-1.8287e+00, -4.2645e+00,  4.2668e+00,  ..., -4.0433e+00,\n",
      "          -4.5094e+00,  1.2119e-01],\n",
      "         [-8.5476e-02, -2.3948e+00,  4.8318e+00,  ..., -1.7451e+00,\n",
      "          -2.0216e+00, -6.8212e-01],\n",
      "         ...,\n",
      "         [-5.6350e+00, -3.7388e+00,  1.7976e+00,  ..., -4.3909e+00,\n",
      "          -4.0945e+00, -1.7299e+00],\n",
      "         [-6.0337e+00, -3.7710e+00,  1.4982e+00,  ..., -4.5384e+00,\n",
      "          -4.2364e+00, -1.5256e+00],\n",
      "         [-5.9833e+00, -3.9159e+00,  1.1307e+00,  ..., -4.5970e+00,\n",
      "          -4.3797e+00, -1.5946e+00]],\n",
      "\n",
      "        [[-2.4837e+00, -4.6692e+00,  3.8812e+00,  ..., -4.8785e+00,\n",
      "          -5.1526e+00, -3.8478e-01],\n",
      "         [-2.7725e+00, -4.4655e+00,  3.5449e+00,  ..., -4.6628e+00,\n",
      "          -4.8364e+00, -6.9641e-01],\n",
      "         [-7.4439e+00, -5.0795e+00,  3.4055e-01,  ..., -5.0054e+00,\n",
      "          -5.1646e+00, -6.4889e-01],\n",
      "         ...,\n",
      "         [-7.0701e+00, -4.3879e+00,  1.1092e+00,  ..., -4.7704e+00,\n",
      "          -4.5158e+00, -1.0603e+00],\n",
      "         [-7.5060e+00, -4.3545e+00,  8.3304e-01,  ..., -4.7542e+00,\n",
      "          -4.5578e+00, -6.3607e-01],\n",
      "         [-7.2531e+00, -4.5191e+00,  6.3233e-01,  ..., -4.9233e+00,\n",
      "          -4.7859e+00, -5.7651e-01]],\n",
      "\n",
      "        [[-2.4118e+00, -4.6887e+00,  3.9509e+00,  ..., -4.8658e+00,\n",
      "          -5.4757e+00, -1.7581e+00],\n",
      "         [-3.0537e+00, -4.6271e+00,  3.8972e+00,  ..., -4.7468e+00,\n",
      "          -5.3214e+00, -1.7465e+00],\n",
      "         [-3.8884e+00, -4.0338e+00,  3.9490e+00,  ..., -4.3629e+00,\n",
      "          -4.5591e+00, -2.1484e+00],\n",
      "         ...,\n",
      "         [-1.2697e+01, -5.0057e+00,  1.3369e+01,  ..., -5.7864e+00,\n",
      "          -5.7013e+00, -4.6078e+00],\n",
      "         [-1.1949e+01, -5.2580e+00,  1.3802e+01,  ..., -5.6352e+00,\n",
      "          -5.6301e+00, -3.7663e+00],\n",
      "         [-1.2164e+01, -4.9814e+00,  1.2428e+01,  ..., -5.5050e+00,\n",
      "          -5.5106e+00, -4.4270e+00]]], grad_fn=<AddBackward0>), past_key_values=None, decoder_hidden_states=(tensor([[[ 5.4487e-01, -2.0410e-01,  1.1865e-01,  ...,  2.6924e-01,\n",
      "          -1.1948e-01,  1.9697e-01],\n",
      "         [ 5.9953e-01, -1.8683e-01,  1.0440e-01,  ...,  1.6923e-01,\n",
      "          -2.8865e-03,  2.1166e-01],\n",
      "         [-2.7799e-01,  2.8828e-01, -7.3028e-01,  ..., -1.1586e+00,\n",
      "           1.8877e-01, -2.9742e-02],\n",
      "         ...,\n",
      "         [ 3.5735e-01,  1.4292e-01,  4.5212e-02,  ..., -9.3843e-02,\n",
      "          -2.7352e-02, -2.3044e-01],\n",
      "         [ 1.4780e-01, -4.1068e-02,  6.6296e-02,  ...,  1.4331e-01,\n",
      "          -3.3200e-01, -3.0151e-01],\n",
      "         [ 2.8179e-01, -2.3902e-01,  3.1419e-03,  ...,  2.1478e-01,\n",
      "          -6.5562e-01, -2.9937e-01]],\n",
      "\n",
      "        [[ 5.4487e-01, -2.0410e-01,  1.1865e-01,  ...,  2.6924e-01,\n",
      "          -1.1948e-01,  1.9697e-01],\n",
      "         [ 5.9953e-01, -1.8683e-01,  1.0440e-01,  ...,  1.6923e-01,\n",
      "          -2.8865e-03,  2.1166e-01],\n",
      "         [-4.1811e-02, -2.1991e-01,  3.6687e-01,  ...,  4.9226e-01,\n",
      "           8.5032e-01, -1.1033e-01],\n",
      "         ...,\n",
      "         [ 3.5735e-01,  1.4292e-01,  4.5212e-02,  ..., -9.3843e-02,\n",
      "          -2.7352e-02, -2.3044e-01],\n",
      "         [ 1.4780e-01, -4.1068e-02,  6.6296e-02,  ...,  1.4331e-01,\n",
      "          -3.3200e-01, -3.0151e-01],\n",
      "         [ 2.8179e-01, -2.3902e-01,  3.1419e-03,  ...,  2.1478e-01,\n",
      "          -6.5562e-01, -2.9937e-01]],\n",
      "\n",
      "        [[ 5.4487e-01, -2.0410e-01,  1.1865e-01,  ...,  2.6924e-01,\n",
      "          -1.1948e-01,  1.9697e-01],\n",
      "         [ 5.9953e-01, -1.8683e-01,  1.0440e-01,  ...,  1.6923e-01,\n",
      "          -2.8865e-03,  2.1166e-01],\n",
      "         [ 7.9545e-01, -1.5292e-01,  4.2505e-02,  ...,  1.6360e-01,\n",
      "          -4.8583e-01,  7.2543e-02],\n",
      "         ...,\n",
      "         [ 3.5735e-01,  1.4292e-01,  4.5212e-02,  ..., -9.3843e-02,\n",
      "          -2.7352e-02, -2.3044e-01],\n",
      "         [ 1.4780e-01, -4.1068e-02,  6.6296e-02,  ...,  1.4331e-01,\n",
      "          -3.3200e-01, -3.0151e-01],\n",
      "         [ 2.8179e-01, -2.3902e-01,  3.1419e-03,  ...,  2.1478e-01,\n",
      "          -6.5562e-01, -2.9937e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 5.4487e-01, -2.0410e-01,  1.1865e-01,  ...,  2.6924e-01,\n",
      "          -1.1948e-01,  1.9697e-01],\n",
      "         [ 5.9953e-01, -1.8683e-01,  1.0440e-01,  ...,  1.6923e-01,\n",
      "          -2.8865e-03,  2.1166e-01],\n",
      "         [ 3.0398e-01,  2.7637e-01, -3.5502e-01,  ...,  1.8763e-01,\n",
      "          -7.9256e-04, -2.0805e-01],\n",
      "         ...,\n",
      "         [ 3.5735e-01,  1.4292e-01,  4.5212e-02,  ..., -9.3843e-02,\n",
      "          -2.7352e-02, -2.3044e-01],\n",
      "         [ 1.4780e-01, -4.1068e-02,  6.6296e-02,  ...,  1.4331e-01,\n",
      "          -3.3200e-01, -3.0151e-01],\n",
      "         [ 2.8179e-01, -2.3902e-01,  3.1419e-03,  ...,  2.1478e-01,\n",
      "          -6.5562e-01, -2.9937e-01]],\n",
      "\n",
      "        [[ 5.4487e-01, -2.0410e-01,  1.1865e-01,  ...,  2.6924e-01,\n",
      "          -1.1948e-01,  1.9697e-01],\n",
      "         [ 5.9953e-01, -1.8683e-01,  1.0440e-01,  ...,  1.6923e-01,\n",
      "          -2.8865e-03,  2.1166e-01],\n",
      "         [-1.7130e-01,  3.7637e-01,  9.2002e-02,  ..., -5.6459e-01,\n",
      "          -5.7175e-01, -4.7302e-01],\n",
      "         ...,\n",
      "         [ 3.5735e-01,  1.4292e-01,  4.5212e-02,  ..., -9.3843e-02,\n",
      "          -2.7352e-02, -2.3044e-01],\n",
      "         [ 1.4780e-01, -4.1068e-02,  6.6296e-02,  ...,  1.4331e-01,\n",
      "          -3.3200e-01, -3.0151e-01],\n",
      "         [ 2.8179e-01, -2.3902e-01,  3.1419e-03,  ...,  2.1478e-01,\n",
      "          -6.5562e-01, -2.9937e-01]],\n",
      "\n",
      "        [[ 5.4487e-01, -2.0410e-01,  1.1865e-01,  ...,  2.6924e-01,\n",
      "          -1.1948e-01,  1.9697e-01],\n",
      "         [ 5.9953e-01, -1.8683e-01,  1.0440e-01,  ...,  1.6923e-01,\n",
      "          -2.8865e-03,  2.1166e-01],\n",
      "         [-7.2732e-01,  2.1149e-02, -4.7241e-01,  ..., -2.9487e-01,\n",
      "          -2.7592e-01, -5.3279e-01],\n",
      "         ...,\n",
      "         [ 3.5735e-01,  1.4292e-01,  4.5212e-02,  ..., -9.3843e-02,\n",
      "          -2.7352e-02, -2.3044e-01],\n",
      "         [ 1.4780e-01, -4.1068e-02,  6.6296e-02,  ...,  1.4331e-01,\n",
      "          -3.3200e-01, -3.0151e-01],\n",
      "         [ 2.8179e-01, -2.3902e-01,  3.1419e-03,  ...,  2.1478e-01,\n",
      "          -6.5562e-01, -2.9937e-01]]]), tensor([[[ 0.8177, -0.6094,  0.1746,  ..., -0.4937, -0.6662,  0.1349],\n",
      "         [ 0.8242, -0.5230,  0.0210,  ..., -0.5922, -0.7126,  0.0549],\n",
      "         [-0.1082,  0.2678, -0.3901,  ..., -0.2235, -0.2935, -0.3488],\n",
      "         ...,\n",
      "         [ 0.2544, -0.1317,  0.3314,  ...,  0.1465, -0.1430, -0.1641],\n",
      "         [ 0.1885, -0.1789,  0.3443,  ...,  0.3345, -0.2932, -0.2502],\n",
      "         [ 0.2499, -0.2427,  0.2215,  ...,  0.4331, -0.5685, -0.1282]],\n",
      "\n",
      "        [[ 0.8309, -0.7426,  0.3002,  ..., -0.3325, -0.5250,  0.1782],\n",
      "         [ 0.8480, -0.5762,  0.0355,  ..., -0.3942, -0.5186,  0.1310],\n",
      "         [ 0.0516, -0.4715,  0.3517,  ..., -0.0905,  0.0189,  0.0294],\n",
      "         ...,\n",
      "         [ 0.3090, -0.1461,  0.2909,  ...,  0.1743, -0.1508, -0.1559],\n",
      "         [ 0.2446, -0.1677,  0.2951,  ...,  0.3550, -0.3070, -0.2435],\n",
      "         [ 0.3289, -0.2782,  0.2143,  ...,  0.4851, -0.6372, -0.1309]],\n",
      "\n",
      "        [[ 0.7244, -0.7349,  0.3547,  ..., -0.4997, -0.7456,  0.1880],\n",
      "         [ 0.7824, -0.6361,  0.1712,  ..., -0.5330, -0.7894,  0.1564],\n",
      "         [ 0.3924,  0.4659,  0.1106,  ...,  0.2561, -0.1611,  0.1239],\n",
      "         ...,\n",
      "         [ 0.0657, -0.1594,  0.1858,  ...,  0.3112, -0.3392, -0.2444],\n",
      "         [ 0.0061, -0.1710,  0.1939,  ...,  0.5205, -0.5048, -0.3485],\n",
      "         [ 0.0869, -0.2967,  0.1336,  ...,  0.6176, -0.7984, -0.2178]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.8318, -0.5231,  0.3791,  ..., -0.4031, -0.4696,  0.2607],\n",
      "         [ 0.8592, -0.3877,  0.1483,  ..., -0.4840, -0.5369,  0.2581],\n",
      "         [ 0.2080,  0.3219, -0.1685,  ...,  0.1763, -0.3115, -0.6361],\n",
      "         ...,\n",
      "         [ 0.1745, -0.0590,  0.3440,  ...,  0.2843, -0.3146, -0.2953],\n",
      "         [ 0.1174, -0.0878,  0.3380,  ...,  0.4825, -0.4529, -0.3828],\n",
      "         [ 0.2017, -0.1997,  0.2544,  ...,  0.5852, -0.7270, -0.2658]],\n",
      "\n",
      "        [[ 0.9496, -0.8648,  0.5266,  ..., -0.2207, -0.5577,  0.1862],\n",
      "         [ 0.9208, -0.7088,  0.2920,  ..., -0.3601, -0.5335,  0.2297],\n",
      "         [-0.1324, -0.0022,  0.6419,  ..., -0.4120, -0.7361, -0.1365],\n",
      "         ...,\n",
      "         [ 0.1538, -0.0680,  0.0991,  ...,  0.3041, -0.3195, -0.2128],\n",
      "         [ 0.1329, -0.0804,  0.1051,  ...,  0.5447, -0.5211, -0.3197],\n",
      "         [ 0.1891, -0.2009,  0.0685,  ...,  0.6209, -0.7821, -0.2220]],\n",
      "\n",
      "        [[ 0.7871, -0.7451,  0.1484,  ..., -0.4389, -0.5039,  0.0979],\n",
      "         [ 0.7102, -0.6218, -0.0106,  ..., -0.4922, -0.5840,  0.0352],\n",
      "         [-0.8181, -0.4240, -0.2331,  ..., -0.2083, -0.4541, -0.4052],\n",
      "         ...,\n",
      "         [ 0.3153, -0.1667,  0.3037,  ...,  0.2233, -0.1945, -0.1546],\n",
      "         [ 0.2392, -0.1694,  0.3051,  ...,  0.3963, -0.3253, -0.2448],\n",
      "         [ 0.3217, -0.2819,  0.2207,  ...,  0.5480, -0.6390, -0.1422]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0460, -0.5507, -0.0342,  ..., -0.6348, -0.2056,  0.5845],\n",
      "         [ 0.1062, -0.2538, -0.4240,  ..., -0.8430, -0.4171,  0.2628],\n",
      "         [-0.2143,  0.5428, -0.1579,  ..., -0.1935,  0.0598,  0.2524],\n",
      "         ...,\n",
      "         [ 0.1570,  0.1094, -0.3391,  ..., -0.2417, -0.2382,  0.3576],\n",
      "         [ 0.2472,  0.1620, -0.2878,  ..., -0.2901, -0.1887,  0.3936],\n",
      "         [ 0.0702,  0.0991, -0.1535,  ..., -0.1239, -0.1615,  0.4466]],\n",
      "\n",
      "        [[ 0.3759, -0.8123, -0.0747,  ..., -0.7456,  0.0519,  0.5006],\n",
      "         [ 0.3245, -0.5741, -0.4032,  ..., -0.8267,  0.0145,  0.2463],\n",
      "         [ 0.0086, -0.1432,  0.2629,  ..., -0.2068,  0.6823,  0.0488],\n",
      "         ...,\n",
      "         [ 0.1102,  0.0309, -0.3332,  ..., -0.2758, -0.2231,  0.3254],\n",
      "         [ 0.2271,  0.0395, -0.3426,  ..., -0.2613, -0.1174,  0.2542],\n",
      "         [ 0.1194,  0.0103, -0.3327,  ..., -0.2467, -0.0942,  0.2661]],\n",
      "\n",
      "        [[-0.0368, -0.8653,  0.0975,  ..., -0.4969, -0.3580,  0.4638],\n",
      "         [ 0.0728, -0.6183, -0.2746,  ..., -0.4766, -0.4376,  0.1504],\n",
      "         [ 0.0819,  0.2300, -0.2259,  ...,  0.4192, -0.2569,  0.0364],\n",
      "         ...,\n",
      "         [ 0.0107,  0.2992,  0.1979,  ..., -0.3943, -0.4371,  0.1803],\n",
      "         [ 0.1091,  0.3198,  0.1740,  ..., -0.4223, -0.3231,  0.1204],\n",
      "         [-0.0316,  0.3294,  0.2064,  ..., -0.3921, -0.1925,  0.0364]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.3544, -0.5544,  0.2209,  ..., -0.4987, -0.0421,  0.6211],\n",
      "         [ 0.3538, -0.3391, -0.2224,  ..., -0.6302, -0.1734,  0.4340],\n",
      "         [-0.0473,  0.6051, -0.2227,  ..., -0.1813,  0.5181, -0.1149],\n",
      "         ...,\n",
      "         [ 0.1019,  0.4386,  0.1992,  ..., -0.4149, -0.3737,  0.1632],\n",
      "         [ 0.2256,  0.4593,  0.1969,  ..., -0.4411, -0.2734,  0.1130],\n",
      "         [ 0.1192,  0.4709,  0.1569,  ..., -0.3930, -0.1303,  0.0362]],\n",
      "\n",
      "        [[ 0.3018, -0.8075,  0.1941,  ..., -0.5226, -0.1913,  0.7082],\n",
      "         [ 0.1593, -0.5278, -0.1230,  ..., -0.5153, -0.3054,  0.4612],\n",
      "         [-0.1060,  0.7678,  0.5940,  ..., -0.6241,  0.2618,  0.1035],\n",
      "         ...,\n",
      "         [ 0.0323,  0.4600,  0.1270,  ..., -0.4667, -0.4013,  0.2151],\n",
      "         [ 0.1631,  0.4365,  0.1536,  ..., -0.4345, -0.3244,  0.1616],\n",
      "         [-0.0123,  0.4231,  0.1677,  ..., -0.4147, -0.2298,  0.0381]],\n",
      "\n",
      "        [[ 0.2580, -0.3186, -0.1467,  ..., -0.6126, -0.0060,  0.5709],\n",
      "         [ 0.0996, -0.1481, -0.3466,  ..., -0.7584, -0.0465,  0.3114],\n",
      "         [-0.3307,  0.1355, -0.1009,  ..., -0.4786, -0.1745,  0.1194],\n",
      "         ...,\n",
      "         [ 0.1720, -0.0446, -0.2380,  ..., -0.0988, -0.3423,  0.2340],\n",
      "         [ 0.2758, -0.0400, -0.1825,  ..., -0.0284, -0.2237,  0.1975],\n",
      "         [ 0.2719, -0.0847, -0.1907,  ..., -0.0185, -0.2446,  0.2777]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 6.6994e-01, -2.9412e-01,  4.1404e-01,  ..., -2.9001e-01,\n",
      "           3.1339e-01,  2.8256e-01],\n",
      "         [ 7.2506e-01, -1.3628e-01,  4.4727e-06,  ..., -3.9130e-01,\n",
      "           3.6477e-01,  4.5748e-01],\n",
      "         [ 2.2264e-02,  4.0924e-01, -4.4171e-01,  ...,  1.6144e-01,\n",
      "          -3.6181e-01,  3.2772e-01],\n",
      "         ...,\n",
      "         [ 1.1623e-01,  8.7376e-02, -5.5142e-01,  ..., -3.9173e-01,\n",
      "          -3.1150e-01,  5.4974e-01],\n",
      "         [ 1.8532e-01,  1.4655e-01, -6.6066e-01,  ..., -3.3628e-01,\n",
      "          -4.3028e-01,  4.6432e-01],\n",
      "         [ 1.4382e-01,  1.2802e-01, -5.9368e-01,  ..., -1.0620e-01,\n",
      "          -5.4837e-01,  6.0650e-01]],\n",
      "\n",
      "        [[ 8.4073e-01, -7.3253e-01,  2.9702e-01,  ..., -2.7868e-01,\n",
      "           5.6776e-01,  8.1798e-01],\n",
      "         [ 8.2489e-01, -5.7380e-01,  7.6293e-03,  ..., -4.1258e-01,\n",
      "           6.8677e-01,  7.2123e-01],\n",
      "         [ 6.6946e-01, -5.0693e-01, -2.2691e-01,  ...,  2.9173e-02,\n",
      "           1.8031e-01,  3.3937e-01],\n",
      "         ...,\n",
      "         [-3.0986e-03,  1.4452e-01, -3.3292e-01,  ..., -3.6063e-01,\n",
      "          -3.5572e-01,  4.4167e-01],\n",
      "         [ 2.0894e-01,  1.3398e-01, -3.8052e-01,  ..., -3.0108e-01,\n",
      "          -4.0566e-01,  2.8433e-01],\n",
      "         [ 2.2365e-01,  1.7098e-01, -3.8225e-01,  ..., -1.5016e-01,\n",
      "          -4.7373e-01,  2.7159e-01]],\n",
      "\n",
      "        [[ 6.0422e-01, -4.7721e-01,  4.3938e-01,  ...,  8.8376e-03,\n",
      "           4.4451e-01,  5.2686e-01],\n",
      "         [ 7.0114e-01, -3.7239e-01,  1.1665e-01,  ..., -6.7494e-02,\n",
      "           5.3081e-01,  5.0145e-01],\n",
      "         [ 1.9058e-01,  2.6534e-01, -1.0473e-01,  ...,  8.3187e-01,\n",
      "          -2.5193e-01,  2.5555e-01],\n",
      "         ...,\n",
      "         [-1.3732e-02,  1.6827e-01, -6.3588e-02,  ..., -5.1261e-02,\n",
      "          -5.0977e-01,  4.6101e-01],\n",
      "         [ 1.7669e-01,  2.8174e-01, -1.7584e-01,  ..., -1.0514e-01,\n",
      "          -5.5632e-01,  3.3670e-01],\n",
      "         [ 9.7385e-02,  2.5580e-01, -2.3110e-01,  ...,  3.1276e-02,\n",
      "          -4.7011e-01,  3.6852e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 8.9059e-01, -5.0175e-01,  4.5332e-01,  ..., -2.4973e-01,\n",
      "           3.4880e-01,  6.5817e-01],\n",
      "         [ 8.2217e-01, -3.2300e-01,  8.6649e-02,  ..., -3.8461e-01,\n",
      "           3.8357e-01,  6.5181e-01],\n",
      "         [ 9.1324e-03,  2.9477e-01, -2.3274e-01,  ..., -7.1945e-02,\n",
      "           1.8116e-02,  3.3618e-01],\n",
      "         ...,\n",
      "         [-5.1656e-02,  2.8528e-01, -1.8299e-01,  ..., -1.0619e-01,\n",
      "          -4.9234e-01,  1.5548e-01],\n",
      "         [ 1.3750e-01,  3.4901e-01, -2.3010e-01,  ..., -1.3202e-01,\n",
      "          -5.3670e-01,  8.5969e-03],\n",
      "         [ 1.2771e-01,  3.6303e-01, -2.4930e-01,  ..., -2.8311e-02,\n",
      "          -4.5957e-01,  5.9810e-02]],\n",
      "\n",
      "        [[ 8.6502e-01, -6.7084e-01,  5.9353e-01,  ..., -2.3356e-01,\n",
      "           9.3423e-02,  5.8608e-01],\n",
      "         [ 9.8200e-01, -3.9195e-01,  3.8288e-01,  ..., -2.8883e-01,\n",
      "           1.7323e-01,  5.0508e-01],\n",
      "         [-1.1354e-01,  4.0352e-01,  4.6690e-01,  ...,  3.2114e-02,\n",
      "          -1.4115e-01,  4.8072e-01],\n",
      "         ...,\n",
      "         [ 7.6927e-02,  4.5027e-01, -1.3028e-01,  ..., -5.7322e-02,\n",
      "          -5.1056e-01,  1.8650e-01],\n",
      "         [ 2.7561e-01,  5.1444e-01, -1.6088e-01,  ..., -6.8796e-02,\n",
      "          -6.1163e-01,  7.2217e-02],\n",
      "         [ 1.4974e-01,  4.9234e-01, -2.0846e-01,  ...,  6.0785e-02,\n",
      "          -5.4598e-01,  1.3754e-01]],\n",
      "\n",
      "        [[ 6.8324e-01, -3.6790e-01,  1.9614e-01,  ..., -5.0395e-01,\n",
      "           2.1839e-01,  5.1307e-01],\n",
      "         [ 8.6151e-01, -2.3298e-01,  1.0977e-02,  ..., -5.7588e-01,\n",
      "           3.6883e-01,  4.4343e-01],\n",
      "         [-3.3246e-01, -3.6400e-01, -5.1351e-01,  ..., -1.0317e-01,\n",
      "          -5.6607e-01,  4.3935e-01],\n",
      "         ...,\n",
      "         [-9.0532e-02,  2.4092e-01, -4.5868e-01,  ..., -3.8505e-01,\n",
      "          -4.0542e-01,  3.6181e-01],\n",
      "         [-3.1739e-02,  1.7718e-01, -4.2993e-01,  ..., -2.8058e-01,\n",
      "          -3.6834e-01,  1.5932e-01],\n",
      "         [ 3.9348e-02,  1.8285e-01, -4.1700e-01,  ..., -1.1050e-01,\n",
      "          -4.8037e-01,  2.5389e-01]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0233, -0.2012,  1.1058,  ...,  0.0645, -0.3479, -0.2321],\n",
      "         [-0.1139, -0.0530,  0.7290,  ...,  0.0608, -0.3247, -0.1205],\n",
      "         [-0.3364,  0.1397,  0.2355,  ...,  0.0664, -0.3445,  0.4728],\n",
      "         ...,\n",
      "         [ 0.2717,  0.2016, -0.2003,  ..., -0.5510, -0.5706, -0.1198],\n",
      "         [ 0.0852,  0.2168, -0.2901,  ..., -0.6348, -0.5709, -0.3095],\n",
      "         [-0.2318,  0.1375, -0.3598,  ..., -0.5379, -0.6036, -0.4335]],\n",
      "\n",
      "        [[-0.3907, -0.4100,  0.8230,  ...,  0.0964, -0.2883, -0.1393],\n",
      "         [-0.3479, -0.2824,  0.6090,  ...,  0.1456, -0.2465, -0.2005],\n",
      "         [-0.3789, -0.6320,  0.2812,  ...,  0.1861, -0.3635,  0.2886],\n",
      "         ...,\n",
      "         [ 0.0165,  0.1176, -0.0318,  ..., -0.5226, -0.5874, -0.3057],\n",
      "         [ 0.1007,  0.0883, -0.1172,  ..., -0.5553, -0.6151, -0.3767],\n",
      "         [ 0.1325,  0.1212, -0.1541,  ..., -0.4429, -0.6280, -0.3790]],\n",
      "\n",
      "        [[-0.2935, -0.3891,  0.9147,  ...,  0.2835, -0.3284, -0.0774],\n",
      "         [-0.2741, -0.2268,  0.6548,  ...,  0.2336, -0.3124, -0.0593],\n",
      "         [-0.4167,  0.1356,  0.2630,  ...,  0.7751, -0.2988,  0.0775],\n",
      "         ...,\n",
      "         [-0.0161,  0.4215,  0.2460,  ..., -0.3463, -0.4669, -0.1843],\n",
      "         [-0.0206,  0.4640,  0.2085,  ..., -0.4154, -0.5148, -0.2539],\n",
      "         [ 0.0150,  0.4464,  0.1837,  ..., -0.3719, -0.4611, -0.3996]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.1920, -0.3993,  1.0040,  ...,  0.1421, -0.3034, -0.1303],\n",
      "         [-0.1501, -0.1165,  0.6959,  ...,  0.1013, -0.2896, -0.1362],\n",
      "         [-0.3354,  0.0255,  0.2353,  ..., -0.3290, -0.4355,  0.3028],\n",
      "         ...,\n",
      "         [ 0.0565,  0.5175,  0.0338,  ..., -0.4079, -0.4741, -0.2431],\n",
      "         [ 0.0612,  0.5220, -0.0728,  ..., -0.3907, -0.5374, -0.2605],\n",
      "         [ 0.1052,  0.6385, -0.0769,  ..., -0.3060, -0.5124, -0.3394]],\n",
      "\n",
      "        [[-0.1105, -0.5601,  0.9924,  ..., -0.1084, -0.4426, -0.3863],\n",
      "         [ 0.0721, -0.3126,  0.8732,  ...,  0.0070, -0.3987, -0.3259],\n",
      "         [-0.2926,  0.2037,  0.4139,  ...,  0.2087, -0.3483,  0.2913],\n",
      "         ...,\n",
      "         [-0.1003,  0.4432,  0.0805,  ..., -0.5151, -0.4492, -0.2164],\n",
      "         [-0.0912,  0.3476,  0.0159,  ..., -0.5561, -0.5289, -0.2203],\n",
      "         [-0.1575,  0.3708, -0.0181,  ..., -0.4303, -0.4698, -0.2883]],\n",
      "\n",
      "        [[-0.1750, -0.3913,  0.8222,  ..., -0.0398, -0.3460, -0.1544],\n",
      "         [-0.0349, -0.2005,  0.6616,  ..., -0.0455, -0.2956, -0.1770],\n",
      "         [-0.5213, -0.3437,  0.0786,  ...,  0.0620, -0.5171,  0.0017],\n",
      "         ...,\n",
      "         [-0.3336,  0.0254, -0.4494,  ..., -0.6624, -0.5438, -0.3855],\n",
      "         [-0.1912, -0.0353, -0.5206,  ..., -0.6630, -0.5033, -0.4034],\n",
      "         [-0.1247,  0.0026, -0.5856,  ..., -0.5063, -0.5569, -0.4102]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0306,  0.0431,  0.8228,  ..., -0.2610, -0.2095,  0.1312],\n",
      "         [-0.1632,  0.1364,  0.5296,  ..., -0.1060, -0.1337,  0.0155],\n",
      "         [-0.0657,  0.1953,  0.5430,  ..., -0.1617, -0.1933,  0.6493],\n",
      "         ...,\n",
      "         [ 0.1187,  0.6439, -0.4288,  ..., -0.5561, -0.2961, -0.5008],\n",
      "         [ 0.0768,  0.6972, -0.4553,  ..., -0.6329, -0.2729, -0.4464],\n",
      "         [-0.1193,  0.6898, -0.4681,  ..., -0.5448, -0.2994, -0.3773]],\n",
      "\n",
      "        [[-0.3152, -0.2267,  0.4486,  ..., -0.2435, -0.1550,  0.1469],\n",
      "         [-0.3439, -0.1171,  0.2856,  ..., -0.0207, -0.0768, -0.0896],\n",
      "         [-0.0926, -0.3706,  0.3710,  ...,  0.2184, -0.1425,  0.3495],\n",
      "         ...,\n",
      "         [-0.2991,  0.6207, -0.2562,  ..., -0.6035, -0.2780, -0.5770],\n",
      "         [-0.2276,  0.5936, -0.3261,  ..., -0.6409, -0.2839, -0.5792],\n",
      "         [-0.2454,  0.6481, -0.3727,  ..., -0.6201, -0.2620, -0.5817]],\n",
      "\n",
      "        [[-0.2449, -0.2304,  0.5769,  ..., -0.1648, -0.1728,  0.3062],\n",
      "         [-0.2383, -0.1062,  0.4745,  ..., -0.1198, -0.1605,  0.2101],\n",
      "         [-0.0823,  0.2119,  0.3828,  ...,  0.3955, -0.1511,  0.3188],\n",
      "         ...,\n",
      "         [-0.3169,  0.5638, -0.0360,  ..., -0.4337, -0.2176, -0.3450],\n",
      "         [-0.3479,  0.5735, -0.1331,  ..., -0.4218, -0.2489, -0.2831],\n",
      "         [-0.3482,  0.4964, -0.1268,  ..., -0.3177, -0.2239, -0.3557]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0971, -0.2486,  0.5745,  ..., -0.3551, -0.1616,  0.0597],\n",
      "         [-0.1207,  0.0618,  0.4963,  ..., -0.2769, -0.0795, -0.1021],\n",
      "         [ 0.2539,  0.0911,  0.1623,  ..., -0.4405, -0.0886,  0.1432],\n",
      "         ...,\n",
      "         [-0.0804,  0.5911, -0.2293,  ..., -0.6095, -0.1486, -0.4267],\n",
      "         [-0.1197,  0.5787, -0.4073,  ..., -0.5679, -0.1882, -0.3078],\n",
      "         [-0.1441,  0.6097, -0.4198,  ..., -0.4835, -0.1757, -0.4174]],\n",
      "\n",
      "        [[ 0.0038, -0.3770,  0.6321,  ..., -0.4808, -0.2565,  0.1146],\n",
      "         [ 0.0996, -0.1573,  0.7202,  ..., -0.3600, -0.1522,  0.1207],\n",
      "         [-0.2128,  0.2207,  0.1150,  ..., -0.0216, -0.1405,  0.2882],\n",
      "         ...,\n",
      "         [-0.3180,  0.4745, -0.0209,  ..., -0.4454, -0.2104, -0.2458],\n",
      "         [-0.3467,  0.4241, -0.1282,  ..., -0.4791, -0.2579, -0.2296],\n",
      "         [-0.3748,  0.3898, -0.1698,  ..., -0.3880, -0.2366, -0.2457]],\n",
      "\n",
      "        [[-0.1083, -0.1650,  0.5280,  ..., -0.4121, -0.1604,  0.1440],\n",
      "         [-0.1211,  0.0365,  0.5161,  ..., -0.3018, -0.0896, -0.0042],\n",
      "         [-0.1589, -0.1707,  0.1098,  ..., -0.1796, -0.2544, -0.1206],\n",
      "         ...,\n",
      "         [-0.3171,  0.1795, -0.9397,  ..., -0.6332, -0.2661,  0.0845],\n",
      "         [-0.1146,  0.0532, -0.9974,  ..., -0.6202, -0.2567, -0.0514],\n",
      "         [-0.0444,  0.1624, -1.0465,  ..., -0.5637, -0.2403, -0.0377]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 1.6015, -0.6591,  0.8172,  ..., -1.4320,  1.0564,  0.7214],\n",
      "         [ 1.2367, -0.7794,  0.5586,  ..., -0.5861,  1.3261,  0.1885],\n",
      "         [ 2.2517, -0.3394,  1.0571,  ..., -0.9662,  0.5614,  0.3619],\n",
      "         ...,\n",
      "         [ 0.6633,  2.1271, -1.4084,  ...,  0.3126,  0.9230, -2.3351],\n",
      "         [ 0.2485,  1.8852, -1.5463,  ...,  0.1018,  0.9651, -1.9293],\n",
      "         [-0.1801,  1.5334, -1.0753,  ...,  0.2458,  0.8987, -1.8022]],\n",
      "\n",
      "        [[ 1.5392, -1.3287,  0.7654,  ..., -1.5376,  1.2790,  1.1864],\n",
      "         [ 1.1814, -1.4137,  0.4860,  ..., -0.6200,  1.7338,  0.6278],\n",
      "         [ 0.7998, -2.4653,  0.8368,  ...,  0.3600,  1.0001,  1.1611],\n",
      "         ...,\n",
      "         [-0.9676,  1.6858, -0.7421,  ..., -0.0198,  0.6471, -2.5167],\n",
      "         [-0.7739,  1.4090, -1.0572,  ..., -0.1533,  0.6442, -2.4356],\n",
      "         [-0.7769,  1.6195, -1.1258,  ..., -0.0258,  0.5899, -2.3856]],\n",
      "\n",
      "        [[ 1.2893, -1.1547,  0.5591,  ..., -1.4754,  1.2700,  1.2299],\n",
      "         [ 0.9575, -1.1835,  0.4760,  ..., -0.9544,  1.4003,  1.0729],\n",
      "         [ 0.5483, -0.3565, -0.0743,  ...,  0.7019,  1.2116,  1.1915],\n",
      "         ...,\n",
      "         [-0.4808,  2.1179, -0.1204,  ..., -0.0399,  0.1379, -0.9448],\n",
      "         [-0.5752,  2.0313, -0.2886,  ..., -0.0768,  0.1938, -0.8775],\n",
      "         [-0.7782,  1.7809, -0.0287,  ...,  0.1163,  0.2926, -0.9847]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.6058, -0.7315,  0.7476,  ..., -1.9641,  1.1712,  1.2666],\n",
      "         [ 1.3945, -0.3779,  0.6143,  ..., -1.4837,  1.6756,  0.7511],\n",
      "         [ 1.3170,  0.2131,  0.7630,  ..., -0.6477,  2.2749,  0.3550],\n",
      "         ...,\n",
      "         [ 0.2112,  1.5422, -0.8066,  ..., -0.2076, -0.0739, -1.4942],\n",
      "         [ 0.0775,  1.5637, -1.0481,  ..., -0.1383,  0.0779, -1.3566],\n",
      "         [ 0.0543,  1.5394, -1.0859,  ..., -0.0690, -0.0403, -1.5060]],\n",
      "\n",
      "        [[ 1.5952, -1.5336, -0.2036,  ..., -2.2787,  0.6212,  0.8458],\n",
      "         [ 1.4159, -1.3204, -0.0995,  ..., -1.7350,  1.0989,  0.5396],\n",
      "         [ 1.7156, -0.3682, -1.0020,  ..., -0.3409,  0.8828,  0.0580],\n",
      "         ...,\n",
      "         [ 0.1514,  1.3665, -0.2757,  ..., -0.5764, -0.4731, -1.4757],\n",
      "         [ 0.0337,  1.3153, -0.2449,  ..., -0.5989, -0.5089, -1.5230],\n",
      "         [-0.1231,  1.1839, -0.1271,  ..., -0.5371, -0.5071, -1.5584]],\n",
      "\n",
      "        [[ 1.2130, -0.9283, -0.3625,  ..., -2.3908,  0.8964,  0.8542],\n",
      "         [ 0.9895, -0.7971, -0.4363,  ..., -1.7980,  1.3691,  0.2820],\n",
      "         [ 0.9798, -0.4062, -0.1493,  ..., -0.5860, -0.2613,  0.2109],\n",
      "         ...,\n",
      "         [ 0.6032,  0.9833, -3.2946,  ..., -1.4324, -0.5988, -0.2461],\n",
      "         [ 1.2506,  0.4962, -3.5521,  ..., -1.2915, -0.8123, -0.4375],\n",
      "         [ 1.0881,  0.7412, -3.7146,  ..., -0.8349, -0.3194, -0.4821]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)), decoder_attentions=None, cross_attentions=None, encoder_last_hidden_state=tensor([[[-3.6942e-02,  7.8996e-03, -4.2825e-04,  ...,  1.1968e-02,\n",
      "          -1.9521e-03, -1.2582e-02],\n",
      "         [ 1.3798e-02,  5.2740e-02,  3.3189e-01,  ...,  1.8133e-01,\n",
      "           1.8695e-01,  2.4382e-01],\n",
      "         [-3.3700e-01,  3.7031e-01,  2.5821e-01,  ...,  3.0003e-01,\n",
      "          -8.9612e-02, -2.0619e-01],\n",
      "         ...,\n",
      "         [ 2.4884e-02,  1.1798e-01,  2.1440e-03,  ...,  7.1982e-02,\n",
      "           2.3062e-01, -4.0555e-02],\n",
      "         [ 5.4103e-03,  8.7359e-02,  3.6397e-02,  ...,  6.9802e-02,\n",
      "           1.0857e-01, -2.6055e-02],\n",
      "         [-2.8112e-02,  7.9329e-02,  3.5627e-03,  ...,  6.3460e-02,\n",
      "           1.1320e-01, -4.5296e-02]],\n",
      "\n",
      "        [[-3.5177e-02,  9.1808e-03, -2.2728e-03,  ...,  1.1474e-02,\n",
      "          -1.3676e-03, -1.0896e-02],\n",
      "         [ 2.9651e-02,  4.1680e-01,  2.4366e-01,  ...,  2.2203e-01,\n",
      "           6.1334e-02,  1.1966e-01],\n",
      "         [-1.0972e-01, -5.6000e-03,  5.9692e-02,  ...,  3.0073e-01,\n",
      "           3.5286e-01,  2.8653e-01],\n",
      "         ...,\n",
      "         [ 1.2277e-01,  8.1043e-02,  1.2308e-01,  ...,  3.6233e-02,\n",
      "           1.9720e-01,  1.8394e-01],\n",
      "         [ 5.0613e-02,  6.2781e-02,  6.2379e-02,  ...,  3.9920e-02,\n",
      "           7.8100e-02,  5.5935e-02],\n",
      "         [ 1.5843e-02,  6.4847e-02,  2.1439e-02,  ...,  1.5283e-02,\n",
      "           8.9256e-02,  4.1576e-02]],\n",
      "\n",
      "        [[-3.7149e-02,  8.0792e-03, -1.5225e-03,  ...,  1.1755e-02,\n",
      "          -1.4891e-03, -1.3061e-02],\n",
      "         [-8.6494e-03, -2.5836e-02,  2.2415e-01,  ...,  3.3119e-01,\n",
      "          -2.4001e-01,  4.8439e-01],\n",
      "         [-3.0078e-01, -8.2160e-02,  4.2907e-01,  ...,  2.2598e-01,\n",
      "          -7.9946e-02,  2.3608e-01],\n",
      "         ...,\n",
      "         [ 1.0614e-01,  1.8946e-01, -1.9936e-02,  ...,  5.2146e-02,\n",
      "           2.7098e-01,  6.4949e-02],\n",
      "         [ 1.2734e-01,  5.8392e-02, -5.3437e-02,  ...,  6.2603e-02,\n",
      "           3.7245e-01,  6.0236e-02],\n",
      "         [ 1.3935e-01,  4.5453e-02, -8.8622e-02,  ...,  1.6477e-01,\n",
      "           4.3821e-01,  3.2251e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.7059e-02,  7.9947e-03, -1.4724e-03,  ...,  1.1367e-02,\n",
      "          -1.9510e-03, -1.4218e-02],\n",
      "         [-1.1843e-03,  3.1973e-01,  4.8026e-01,  ..., -8.2604e-03,\n",
      "          -1.3907e-01,  9.1765e-02],\n",
      "         [ 6.4474e-02,  2.3424e-01,  1.9595e-02,  ...,  6.6847e-02,\n",
      "           7.0244e-02,  1.5343e-01],\n",
      "         ...,\n",
      "         [-1.6014e-02,  2.4802e-01,  8.0870e-03,  ...,  3.6112e-03,\n",
      "           3.0494e-01,  3.1774e-02],\n",
      "         [ 9.5586e-03,  1.3095e-01,  3.0157e-02,  ...,  4.8129e-02,\n",
      "           2.8814e-02, -4.8882e-02],\n",
      "         [ 7.6966e-03,  1.5696e-01,  3.1137e-02,  ...,  5.1053e-02,\n",
      "           5.4411e-02, -7.8420e-02]],\n",
      "\n",
      "        [[-3.6614e-02,  7.2931e-03, -2.0682e-03,  ...,  1.2549e-02,\n",
      "          -3.4022e-04, -1.4590e-02],\n",
      "         [-1.8347e-01,  1.4000e-01,  2.0078e-01,  ...,  2.4640e-01,\n",
      "          -3.9356e-01, -1.1996e-01],\n",
      "         [-6.3016e-02,  1.8330e-01, -3.4979e-01,  ...,  2.9593e-01,\n",
      "          -1.6424e-01, -1.5812e-01],\n",
      "         ...,\n",
      "         [ 1.4133e-01,  1.1040e-02,  9.9254e-02,  ...,  2.1092e-02,\n",
      "           3.9644e-01,  1.3697e-01],\n",
      "         [ 7.4464e-02,  7.0928e-02,  4.1733e-02,  ...,  6.5397e-02,\n",
      "           2.1167e-01, -2.6823e-02],\n",
      "         [ 1.6819e-01,  2.2471e-02,  3.9231e-02,  ...,  5.7159e-02,\n",
      "           4.3849e-01,  8.3982e-02]],\n",
      "\n",
      "        [[-3.4929e-02,  8.7599e-03, -1.9166e-03,  ...,  1.1186e-02,\n",
      "          -1.2344e-03, -1.0955e-02],\n",
      "         [-2.1948e-01,  4.1065e-01,  3.8908e-01,  ...,  2.3295e-01,\n",
      "          -4.0531e-01,  2.1880e-01],\n",
      "         [ 3.1309e-02,  1.7303e-01,  2.1738e-01,  ...,  1.1751e-01,\n",
      "          -8.0240e-02, -3.3071e-02],\n",
      "         ...,\n",
      "         [ 1.9755e-01,  8.2803e-02,  3.0834e-02,  ...,  6.0052e-02,\n",
      "           2.4766e-01,  1.2810e-01],\n",
      "         [ 6.4714e-02,  7.6333e-02,  2.3721e-02,  ...,  2.5278e-02,\n",
      "           5.4639e-02,  3.9037e-02],\n",
      "         [ 7.4809e-02,  1.0626e-01,  6.3061e-02,  ..., -3.1076e-03,\n",
      "           6.0709e-02,  8.2175e-02]]]), encoder_hidden_states=(tensor([[[ 0.1810, -0.1337,  0.0424,  ...,  0.0790, -0.2555,  0.0459],\n",
      "         [ 0.3475, -0.0152,  0.4136,  ...,  0.1141, -0.0854,  0.0133],\n",
      "         [-0.5751, -0.0044,  0.4022,  ...,  0.0959, -0.2763, -0.2070],\n",
      "         ...,\n",
      "         [ 0.4157,  0.6533, -0.0622,  ...,  0.3168,  0.2436, -0.2378],\n",
      "         [ 0.4539,  0.4505, -0.0401,  ...,  0.4219,  0.1895, -0.1260],\n",
      "         [ 0.5269,  0.2336, -0.2028,  ...,  0.7653,  0.1041, -0.1408]],\n",
      "\n",
      "        [[ 0.1810, -0.1337,  0.0424,  ...,  0.0790, -0.2555,  0.0459],\n",
      "         [ 0.3419, -0.0214, -0.0626,  ..., -0.3502, -0.3511, -0.0664],\n",
      "         [-0.0421, -0.0499,  0.0168,  ...,  0.3547,  0.6514, -0.3531],\n",
      "         ...,\n",
      "         [ 0.4157,  0.6533, -0.0622,  ...,  0.3168,  0.2436, -0.2378],\n",
      "         [ 0.4539,  0.4505, -0.0401,  ...,  0.4219,  0.1895, -0.1260],\n",
      "         [ 0.5269,  0.2336, -0.2028,  ...,  0.7653,  0.1041, -0.1408]],\n",
      "\n",
      "        [[ 0.1810, -0.1337,  0.0424,  ...,  0.0790, -0.2555,  0.0459],\n",
      "         [-0.1447, -0.0830,  0.4208,  ..., -0.2195, -0.1394,  0.0478],\n",
      "         [-0.6289, -0.0054,  0.3194,  ...,  0.0790,  0.3527, -0.0189],\n",
      "         ...,\n",
      "         [ 0.4157,  0.6533, -0.0622,  ...,  0.3168,  0.2436, -0.2378],\n",
      "         [ 0.4539,  0.4505, -0.0401,  ...,  0.4219,  0.1895, -0.1260],\n",
      "         [ 0.5269,  0.2336, -0.2028,  ...,  0.7653,  0.1041, -0.1408]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1810, -0.1337,  0.0424,  ...,  0.0790, -0.2555,  0.0459],\n",
      "         [-0.2229,  0.1330,  0.3275,  ..., -0.0335,  0.1806, -0.1716],\n",
      "         [-0.2213, -0.5148,  0.0299,  ...,  0.1134,  0.0574,  0.2151],\n",
      "         ...,\n",
      "         [ 0.4157,  0.6533, -0.0622,  ...,  0.3168,  0.2436, -0.2378],\n",
      "         [ 0.4539,  0.4505, -0.0401,  ...,  0.4219,  0.1895, -0.1260],\n",
      "         [ 0.5269,  0.2336, -0.2028,  ...,  0.7653,  0.1041, -0.1408]],\n",
      "\n",
      "        [[ 0.1810, -0.1337,  0.0424,  ...,  0.0790, -0.2555,  0.0459],\n",
      "         [ 0.0855, -0.3713,  0.0629,  ..., -0.2088, -0.6630,  0.1069],\n",
      "         [-0.6389, -0.3331, -0.4516,  ...,  0.0081, -0.8600, -0.1994],\n",
      "         ...,\n",
      "         [ 0.4157,  0.6533, -0.0622,  ...,  0.3168,  0.2436, -0.2378],\n",
      "         [ 0.4539,  0.4505, -0.0401,  ...,  0.4219,  0.1895, -0.1260],\n",
      "         [ 0.5269,  0.2336, -0.2028,  ...,  0.7653,  0.1041, -0.1408]],\n",
      "\n",
      "        [[ 0.1810, -0.1337,  0.0424,  ...,  0.0790, -0.2555,  0.0459],\n",
      "         [-0.0739, -0.5166,  0.0518,  ...,  0.2304, -0.0428,  0.1685],\n",
      "         [-0.0190, -0.1306,  0.3262,  ..., -0.5934, -0.2479,  0.0539],\n",
      "         ...,\n",
      "         [ 0.4157,  0.6533, -0.0622,  ...,  0.3168,  0.2436, -0.2378],\n",
      "         [ 0.4539,  0.4505, -0.0401,  ...,  0.4219,  0.1895, -0.1260],\n",
      "         [ 0.5269,  0.2336, -0.2028,  ...,  0.7653,  0.1041, -0.1408]]]), tensor([[[-0.0401, -0.0302,  0.0573,  ...,  0.0284, -0.0379,  0.0220],\n",
      "         [ 0.4785, -0.0605,  0.7041,  ...,  0.3638,  0.2434,  0.2816],\n",
      "         [-0.2596,  0.5467,  0.0483,  ...,  0.0056, -0.4474,  0.0085],\n",
      "         ...,\n",
      "         [ 0.2407,  0.6746,  0.0399,  ...,  0.5648,  0.6532, -0.2634],\n",
      "         [ 0.2628,  0.6219,  0.1077,  ...,  0.7165,  0.3941,  0.0027],\n",
      "         [ 0.1374,  0.4258, -0.1515,  ...,  0.8889,  0.4015, -0.1466]],\n",
      "\n",
      "        [[-0.0522, -0.0313,  0.0632,  ...,  0.0118, -0.0301,  0.0212],\n",
      "         [ 0.1499, -0.1468,  0.1785,  ...,  0.1151,  0.1443,  0.2200],\n",
      "         [-0.3638, -0.0233,  0.0919,  ...,  0.4808,  0.9765, -0.0772],\n",
      "         ...,\n",
      "         [ 0.3879,  0.4673,  0.1177,  ...,  0.3122,  0.4073, -0.1002],\n",
      "         [ 0.3857,  0.4824,  0.1310,  ...,  0.5747,  0.3861,  0.0064],\n",
      "         [ 0.4002,  0.4815, -0.1510,  ...,  0.7763,  0.4643, -0.0037]],\n",
      "\n",
      "        [[-0.0314, -0.0299,  0.0592,  ...,  0.0224, -0.0277,  0.0194],\n",
      "         [-0.0378,  0.1077,  0.1277,  ...,  0.1943, -0.4423,  0.3834],\n",
      "         [-0.5695,  0.2256,  0.5461,  ...,  0.3956,  0.1398,  0.4318],\n",
      "         ...,\n",
      "         [ 0.3727,  0.6691,  0.0706,  ...,  0.3775,  0.5328, -0.2424],\n",
      "         [ 0.5310,  0.5737,  0.0735,  ...,  0.4221,  0.4800, -0.0653],\n",
      "         [ 0.5656,  0.4421, -0.1159,  ...,  0.7170,  0.4520,  0.0222]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0416, -0.0381,  0.0520,  ...,  0.0232, -0.0325,  0.0128],\n",
      "         [-0.3961,  0.0541,  0.6331,  ...,  0.1948,  0.2405, -0.0674],\n",
      "         [-0.2000, -0.2136, -0.0302,  ...,  0.2199,  0.1485,  0.1255],\n",
      "         ...,\n",
      "         [ 0.2561,  0.5954,  0.0554,  ...,  0.4688,  0.4839, -0.1142],\n",
      "         [ 0.2467,  0.3396,  0.0433,  ...,  0.6233,  0.3808, -0.1925],\n",
      "         [ 0.4213,  0.1954,  0.0022,  ...,  0.5625,  0.2593, -0.0494]],\n",
      "\n",
      "        [[-0.0405, -0.0378,  0.0550,  ...,  0.0278, -0.0277,  0.0175],\n",
      "         [-0.1060, -0.3268,  0.2211,  ...,  0.1553, -0.9374,  0.1868],\n",
      "         [-0.3627, -0.1483, -0.6215,  ...,  0.2853, -0.7523,  0.0529],\n",
      "         ...,\n",
      "         [ 0.2717,  0.4769,  0.0967,  ...,  0.2934,  0.5310, -0.2393],\n",
      "         [ 0.4794,  0.4267,  0.0437,  ...,  0.6317,  0.4720, -0.1222],\n",
      "         [ 0.4383,  0.2824, -0.1572,  ...,  0.9108,  0.4208,  0.1346]],\n",
      "\n",
      "        [[-0.0346, -0.0505,  0.0487,  ...,  0.0296, -0.0460,  0.0208],\n",
      "         [ 0.0098, -0.1233,  0.0354,  ...,  0.3480, -0.2706,  0.3629],\n",
      "         [ 0.0165,  0.4068, -0.0556,  ..., -0.0988, -0.0677,  0.1577],\n",
      "         ...,\n",
      "         [ 0.3627,  0.6420,  0.0016,  ...,  0.4278,  0.4246, -0.3632],\n",
      "         [ 0.3495,  0.4010,  0.1382,  ...,  0.5537,  0.3471, -0.2115],\n",
      "         [ 0.4533,  0.0996, -0.0638,  ...,  0.6005,  0.3475, -0.1539]]]), tensor([[[-5.7345e-02,  5.3780e-03,  3.8222e-02,  ..., -3.4535e-02,\n",
      "          -5.6338e-02,  4.3146e-03],\n",
      "         [ 5.0001e-01,  3.0706e-01,  8.1292e-01,  ...,  3.8761e-01,\n",
      "           3.6608e-01,  4.9482e-01],\n",
      "         [-2.1315e-01,  3.9885e-01,  2.4481e-02,  ..., -1.0140e-01,\n",
      "          -4.2353e-01, -1.6398e-01],\n",
      "         ...,\n",
      "         [ 2.4107e-01,  5.4908e-01,  1.1832e-01,  ...,  2.3422e-01,\n",
      "           8.3915e-01, -2.4625e-01],\n",
      "         [ 1.8392e-01,  6.0450e-01,  1.4671e-01,  ...,  3.1065e-01,\n",
      "           5.8640e-01, -1.9237e-01],\n",
      "         [ 9.1092e-02,  6.5062e-01, -7.7226e-02,  ...,  5.5890e-01,\n",
      "           6.3702e-01, -2.9711e-01]],\n",
      "\n",
      "        [[-7.3282e-02,  4.4293e-03,  3.9231e-02,  ..., -3.5283e-02,\n",
      "          -5.5378e-02, -3.7234e-03],\n",
      "         [-4.5239e-02, -1.0002e-01,  3.6805e-01,  ...,  3.7279e-01,\n",
      "           2.8924e-01,  3.5660e-01],\n",
      "         [-3.4970e-01,  1.6865e-01, -9.6059e-02,  ...,  6.3489e-01,\n",
      "           1.1394e+00, -1.7620e-01],\n",
      "         ...,\n",
      "         [ 4.0250e-01,  4.0507e-01,  3.3844e-01,  ...,  1.9269e-01,\n",
      "           6.1166e-01, -1.8244e-01],\n",
      "         [ 4.6353e-01,  4.4883e-01,  2.9129e-01,  ...,  4.1281e-01,\n",
      "           6.2840e-01, -1.1551e-02],\n",
      "         [ 3.3164e-01,  5.6077e-01, -2.5097e-04,  ...,  4.2581e-01,\n",
      "           6.9281e-01, -1.0279e-01]],\n",
      "\n",
      "        [[-5.8529e-02,  1.0114e-02,  3.3426e-02,  ..., -4.6325e-02,\n",
      "          -6.3167e-02, -1.1228e-02],\n",
      "         [-3.7650e-01, -8.1175e-02,  2.8906e-01,  ...,  4.5502e-01,\n",
      "          -2.5888e-01,  5.1081e-01],\n",
      "         [-4.7619e-01,  3.8891e-01,  7.1263e-01,  ...,  2.0391e-01,\n",
      "           4.8641e-02,  4.1329e-01],\n",
      "         ...,\n",
      "         [ 3.4505e-01,  4.9163e-01,  8.5290e-02,  ...,  1.2115e-01,\n",
      "           7.2814e-01, -1.8349e-01],\n",
      "         [ 5.3539e-01,  4.6670e-01,  4.5893e-03,  ...,  1.4532e-01,\n",
      "           6.6969e-01, -7.7915e-02],\n",
      "         [ 5.4807e-01,  4.9429e-01, -1.0747e-01,  ...,  4.0025e-01,\n",
      "           6.3539e-01, -2.0525e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-6.3304e-02,  2.6716e-03,  3.3097e-02,  ..., -3.8337e-02,\n",
      "          -4.9800e-02, -1.1336e-02],\n",
      "         [-3.7613e-01,  1.1770e-01,  7.2471e-01,  ...,  3.1091e-01,\n",
      "           2.9197e-01,  3.5439e-01],\n",
      "         [-2.0595e-01,  8.3213e-03, -6.4661e-02,  ...,  1.8570e-01,\n",
      "          -4.3127e-02, -1.7470e-01],\n",
      "         ...,\n",
      "         [ 2.1181e-01,  5.1634e-01,  1.3198e-01,  ...,  2.4609e-01,\n",
      "           7.1271e-01, -2.9151e-01],\n",
      "         [ 2.2231e-01,  3.3238e-01,  5.3999e-02,  ...,  3.4423e-01,\n",
      "           5.8821e-01, -4.0104e-01],\n",
      "         [ 3.2947e-01,  3.9175e-01,  3.2590e-02,  ...,  6.4501e-01,\n",
      "           4.0278e-01, -2.5607e-01]],\n",
      "\n",
      "        [[-6.7607e-02,  1.2994e-03,  3.1475e-02,  ..., -3.9594e-02,\n",
      "          -6.4606e-02, -1.0376e-02],\n",
      "         [-1.7268e-01, -3.0745e-01,  2.3972e-01,  ...,  2.2052e-01,\n",
      "          -7.5570e-01,  4.2453e-01],\n",
      "         [-2.5073e-01,  7.9897e-02, -7.1820e-01,  ...,  1.8321e-01,\n",
      "          -4.7730e-01, -1.3881e-02],\n",
      "         ...,\n",
      "         [ 2.6915e-01,  2.3515e-01,  1.3999e-01,  ...,  2.4588e-01,\n",
      "           7.0701e-01, -2.9496e-01],\n",
      "         [ 4.1690e-01,  1.8660e-01,  9.3368e-02,  ...,  3.9169e-01,\n",
      "           7.1597e-01, -1.6247e-01],\n",
      "         [ 3.6174e-01,  3.4106e-01, -7.4907e-02,  ...,  5.9409e-01,\n",
      "           6.2167e-01, -1.3746e-01]],\n",
      "\n",
      "        [[-5.5818e-02,  2.1843e-03,  3.5037e-02,  ..., -2.9514e-02,\n",
      "          -6.4418e-02,  1.6583e-04],\n",
      "         [-2.0887e-01,  4.5720e-02,  3.5553e-01,  ...,  3.5300e-01,\n",
      "          -3.5547e-01,  7.2444e-01],\n",
      "         [ 1.5700e-02,  1.5028e-01,  2.3701e-01,  ..., -1.6158e-01,\n",
      "          -2.1342e-01,  9.3381e-02],\n",
      "         ...,\n",
      "         [ 3.0517e-01,  5.6947e-01,  1.0979e-01,  ...,  1.5045e-01,\n",
      "           5.6865e-01, -3.2008e-01],\n",
      "         [ 2.9788e-01,  4.1175e-01,  2.5690e-01,  ...,  3.3890e-01,\n",
      "           4.8086e-01, -2.6168e-01],\n",
      "         [ 4.0378e-01,  2.7329e-01,  5.8658e-02,  ...,  4.0649e-01,\n",
      "           4.2339e-01, -4.0242e-01]]]), tensor([[[-6.4836e-02,  2.5865e-02,  7.6336e-03,  ..., -1.0030e-02,\n",
      "          -4.6134e-03, -5.5859e-05],\n",
      "         [ 3.6378e-01,  4.6150e-01,  5.7483e-01,  ...,  4.1912e-01,\n",
      "           1.8896e-01,  5.9806e-01],\n",
      "         [-2.1341e-01,  5.4893e-01,  1.4425e-01,  ...,  3.6617e-03,\n",
      "          -5.7477e-01, -2.7942e-01],\n",
      "         ...,\n",
      "         [ 3.7791e-02,  4.6441e-02,  2.6049e-01,  ...,  2.7593e-01,\n",
      "           8.0037e-01,  1.2132e-01],\n",
      "         [ 9.1862e-02,  1.4256e-01,  2.9337e-01,  ...,  5.0215e-01,\n",
      "           5.3871e-01,  1.8974e-01],\n",
      "         [-5.4685e-02,  7.9604e-02,  1.0161e-01,  ...,  6.1449e-01,\n",
      "           5.6404e-01,  2.1832e-01]],\n",
      "\n",
      "        [[-6.8575e-02,  2.1079e-02,  1.2647e-02,  ..., -8.9744e-03,\n",
      "          -1.9350e-03,  1.6298e-04],\n",
      "         [-1.5718e-02,  2.7819e-01,  3.2411e-01,  ...,  4.4879e-01,\n",
      "           1.3630e-01,  2.8759e-01],\n",
      "         [-2.7514e-01,  1.5282e-01, -9.7904e-03,  ...,  7.7441e-01,\n",
      "           8.3211e-01, -1.9245e-01],\n",
      "         ...,\n",
      "         [ 1.9348e-01, -1.2545e-02,  4.6033e-01,  ...,  2.4138e-01,\n",
      "           4.9169e-01,  1.8932e-01],\n",
      "         [ 1.7787e-01,  1.1810e-03,  4.4541e-01,  ...,  5.0374e-01,\n",
      "           4.4528e-01,  4.0462e-01],\n",
      "         [ 3.8312e-02,  5.4292e-03,  1.7114e-01,  ...,  4.6907e-01,\n",
      "           5.6096e-01,  3.1111e-01]],\n",
      "\n",
      "        [[-6.4386e-02,  2.3885e-02,  1.1334e-02,  ..., -2.3963e-02,\n",
      "          -1.2790e-02, -4.3995e-03],\n",
      "         [-5.2226e-01,  2.0505e-01,  1.2826e-01,  ...,  4.0352e-01,\n",
      "          -3.0091e-01,  5.0302e-01],\n",
      "         [-6.4748e-01,  3.0408e-01,  7.3042e-01,  ...,  5.4930e-01,\n",
      "          -1.1537e-01,  2.9655e-01],\n",
      "         ...,\n",
      "         [-6.7300e-02, -7.1239e-02,  1.2244e-01,  ...,  9.0218e-02,\n",
      "           6.1646e-01,  7.9803e-02],\n",
      "         [ 1.3322e-01, -1.6610e-01, -1.4008e-02,  ...,  1.9606e-01,\n",
      "           6.3090e-01,  1.7419e-01],\n",
      "         [ 1.1084e-01, -1.1566e-01, -1.5442e-01,  ...,  3.6930e-01,\n",
      "           6.0945e-01,  1.0709e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-6.4091e-02,  2.3619e-02,  5.6413e-03,  ..., -1.4668e-02,\n",
      "          -2.9900e-03, -3.1617e-03],\n",
      "         [-2.9832e-01,  2.3401e-01,  8.4418e-01,  ...,  2.0654e-01,\n",
      "           7.5617e-02,  6.5506e-01],\n",
      "         [-1.9204e-02,  9.2550e-02,  4.5844e-02,  ...,  8.7341e-02,\n",
      "          -1.3066e-01, -2.0207e-01],\n",
      "         ...,\n",
      "         [-1.7182e-01,  4.2167e-02,  1.5252e-01,  ...,  2.0311e-01,\n",
      "           6.0031e-01,  1.3029e-01],\n",
      "         [-6.4997e-02, -3.2879e-03,  1.0843e-01,  ...,  3.1942e-01,\n",
      "           4.1390e-01,  4.1493e-02],\n",
      "         [-6.0236e-02, -1.6663e-01,  1.5892e-01,  ...,  3.6541e-01,\n",
      "           3.4216e-01,  2.2665e-01]],\n",
      "\n",
      "        [[-6.7055e-02,  1.6096e-02,  8.7038e-03,  ..., -2.1353e-02,\n",
      "          -6.8234e-03, -5.2163e-03],\n",
      "         [-3.0578e-01,  5.3275e-02,  2.8332e-01,  ...,  3.4235e-01,\n",
      "          -6.9956e-01,  1.7680e-01],\n",
      "         [-1.6771e-01,  1.4809e-01, -8.4149e-01,  ...,  1.8239e-01,\n",
      "          -3.9321e-01, -4.6478e-02],\n",
      "         ...,\n",
      "         [-1.8586e-02, -3.1429e-01,  1.6805e-01,  ...,  1.1470e-01,\n",
      "           6.6959e-01, -1.1061e-01],\n",
      "         [ 1.3926e-01, -2.9736e-01,  1.6777e-01,  ...,  3.3625e-01,\n",
      "           6.7738e-01,  7.1454e-02],\n",
      "         [ 1.0492e-01, -2.7087e-01, -1.7358e-02,  ...,  4.2754e-01,\n",
      "           5.8731e-01,  7.1832e-02]],\n",
      "\n",
      "        [[-6.3884e-02,  1.9993e-02,  8.2319e-03,  ..., -1.6249e-02,\n",
      "          -8.5723e-03,  3.6483e-03],\n",
      "         [-4.3301e-01,  2.8058e-01,  4.6800e-01,  ...,  5.8740e-01,\n",
      "          -3.8236e-01,  8.5818e-01],\n",
      "         [-7.6429e-02,  4.7671e-01,  2.6661e-01,  ...,  1.8875e-01,\n",
      "          -9.3325e-02, -1.2720e-01],\n",
      "         ...,\n",
      "         [ 1.7312e-01,  3.9663e-02,  2.2585e-01,  ...,  2.6204e-01,\n",
      "           5.6741e-01,  3.7220e-02],\n",
      "         [ 1.0996e-01, -1.6204e-02,  3.8555e-01,  ...,  4.7249e-01,\n",
      "           3.4509e-01,  1.4678e-01],\n",
      "         [ 1.9701e-01, -2.4556e-01,  3.2452e-01,  ...,  4.4684e-01,\n",
      "           3.4811e-01,  1.6223e-01]]]), tensor([[[-0.0876,  0.0275, -0.0089,  ...,  0.0489,  0.0059, -0.0052],\n",
      "         [ 0.3181,  0.1675,  0.5268,  ...,  0.6073,  0.2492,  0.5793],\n",
      "         [-0.3212,  0.5143,  0.0629,  ...,  0.1285, -0.3566,  0.0016],\n",
      "         ...,\n",
      "         [ 0.2064,  0.0679,  0.2287,  ...,  0.3842,  1.0058,  0.1622],\n",
      "         [ 0.2036,  0.0828,  0.2787,  ...,  0.5193,  0.7186,  0.2913],\n",
      "         [ 0.1028,  0.0495,  0.0867,  ...,  0.6027,  0.7085,  0.2418]],\n",
      "\n",
      "        [[-0.0911,  0.0233, -0.0126,  ...,  0.0477,  0.0060, -0.0069],\n",
      "         [ 0.0145,  0.5324,  0.5317,  ...,  0.4373,  0.1235,  0.3751],\n",
      "         [ 0.0437,  0.0915,  0.0884,  ...,  0.8771,  0.7832,  0.3288],\n",
      "         ...,\n",
      "         [ 0.4045,  0.0952,  0.5085,  ...,  0.3225,  0.5593,  0.2063],\n",
      "         [ 0.2932,  0.1805,  0.4309,  ...,  0.3069,  0.4107,  0.3796],\n",
      "         [ 0.1844,  0.1007,  0.2025,  ...,  0.3963,  0.5279,  0.2412]],\n",
      "\n",
      "        [[-0.0897,  0.0291, -0.0106,  ...,  0.0475,  0.0041, -0.0074],\n",
      "         [-0.3335,  0.3448,  0.0844,  ...,  0.5294, -0.4934,  0.8925],\n",
      "         [-0.3867,  0.3535,  0.5809,  ...,  0.5601, -0.1971,  0.2056],\n",
      "         ...,\n",
      "         [ 0.0634, -0.1188,  0.1217,  ...,  0.2935,  0.7363,  0.3562],\n",
      "         [ 0.2280, -0.2849, -0.0022,  ...,  0.4713,  0.8101,  0.2922],\n",
      "         [ 0.2375, -0.2450, -0.1288,  ...,  0.5624,  0.8042,  0.2233]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0871,  0.0256, -0.0101,  ...,  0.0477,  0.0079, -0.0097],\n",
      "         [ 0.1257,  0.1513,  0.7661,  ...,  0.4137, -0.0825,  0.4953],\n",
      "         [ 0.0824,  0.3130,  0.0767,  ...,  0.2378,  0.1686, -0.3062],\n",
      "         ...,\n",
      "         [ 0.0016,  0.1311,  0.1478,  ...,  0.2923,  0.8058,  0.2160],\n",
      "         [-0.0155,  0.2781,  0.1053,  ...,  0.3234,  0.5128,  0.0960],\n",
      "         [-0.0080,  0.0426,  0.2158,  ...,  0.4383,  0.4728,  0.2608]],\n",
      "\n",
      "        [[-0.0890,  0.0226, -0.0159,  ...,  0.0512,  0.0086, -0.0106],\n",
      "         [-0.2357,  0.1038,  0.2175,  ...,  0.5104, -0.7657, -0.1727],\n",
      "         [-0.0144,  0.1286, -0.8880,  ...,  0.3542, -0.5063, -0.0710],\n",
      "         ...,\n",
      "         [ 0.1574, -0.3221,  0.2092,  ...,  0.3469,  0.8804,  0.0349],\n",
      "         [ 0.2257, -0.2132,  0.2090,  ...,  0.4067,  0.8178,  0.2237],\n",
      "         [ 0.2661, -0.2187,  0.0925,  ...,  0.5145,  0.8427,  0.0903]],\n",
      "\n",
      "        [[-0.0905,  0.0252, -0.0090,  ...,  0.0463,  0.0047, -0.0069],\n",
      "         [-0.2015,  0.4345,  0.4038,  ...,  0.9648, -0.5067,  0.2997],\n",
      "         [-0.1571,  0.5995,  0.3785,  ...,  0.4785, -0.2121, -0.3423],\n",
      "         ...,\n",
      "         [ 0.5226, -0.0055,  0.2607,  ...,  0.3044,  0.6339,  0.1473],\n",
      "         [ 0.3403, -0.0139,  0.3778,  ...,  0.4322,  0.3006,  0.3121],\n",
      "         [ 0.3333, -0.2133,  0.3872,  ...,  0.4248,  0.2987,  0.4224]]]), tensor([[[-7.0444e-02,  1.9429e-02, -2.6480e-03,  ...,  2.2352e-02,\n",
      "           2.0865e-03, -1.8225e-02],\n",
      "         [ 1.8699e-01,  2.2162e-01,  6.6644e-01,  ...,  4.4820e-01,\n",
      "           5.4179e-01,  4.5434e-01],\n",
      "         [-4.4345e-01,  4.7373e-01,  1.9932e-01,  ...,  1.3050e-01,\n",
      "          -3.4505e-01, -8.9867e-02],\n",
      "         ...,\n",
      "         [ 2.2243e-01,  2.6237e-01,  1.1887e-01,  ...,  2.4953e-01,\n",
      "           9.3363e-01,  1.1560e-01],\n",
      "         [ 2.2527e-01,  2.6133e-01,  2.0001e-01,  ...,  1.9434e-01,\n",
      "           5.5941e-01,  2.1219e-01],\n",
      "         [ 9.2019e-02,  1.9821e-01,  4.0158e-02,  ...,  2.4070e-01,\n",
      "           5.9015e-01,  1.4588e-01]],\n",
      "\n",
      "        [[-6.7439e-02,  2.0582e-02, -7.5945e-03,  ...,  2.5285e-02,\n",
      "          -5.3878e-04, -1.4105e-02],\n",
      "         [ 1.0987e-02,  9.4465e-01,  6.3741e-01,  ...,  3.3605e-01,\n",
      "           2.4515e-01,  2.8555e-01],\n",
      "         [-1.1947e-01,  2.7750e-01,  6.4270e-02,  ...,  5.2766e-01,\n",
      "           8.6929e-01,  2.2528e-01],\n",
      "         ...,\n",
      "         [ 4.0167e-01,  9.9790e-02,  3.4733e-01,  ...,  1.6100e-01,\n",
      "           6.2822e-01,  3.8545e-01],\n",
      "         [ 3.2797e-01,  9.7682e-02,  2.8929e-01,  ..., -4.1660e-03,\n",
      "           2.9732e-01,  4.8199e-01],\n",
      "         [ 1.7169e-01,  1.0154e-01,  7.5630e-02,  ..., -2.5806e-02,\n",
      "           4.1694e-01,  4.1646e-01]],\n",
      "\n",
      "        [[-6.6865e-02,  1.7151e-02, -7.3616e-03,  ...,  2.0799e-02,\n",
      "           4.9918e-03, -1.3014e-02],\n",
      "         [-3.5452e-01,  3.5826e-01,  9.2379e-02,  ...,  2.8905e-01,\n",
      "          -3.1608e-01,  6.6498e-01],\n",
      "         [-5.2056e-01,  4.8969e-02,  7.4569e-01,  ...,  4.5929e-01,\n",
      "          -1.9746e-01,  1.0344e-01],\n",
      "         ...,\n",
      "         [ 2.5450e-01,  1.9687e-01,  1.2659e-02,  ...,  1.5050e-01,\n",
      "           7.6822e-01,  1.7303e-01],\n",
      "         [ 2.9885e-01, -3.2778e-02, -8.5283e-02,  ...,  3.2552e-01,\n",
      "           9.0931e-01,  7.5011e-02],\n",
      "         [ 3.0124e-01, -1.4298e-01, -1.8682e-01,  ...,  4.4126e-01,\n",
      "           9.2101e-01,  1.0690e-01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-6.7729e-02,  1.3159e-02, -2.5437e-03,  ...,  1.7427e-02,\n",
      "           2.0431e-03, -1.7399e-02],\n",
      "         [ 2.1648e-01,  4.5742e-01,  9.2237e-01,  ...,  1.3042e-02,\n",
      "          -3.2529e-02,  2.8191e-01],\n",
      "         [ 1.1577e-02,  5.2145e-01,  1.1310e-01,  ...,  9.8960e-02,\n",
      "           1.9448e-01, -1.7847e-01],\n",
      "         ...,\n",
      "         [ 1.3812e-02,  5.0279e-01, -4.8413e-03,  ..., -2.8036e-02,\n",
      "           8.9691e-01,  8.0362e-02],\n",
      "         [ 1.0422e-01,  4.5576e-01,  6.0629e-02,  ...,  4.1898e-02,\n",
      "           2.6090e-01, -1.5744e-02],\n",
      "         [ 5.1805e-02,  3.8968e-01,  9.0563e-02,  ...,  3.1473e-02,\n",
      "           3.2056e-01,  1.1773e-01]],\n",
      "\n",
      "        [[-6.7540e-02,  6.4472e-03, -1.0849e-02,  ...,  2.1477e-02,\n",
      "           7.0310e-03, -1.4248e-02],\n",
      "         [-4.9744e-01,  2.8871e-01,  2.5370e-01,  ...,  4.5126e-01,\n",
      "          -7.1962e-01, -2.1217e-01],\n",
      "         [-8.2791e-03,  9.1278e-02, -7.6262e-01,  ...,  3.4491e-01,\n",
      "          -3.9833e-01, -2.6321e-01],\n",
      "         ...,\n",
      "         [ 2.6705e-01, -2.1570e-01,  1.5166e-01,  ...,  2.1788e-01,\n",
      "           1.0521e+00, -3.5677e-02],\n",
      "         [ 2.4573e-01, -1.5344e-02,  1.0210e-01,  ...,  9.4767e-02,\n",
      "           7.5954e-01,  8.1196e-02],\n",
      "         [ 3.6666e-01, -1.6313e-01,  3.8829e-02,  ...,  2.5094e-01,\n",
      "           1.0586e+00, -8.0578e-02]],\n",
      "\n",
      "        [[-6.6502e-02,  1.8567e-02, -6.1331e-03,  ...,  2.4924e-02,\n",
      "          -2.5354e-04, -1.0742e-02],\n",
      "         [-2.6169e-01,  6.8645e-01,  5.8789e-01,  ...,  4.0002e-01,\n",
      "          -6.6483e-01,  6.3373e-02],\n",
      "         [-1.6341e-01,  5.0211e-01,  3.6158e-01,  ...,  2.3930e-01,\n",
      "          -1.5262e-01, -4.2427e-01],\n",
      "         ...,\n",
      "         [ 5.5242e-01,  1.0795e-01,  1.7171e-01,  ...,  2.0052e-01,\n",
      "           7.0094e-01,  2.2902e-01],\n",
      "         [ 3.1436e-01,  1.3595e-01,  2.1852e-01,  ...,  3.3439e-02,\n",
      "           2.0990e-01,  2.5255e-01],\n",
      "         [ 2.6756e-01,  1.2451e-01,  2.5533e-01,  ..., -2.6316e-02,\n",
      "           2.4383e-01,  3.8008e-01]]]), tensor([[[-3.6942e-02,  7.8996e-03, -4.2825e-04,  ...,  1.1968e-02,\n",
      "          -1.9521e-03, -1.2582e-02],\n",
      "         [ 1.3798e-02,  5.2740e-02,  3.3189e-01,  ...,  1.8133e-01,\n",
      "           1.8695e-01,  2.4382e-01],\n",
      "         [-3.3700e-01,  3.7031e-01,  2.5821e-01,  ...,  3.0003e-01,\n",
      "          -8.9612e-02, -2.0619e-01],\n",
      "         ...,\n",
      "         [ 2.4884e-02,  1.1798e-01,  2.1440e-03,  ...,  7.1982e-02,\n",
      "           2.3062e-01, -4.0555e-02],\n",
      "         [ 5.4103e-03,  8.7359e-02,  3.6397e-02,  ...,  6.9802e-02,\n",
      "           1.0857e-01, -2.6055e-02],\n",
      "         [-2.8112e-02,  7.9329e-02,  3.5627e-03,  ...,  6.3460e-02,\n",
      "           1.1320e-01, -4.5296e-02]],\n",
      "\n",
      "        [[-3.5177e-02,  9.1808e-03, -2.2728e-03,  ...,  1.1474e-02,\n",
      "          -1.3676e-03, -1.0896e-02],\n",
      "         [ 2.9651e-02,  4.1680e-01,  2.4366e-01,  ...,  2.2203e-01,\n",
      "           6.1334e-02,  1.1966e-01],\n",
      "         [-1.0972e-01, -5.6000e-03,  5.9692e-02,  ...,  3.0073e-01,\n",
      "           3.5286e-01,  2.8653e-01],\n",
      "         ...,\n",
      "         [ 1.2277e-01,  8.1043e-02,  1.2308e-01,  ...,  3.6233e-02,\n",
      "           1.9720e-01,  1.8394e-01],\n",
      "         [ 5.0613e-02,  6.2781e-02,  6.2379e-02,  ...,  3.9920e-02,\n",
      "           7.8100e-02,  5.5935e-02],\n",
      "         [ 1.5843e-02,  6.4847e-02,  2.1439e-02,  ...,  1.5283e-02,\n",
      "           8.9256e-02,  4.1576e-02]],\n",
      "\n",
      "        [[-3.7149e-02,  8.0792e-03, -1.5225e-03,  ...,  1.1755e-02,\n",
      "          -1.4891e-03, -1.3061e-02],\n",
      "         [-8.6494e-03, -2.5836e-02,  2.2415e-01,  ...,  3.3119e-01,\n",
      "          -2.4001e-01,  4.8439e-01],\n",
      "         [-3.0078e-01, -8.2160e-02,  4.2907e-01,  ...,  2.2598e-01,\n",
      "          -7.9946e-02,  2.3608e-01],\n",
      "         ...,\n",
      "         [ 1.0614e-01,  1.8946e-01, -1.9936e-02,  ...,  5.2146e-02,\n",
      "           2.7098e-01,  6.4949e-02],\n",
      "         [ 1.2734e-01,  5.8392e-02, -5.3437e-02,  ...,  6.2603e-02,\n",
      "           3.7245e-01,  6.0236e-02],\n",
      "         [ 1.3935e-01,  4.5453e-02, -8.8622e-02,  ...,  1.6477e-01,\n",
      "           4.3821e-01,  3.2251e-02]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-3.7059e-02,  7.9947e-03, -1.4724e-03,  ...,  1.1367e-02,\n",
      "          -1.9510e-03, -1.4218e-02],\n",
      "         [-1.1843e-03,  3.1973e-01,  4.8026e-01,  ..., -8.2604e-03,\n",
      "          -1.3907e-01,  9.1765e-02],\n",
      "         [ 6.4474e-02,  2.3424e-01,  1.9595e-02,  ...,  6.6847e-02,\n",
      "           7.0244e-02,  1.5343e-01],\n",
      "         ...,\n",
      "         [-1.6014e-02,  2.4802e-01,  8.0870e-03,  ...,  3.6112e-03,\n",
      "           3.0494e-01,  3.1774e-02],\n",
      "         [ 9.5586e-03,  1.3095e-01,  3.0157e-02,  ...,  4.8129e-02,\n",
      "           2.8814e-02, -4.8882e-02],\n",
      "         [ 7.6966e-03,  1.5696e-01,  3.1137e-02,  ...,  5.1053e-02,\n",
      "           5.4411e-02, -7.8420e-02]],\n",
      "\n",
      "        [[-3.6614e-02,  7.2931e-03, -2.0682e-03,  ...,  1.2549e-02,\n",
      "          -3.4022e-04, -1.4590e-02],\n",
      "         [-1.8347e-01,  1.4000e-01,  2.0078e-01,  ...,  2.4640e-01,\n",
      "          -3.9356e-01, -1.1996e-01],\n",
      "         [-6.3016e-02,  1.8330e-01, -3.4979e-01,  ...,  2.9593e-01,\n",
      "          -1.6424e-01, -1.5812e-01],\n",
      "         ...,\n",
      "         [ 1.4133e-01,  1.1040e-02,  9.9254e-02,  ...,  2.1092e-02,\n",
      "           3.9644e-01,  1.3697e-01],\n",
      "         [ 7.4464e-02,  7.0928e-02,  4.1733e-02,  ...,  6.5397e-02,\n",
      "           2.1167e-01, -2.6823e-02],\n",
      "         [ 1.6819e-01,  2.2471e-02,  3.9231e-02,  ...,  5.7159e-02,\n",
      "           4.3849e-01,  8.3982e-02]],\n",
      "\n",
      "        [[-3.4929e-02,  8.7599e-03, -1.9166e-03,  ...,  1.1186e-02,\n",
      "          -1.2344e-03, -1.0955e-02],\n",
      "         [-2.1948e-01,  4.1065e-01,  3.8908e-01,  ...,  2.3295e-01,\n",
      "          -4.0531e-01,  2.1880e-01],\n",
      "         [ 3.1309e-02,  1.7303e-01,  2.1738e-01,  ...,  1.1751e-01,\n",
      "          -8.0240e-02, -3.3071e-02],\n",
      "         ...,\n",
      "         [ 1.9755e-01,  8.2803e-02,  3.0834e-02,  ...,  6.0052e-02,\n",
      "           2.4766e-01,  1.2810e-01],\n",
      "         [ 6.4714e-02,  7.6333e-02,  2.3721e-02,  ...,  2.5278e-02,\n",
      "           5.4639e-02,  3.9037e-02],\n",
      "         [ 7.4809e-02,  1.0626e-01,  6.3061e-02,  ..., -3.1076e-03,\n",
      "           6.0709e-02,  8.2175e-02]]])), encoder_attentions=None)\n",
      "model is finished\n",
      "odict_keys(['loss', 'logits', 'decoder_hidden_states', 'encoder_last_hidden_state', 'encoder_hidden_states', 'seg_kwargs', 'loss_0', 'loss_1', 'logits_segm', 'labels_segm'])\n",
      "non_empty_mask: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "out: tensor([[   2,    0,   11,  ...,    9,    5,    2],\n",
      "        [   2,    0,    4,  ..., 1213,   32,    2],\n",
      "        [   2,    0,   11,  ...,    4,   20,    2],\n",
      "        ...,\n",
      "        [   2,    0,   11,  ...,    4,   20,    2],\n",
      "        [   2,    0,   11,  ...,    4,  479,    2],\n",
      "        [   2,    0,   11,  ...,   11,    5,    2]])\n",
      "non_empty_mask: [True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True]\n",
      "out: tensor([[   2,    0,   11,  ..., 4215,  479,    2],\n",
      "        [   2,    0,    4,  ...,  479,  479,    2],\n",
      "        [   2,    0,   11,  ...,  479,    4,    2],\n",
      "        ...,\n",
      "        [   2,    0,    4,  ...,    4,   44,    2],\n",
      "        [   2,    0,   11,  ...,  775,    4,    2],\n",
      "        [   2,    0,   11,  ...,  128,  133,    2]])\n",
      "model_outputs:   in that, they are also in that. They have that, that, in them, in those, they have them in that in them in them. in his team are in that are in the league in that he have them. 'They have them on them in the team in that they are in them as in them on that. 'Those in them are in those. 'That is in that as in that with that in the same. 'This is in them who are in it. 'The two who are on the same level. Press about that.'. 'The same. The two are in this.'The same.''The two. The same..' 'The\n"
     ]
    }
   ],
   "source": [
    "for _, batch in enumerate(train_dataloader):\n",
    "    input_ids = batch[\"input_ids\"].to(model.device)\n",
    "    attention_mask = batch[\"attention_mask\"].to(model.device)\n",
    "    labels = batch[\"labels\"].to(model.device)\n",
    "    \n",
    "    print(\"input_ids.shape:\", input_ids.shape)\n",
    "    print(\"attention_mask.shape:\", attention_mask.shape)\n",
    "    print(\"labels.shape:\", labels.shape)\n",
    "\n",
    "    out = model(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask,\n",
    "        labels=labels,\n",
    "    )\n",
    "    print(out.keys())\n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        # attention_mask=attention_mask,\n",
    "        min_length=0,\n",
    "        max_length=142,\n",
    "        num_beams=4\n",
    "    )\n",
    "    # print(\"out:\", tokenizer.decode(out[0], skip_special_tokens=True))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19],\n",
       "        [ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "         18, 19]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix_tokens = torch.arange(20).long()\n",
    "prefix_tokens\n",
    "prefix_tokens = prefix_tokens.unsqueeze(0)\n",
    "prefix_tokens\n",
    "prefix_tokens = prefix_tokens.expand(32, -1)\n",
    "prefix_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pad_token_id: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[12312414],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       " [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-base\")\n",
    "print(\"pad_token_id:\", tokenizer.pad_token_id)\n",
    "segment_size = 20\n",
    "def get_full_padding_segment():\n",
    "    padding_segment = [tokenizer.pad_token_id for _ in range(segment_size)]\n",
    "    return padding_segment\n",
    "\n",
    "test = get_full_padding_segment()\n",
    "test\n",
    "[test]\n",
    "[[12312414]] + [test] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(342, 341)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ceil = math.ceil(1024/3)\n",
    "ceil, 1024//3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
