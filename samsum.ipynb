{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "from transformers import DataCollatorForSeq2Seq\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric, concatenate_datasets\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, default_data_collator, get_linear_schedule_with_warmup\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from peft import get_peft_config, get_peft_model, get_peft_model_state_dict, PrefixTuningConfig, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "device = 'cuda' \n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(args):\n",
    "    \"\"\"\n",
    "    :param args:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    torch.manual_seed(args)\n",
    "    torch.cuda.manual_seed_all(args)\n",
    "    np.random.seed(args)\n",
    "    random.seed(args)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "random_seed = 42\n",
    "set_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset samsum (/home/is/kaifan-l/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ddf56ee54284b28b45fb374646f2eda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 14732\n",
      "Test dataset size: 819\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集\n",
    "dataset = load_dataset('samsum')\n",
    "\n",
    "print(f\"Train dataset size: {len(dataset['train'])}\")\n",
    "print(f\"Test dataset size: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14732, 819, 818]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_lengths = [len(dataset[split]) for split in dataset]\n",
    "split_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载tokenizer\n",
    "\n",
    "checkpoint = 'google/flan-t5-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 983,040 || all params: 784,133,120 || trainable%: 0.1253664683874085\n"
     ]
    }
   ],
   "source": [
    "# 定义模型\n",
    "peft_config = PrefixTuningConfig(task_type=TaskType.SEQ_2_SEQ_LM, \n",
    "                                 inference_mode=False,\n",
    "                                 num_virtual_tokens=20,) # 所插入的虚拟token数量\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "model = get_peft_model(model, peft_config).to(device)\n",
    "model.print_trainable_parameters() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
    "    for i in range(0, len(list_of_elements), batch_size):\n",
    "        yield list_of_elements[i: i+batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n",
    "                                batch_size=16, device=device,\n",
    "                                column_text=\"article\",\n",
    "                                column_summary=\"highlights\"):\n",
    "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
    "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
    "\n",
    "    for article_batch, target_batch in tqdm(\n",
    "        zip(article_batches, target_batches), total=len(article_batches)):\n",
    "\n",
    "        inputs = tokenizer(article_batch, max_length=1024,  truncation=True,\n",
    "                           padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "        # model.generate是huggingface提供的一种文本生成的方法之一\n",
    "        # 用于基于给定的输入生成相应的输出\n",
    "        # length_penalty: 长度惩罚\n",
    "        # max_length: 生成文本的最大长度\n",
    "        summaries = model.generate(input_ids=inputs[\"input_ids\"].to(device),\n",
    "                                   attention_mask=inputs[\"attention_mask\"].to(device),\n",
    "                                   length_penalty=0.8, num_beams=8, max_length=128)\n",
    "        ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''\n",
    "\n",
    "        # Finally, we decode the generated texts,\n",
    "        # replace the  token, and add the decoded texts with the references to the metric.\n",
    "        decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True,\n",
    "                             clean_up_tokenization_spaces=True)\n",
    "                             for s in summaries]\n",
    "\n",
    "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
    "\n",
    "\n",
    "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
    "\n",
    "    #  Finally compute and return the ROUGE scores.\n",
    "    score = metric.compute()\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = load_metric('rouge')\n",
    "\n",
    "score = calculate_metric_on_test_ds(dataset['test'], rouge_metric, model, tokenizer,\n",
    "                                    column_text = 'dialogue', column_summary='summary', batch_size=8)\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': ['13818513',\n",
       "  '13728867',\n",
       "  '13681000',\n",
       "  '13730747',\n",
       "  '13728094',\n",
       "  '13716343',\n",
       "  '13611672',\n",
       "  '13730463',\n",
       "  '13809976',\n",
       "  '13809912'],\n",
       " 'dialogue': [\"Amanda: I baked  cookies. Do you want some?\\r\\nJerry: Sure!\\r\\nAmanda: I'll bring you tomorrow :-)\",\n",
       "  'Olivia: Who are you voting for in this election? \\r\\nOliver: Liberals as always.\\r\\nOlivia: Me too!!\\r\\nOliver: Great',\n",
       "  \"Tim: Hi, what's up?\\r\\nKim: Bad mood tbh, I was going to do lots of stuff but ended up procrastinating\\r\\nTim: What did you plan on doing?\\r\\nKim: Oh you know, uni stuff and unfucking my room\\r\\nKim: Maybe tomorrow I'll move my ass and do everything\\r\\nKim: We were going to defrost a fridge so instead of shopping I'll eat some defrosted veggies\\r\\nTim: For doing stuff I recommend Pomodoro technique where u use breaks for doing chores\\r\\nTim: It really helps\\r\\nKim: thanks, maybe I'll do that\\r\\nTim: I also like using post-its in kaban style\",\n",
       "  \"Edward: Rachel, I think I'm in ove with Bella..\\r\\nrachel: Dont say anything else..\\r\\nEdward: What do you mean??\\r\\nrachel: Open your fu**ing door.. I'm outside\",\n",
       "  \"Sam: hey  overheard rick say something\\r\\nSam: i don't know what to do :-/\\r\\nNaomi: what did he say??\\r\\nSam: he was talking on the phone with someone\\r\\nSam: i don't know who\\r\\nSam: and he was telling them that he wasn't very happy here\\r\\nNaomi: damn!!!\\r\\nSam: he was saying he doesn't like being my roommate\\r\\nNaomi: wow, how do you feel about it?\\r\\nSam: i thought i was a good rommate\\r\\nSam: and that we have a nice place\\r\\nNaomi: that's true man!!!\\r\\nNaomi: i used to love living with you before i moved in with me boyfriend\\r\\nNaomi: i don't know why he's saying that\\r\\nSam: what should i do???\\r\\nNaomi: honestly if it's bothering you that much you should talk to him\\r\\nNaomi: see what's going on\\r\\nSam: i don't want to get in any kind of confrontation though\\r\\nSam: maybe i'll just let it go\\r\\nSam: and see how it goes in the future\\r\\nNaomi: it's your choice sam\\r\\nNaomi: if i were you i would just talk to him and clear the air\",\n",
       "  \"Neville: Hi there, does anyone remember what date I got married on?\\r\\nDon: Are you serious?\\r\\nNeville: Dead serious. We're on vacation, and Tina's mad at me about something. I have a strange suspicion that this might have something to do with our wedding anniversary, but I have nowhere to check.\\r\\nWyatt: Hang on, I'll ask my wife.\\r\\nDon: Haha, someone's in a lot of trouble :D\\r\\nWyatt: September 17. I hope you remember the year ;)\",\n",
       "  \"John: Ave. Was there any homework for tomorrow?\\r\\nCassandra: hello :D Of course, as always :D\\r\\nJohn: What exactly?\\r\\nCassandra: I'm not sure so I'll check it for you in 20minutes. \\r\\nJohn: Cool, thanks. Sorry I couldn't be there, but I was busy as fuck...my stupid boss as always was trying to piss me off\\r\\nCassandra: No problem, what did he do this time?\\r\\nJohn: Nothing special, just the same as always, treating us like children, commanding to do this and that...\\r\\nCassandra: sorry to hear that. but why don't you just go to your chief and tell him everything?\\r\\nJohn: I would, but I don't have any support from others, they are like goddamn pupets and pretend that everything's fine...I'm not gonna fix everything for everyone\\r\\nCassandra: I understand...Nevertheless, just try to ignore him. I know it might sound ridiculous as fuck, but sometimes there's nothing more you can do.\\r\\nJohn: yeah I know...maybe some beer this week?\\r\\nCassandra: Sure, but I got some time after classes only...this week is gonna be busy\\r\\nJohn: no problem, I can drive you home and we can go to some bar or whatever.\\r\\nCassandra: cool. ok, I got this homework. it's page 15 ex. 2 and 3, I also asked the others to study another chapter, especially the vocabulary from the very first pages. Just read it.\\r\\nJohn: gosh...I don't know if I'm smart enough to do it :'D\\r\\nCassandra: you are, don't worry :P Just circle all the words you don't know and we'll continue on Monday.\\r\\nJohn: ok...then I'll try my best :D\\r\\nCassandra: sure, if you will have any questions just either text or call me and I'll help you.\\r\\nJohn: I hope I won't have to waste your time xD\\r\\nCassandra: you're not wasting my time, I'm your teacher, I'm here to help. This is what I get money for, also :P\\r\\nJohn: just kidding :D ok, so i guess we'll stay in touch then\\r\\nCassandra: sure, have a nice evening :D\\r\\nJohn: you too, se ya\\r\\nCassandra: Byeeeee\",\n",
       "  \"Sarah: I found a song on youtube and I think you'll like it\\r\\nJames: What song?\\r\\nSarah: <file_other>\\r\\nJames: Oh. I know it! \\r\\nJames: I heard it before in some compilation\\r\\nSarah: I can't stop playing it over and over\\r\\nJames: That's exactly how I know lyrics to all of the songs on my playlist :D\\r\\nSarah: Haha. No lyrics here though. Instrumental ;D\\r\\nJames: Instrumental songs are different kind of music. \\r\\nJames: But you have to remember that the activity you do when you listen to this song\\r\\nJames: Is the actvity your brain will connect to the song\\r\\nJames: And everytime you play this song at home\\r\\nJames: You'll be thinking of your work\\r\\nSarah: Yeah, I know that. That's why we sometimes say - I used to like that song, but now it just reminds me of bad memories\\r\\nJames: Yup. Everytime you change your partner, you have to get rid of your favorite music :D\\r\\nSarah: Hahaha. True, true.\",\n",
       "  'Noah: When and where are we meeting? :)\\r\\nMadison: I thought you were busy...?\\r\\nNoah: Yeah, I WAS. I quit my job. \\r\\nMadison: No way! :o :o :o Why? I thought you liked it...?\\r\\nNoah: Well, I used to, until my boss turned into a complete cock... Long story.',\n",
       "  \"Matt: Do you want to go for date?\\r\\nAgnes: Wow! You caught me out with this question Matt.\\r\\nMatt: Why?\\r\\nAgnes: I simply didn't expect this from you.\\r\\nMatt: Well, expect the unexpected.\\r\\nAgnes: Can I think about it?\\r\\nMatt: What is there to think about?\\r\\nAgnes: Well, I don't really know you.\\r\\nMatt: This is the perfect time to get to know eachother\\r\\nAgnes: Well that's true.\\r\\nMatt: So let's go to the Georgian restaurant in Kazimierz.\\r\\nAgnes: Now your convincing me.\\r\\nMatt: Cool, saturday at 6pm?\\r\\nAgnes: That's fine.\\r\\nMatt: I can pick you up on the way to the restaurant.\\r\\nAgnes: That's really kind of you.\\r\\nMatt: No problem.\\r\\nAgnes: See you on saturday.\\r\\nMatt: Yes, looking forward to it.\\r\\nAgnes: Me too.\"],\n",
       " 'summary': ['Amanda baked cookies and will bring Jerry some tomorrow.',\n",
       "  'Olivia and Olivier are voting for liberals in this election. ',\n",
       "  'Kim may try the pomodoro technique recommended by Tim to get more stuff done.',\n",
       "  'Edward thinks he is in love with Bella. Rachel wants Edward to open his door. Rachel is outside. ',\n",
       "  'Sam is confused, because he overheard Rick complaining about him as a roommate. Naomi thinks Sam should talk to Rick. Sam is not sure what to do.',\n",
       "  \"Wyatt reminds Neville his wedding anniversary is on the 17th of September. Neville's wife is upset and it might be because Neville forgot about their anniversary.\",\n",
       "  \"John didn't show up for class due to some work issues with his boss. Cassandra, his teacher told him which exercises to do, and which chapter to study. They are going to meet up for a beer sometime this week after class. \",\n",
       "  'Sarah sends James an instrumental song he might like. James knows the song. The brain connects the songs to the context they were played in and brings to mind the associated memories.',\n",
       "  'Noah wants to meet, he quit his job, because his boss was a dick.',\n",
       "  \"Matt invites Agnes for a date to get to know each other better. They'll go to the Georgian restaurant in Kazimierz on Saturday at 6 pm, and he'll pick her up on the way to the place.\"]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "446560fe55e441a78f100f6822a5e2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14732 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/is/kaifan-l/miniconda3/envs/summarization/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3596: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ede612b3197476fabdbccebcaed1483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4591ec890b490f9d26a9a0c7583711",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def convert_examples_to_features(example_batch):\n",
    "    input_encodings = tokenizer(example_batch['dialogue'], max_length=1024, truncation=True)\n",
    "\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        target_encodings = tokenizer(example_batch['summary'], max_length=128, truncation=True)\n",
    "\n",
    "    return {\n",
    "        'input_ids': input_encodings['input_ids'],\n",
    "        'attention_mask': input_encodings['attention_mask'],\n",
    "        'labels': target_encodings['input_ids']\n",
    "    }\n",
    "\n",
    "dataset_pt = dataset.map(convert_examples_to_features, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 14732\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 819\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 818\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 983,040 || all params: 784,133,120 || trainable%: 0.1253664683874085\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "trainer_args = TrainingArguments(\n",
    "    output_dir='saved',\n",
    "    num_train_epochs=1,\n",
    "    warmup_steps=500,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy='steps',\n",
    "    eval_steps=500,\n",
    "    save_steps=1e6,\n",
    "    gradient_accumulation_steps=16\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=trainer_args,\n",
    "                  tokenizer=tokenizer, data_collator=seq2seq_data_collator,\n",
    "                  train_dataset=dataset_pt['train'],\n",
    "                  eval_dataset=dataset_pt['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('t5-samsum/tokenizer/tokenizer_config.json',\n",
       " 't5-samsum/tokenizer/special_tokens_map.json',\n",
       " 't5-samsum/tokenizer/spiece.model',\n",
       " 't5-samsum/tokenizer/added_tokens.json',\n",
       " 't5-samsum/tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('t5-samsum/model')\n",
    "tokenizer.save_pretrained('t5-samsum/tokenizer')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarization",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
